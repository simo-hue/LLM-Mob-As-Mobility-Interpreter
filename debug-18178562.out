üöÄ AVVIO DIAGNOSTICA OLLAMA
==================================
Job ID: 18178562
Nodo: lrdn3440.leonardo.local
Data: Sat Aug  2 10:40:08 CEST 2025
Working dir: /leonardo/home/userexternal/smattiol/LLM-Mob-As-Mobility-Interpreter

üì¶ Caricamento moduli...
Loading python/3.11.6--gcc--8.5.0
  Loading requirement: bzip2/1.0.8-gp5wcz5 libmd/1.0.4-wja3f5q
    libbsd/0.11.7-cgxjopl expat/2.5.0-bptl3xw ncurses/6.4-asx3jea
    readline/8.2-nyw6mp6 gdbm/1.23-fs6otck libiconv/1.17-d7yvx2s
    xz/5.4.1-hubmwr5 zlib-ng/2.1.4-6htiapk libxml2/2.10.3-5eeeokp
    pigz/2.7-bopr5vp zstd/1.5.5-gawytfl tar/1.34-amqus5s gettext/0.22.3-2g7elif
    libffi/3.4.4-6r7brdq libxcrypt/4.4.35-ss2rzin sqlite/3.43.2
    util-linux-uuid/2.38.1-jkdi7kv
üîß Configurazione:
   Porta: 39001
   Modello: /leonardo/home/userexternal/smattiol/.ollama/models/blobs/sha256-667b0c1932bc6ffc593ed1d03f895bf2dc8dc6df21db3042284a6f4416b06a29
   Ollama: /leonardo/home/userexternal/smattiol/opt/ollama/bin/ollama

‚úÖ Porta salvata in ollama_port.txt
üåç Variabili ambiente Ollama configurate

üöÄ Avvio server Ollama...
   PID server: 1373006
‚è≥ Attesa server (max 60s)...
‚úÖ Server pronto dopo 4 secondi

üì• Setup modello llama3.1:8b...
‚úÖ Modello gi√† presente

üìù Creazione script diagnostico...

üî¨ Esecuzione diagnostica...
üîç DIAGNOSTICA OLLAMA DETTAGLIATA
==================================================
üåê URL base: http://127.0.0.1:39001

1Ô∏è‚É£ Test /api/tags
   Status: 200
   ‚úÖ Modelli: 1
      - llama3.1:8b (4GB)

2Ô∏è‚É£ Test /api/version
   Status: 200
   ‚úÖ Versione: {'version': '0.9.3'}

3Ô∏è‚É£ Test /api/generate
   üì§ Payload: {
    "model": "llama3.1:8b",
    "prompt": "Rispondi solo: TEST_OK",
    "stream": false,
    "options": {
        "num_predict": 10,
        "temperature": 0.1
    }
}
   ‚ùå Timeout (45s)

4Ô∏è‚É£ Test /api/chat
   üì§ Payload: {
    "model": "llama3.1:8b",
    "messages": [
        {
            "role": "user",
            "content": "Rispondi solo: CHAT_OK"
        }
    ],
    "stream": false,
    "options": {
        "num_predict": 10,
        "temperature": 0.1
    }
}
   ‚ùå Timeout (45s)

5Ô∏è‚É£ Test payload sbagliato (/api/chat con 'prompt')
   Status: 200
   Content-Length: 146
   Response: {"model":"llama3.1:8b","created_at":"2025-08-02T08:41:42.479703738Z","message":{"role":"assistant","content":""},"done_reason":"load","done":true}

6Ô∏è‚É£ Stress test (3 richieste rapide)
   ‚ùå Richiesta 1/3 errore: HTTPConnectionPool(host='127.0.0.1', port=39001): Read timed out. (read timeout=30)
   ‚ùå Richiesta 2/3 errore: HTTPConnectionPool(host='127.0.0.1', port=39001): Read timed out. (read timeout=30)
   ‚ùå Richiesta 3/3 errore: HTTPConnectionPool(host='127.0.0.1', port=39001): Read timed out. (read timeout=30)
   ‚ùå Stress test fallito (0/3)

==================================================
üìä RISULTATO: 2/6 test superati
‚ö†Ô∏è  OLLAMA FUNZIONA PARZIALMENTE
üí° Raccomandazione: Verifica payload e endpoint

‚úÖ DIAGNOSTICA COMPLETATA CON SUCCESSO
üìã Controlla l'output sopra per dettagli

üìã INFORMAZIONI SISTEMA
------------------------
GPU disponibili: 0
Memoria GPU:
6, 65536

üìã LOG SERVER (ultime 20 righe):
--------------------------------
llama_kv_cache_unified:        CPU KV buffer size =   512.00 MiB
llama_kv_cache_unified: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB
llama_context:        CPU compute buffer size =   296.01 MiB
llama_context: graph nodes  = 1094
llama_context: graph splits = 1
time=2025-08-02T10:40:16.893+02:00 level=INFO source=server.go:637 msg="llama runner started in 4.01 seconds"
time=2025-08-02T10:40:57.316+02:00 level=ERROR source=server.go:807 msg="post predict" error="Post \"http://127.0.0.1:38201/completion\": context canceled"
[GIN] 2025/08/02 - 10:40:57 | 500 | 45.044581169s |       127.0.0.1 | POST     "/api/generate"
time=2025-08-02T10:41:42.362+02:00 level=ERROR source=server.go:807 msg="post predict" error="Post \"http://127.0.0.1:38201/completion\": context canceled"
[GIN] 2025/08/02 - 10:41:42 | 500 | 45.044527199s |       127.0.0.1 | POST     "/api/chat"
time=2025-08-02T10:41:42.362+02:00 level=INFO source=runner.go:590 msg="aborting completion request due to client closing the connection"
[GIN] 2025/08/02 - 10:41:42 | 200 |  115.437286ms |       127.0.0.1 | POST     "/api/chat"
time=2025-08-02T10:42:12.510+02:00 level=ERROR source=server.go:807 msg="post predict" error="Post \"http://127.0.0.1:38201/completion\": context canceled"
[GIN] 2025/08/02 - 10:42:12 | 500 | 30.029317473s |       127.0.0.1 | POST     "/api/generate"
time=2025-08-02T10:42:12.511+02:00 level=INFO source=runner.go:590 msg="aborting completion request due to client closing the connection"
time=2025-08-02T10:42:42.541+02:00 level=ERROR source=server.go:807 msg="post predict" error="Post \"http://127.0.0.1:38201/completion\": context canceled"
[GIN] 2025/08/02 - 10:42:42 | 500 | 30.029676049s |       127.0.0.1 | POST     "/api/generate"
time=2025-08-02T10:42:42.542+02:00 level=INFO source=runner.go:590 msg="aborting completion request due to client closing the connection"
time=2025-08-02T10:43:12.573+02:00 level=ERROR source=server.go:807 msg="post predict" error="Post \"http://127.0.0.1:38201/completion\": context canceled"
[GIN] 2025/08/02 - 10:43:12 | 500 | 30.030765595s |       127.0.0.1 | POST     "/api/generate"

üèÅ DIAGNOSTICA TERMINATA
Job completato alle: Sat Aug  2 10:43:12 CEST 2025

üßπ Pulizia in corso...
   Terminazione server (PID: 1373006)
‚úÖ Pulizia completata
