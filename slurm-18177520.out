Loading python/3.11.6--gcc--8.5.0
  Loading requirement: bzip2/1.0.8-gp5wcz5 libmd/1.0.4-wja3f5q
    libbsd/0.11.7-cgxjopl expat/2.5.0-bptl3xw ncurses/6.4-asx3jea
    readline/8.2-nyw6mp6 gdbm/1.23-fs6otck libiconv/1.17-d7yvx2s
    xz/5.4.1-hubmwr5 zlib-ng/2.1.4-6htiapk libxml2/2.10.3-5eeeokp
    pigz/2.7-bopr5vp zstd/1.5.5-gawytfl tar/1.34-amqus5s gettext/0.22.3-2g7elif
    libffi/3.4.4-6r7brdq libxcrypt/4.4.35-ss2rzin sqlite/3.43.2
    util-linux-uuid/2.38.1-jkdi7kv
‚úÖ Ho scritto la porta in ollama_port.txt: 39000
‚ñ∂ Avvio server Ollama sulla porta 39000...
‚ñ∂ Nodo di esecuzione: lrdn1959.leonardo.local
‚ñ∂ Working directory: /leonardo/home/userexternal/smattiol/LLM-Mob-As-Mobility-Interpreter
‚ñ∂ Comando: /leonardo/home/userexternal/smattiol/opt/ollama/bin/ollama serve
‚ñ∂ Server PID: 1386613
‚ñ∂ Attesa che il server Ollama sia pronto...
‚úì Server Ollama attivo dopo 2 secondi
‚ñ∂ Caricamento del modello LLaMA...
‚ñ∂ Test del modello...
‚ö†Ô∏è  Test modello fallito, ma continuo comunque...
‚ñ∂ Lancio script Python alle Sat Aug  2 10:17:04 CEST 2025...
‚ñ∂ Directory: /leonardo/home/userexternal/smattiol/LLM-Mob-As-Mobility-Interpreter
üëâ Porta letta da ollama_port.txt: '39000'
üëâ Provo a contattare http://127.0.0.1:39000/api/tags
üìÇ Working dir: /leonardo/home/userexternal/smattiol/LLM-Mob-As-Mobility-Interpreter
üìÑ Contenuto di ollama_port.txt: '39000'
üîÑ Attesa Ollama su http://127.0.0.1:39000...
‚úì Ollama risponde con status 200
‚úì Runner LLaMA completamente attivo
üéâ Connessione Ollama stabilita con successo!
Traceback (most recent call last):
  File "/leonardo/home/userexternal/smattiol/LLM-Mob-As-Mobility-Interpreter/veronacard_mob_with_geom.py", line 90, in <module>
    if not warmup_model():
           ^^^^^^^^^^^^
NameError: name 'warmup_model' is not defined
‚ùå Script Python fallito con codice 1
‚ñ∂ Chiusura server alle Sat Aug  2 10:17:09 CEST 2025...
‚ñ∂ Ultime righe del log server:
llama_kv_cache_unified:        CPU KV buffer size =   512.00 MiB
llama_kv_cache_unified: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB
llama_context:        CPU compute buffer size =   296.01 MiB
llama_context: graph nodes  = 1094
llama_context: graph splits = 1
time=2025-08-02T10:16:09.179+02:00 level=INFO source=server.go:637 msg="llama runner started in 3.76 seconds"
time=2025-08-02T10:17:04.866+02:00 level=ERROR source=server.go:807 msg="post predict" error="Post \"http://127.0.0.1:34403/completion\": context canceled"
[GIN] 2025/08/02 - 10:17:04 | 500 |          1m0s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/08/02 - 10:17:08 | 200 |    1.462865ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/02 - 10:17:08 | 200 |      40.252¬µs |       127.0.0.1 | GET      "/api/version"
‚úÖ Job completato!
