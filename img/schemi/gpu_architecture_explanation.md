# Architettura GPU e Caricamento Modelli - Spiegazione Dettagliata

## 1. DOVE VIENE CARICATO IL MODELLO MIXTRAL:8x7b

### 1.1 Storage del Modello (Disco)
```bash
export OLLAMA_MODELS="$WORK/.ollama/models"
```

**Posizione**: Il modello viene **scaricato e salvato** su disco in:
```
/leonardo_work/IscrC_LLM-Mob/.ollama/models/
â”œâ”€â”€ manifests/
â”‚   â””â”€â”€ registry.ollama.ai/
â”‚       â””â”€â”€ library/
â”‚           â””â”€â”€ mixtral/
â”‚               â””â”€â”€ 8x7b
â””â”€â”€ blobs/
    â”œâ”€â”€ sha256-abc123... (layer 1 - ~4GB)
    â”œâ”€â”€ sha256-def456... (layer 2 - ~4GB) 
    â”œâ”€â”€ sha256-ghi789... (layer 3 - ~4GB)
    â”œâ”€â”€ ...
    â””â”€â”€ sha256-xyz999... (layer N - ~4GB)
```

**Dimensione totale**: ~26GB su disco (filesystem Lustre Leonardo)

### 1.2 Caricamento in Memoria GPU (Runtime)
Quando Ollama avvia, il modello viene caricato **dalla storage su disco â†’ VRAM GPU**:

```
DISCO ($WORK/.ollama/models) â†’ VRAM GPU (64GB A100)
      26GB                         ~45GB utilizzati
```

**IMPORTANTE**: Il modello non viene duplicato 4 volte! Vediamo come...

## 2. RUOLO DELLE 4 GPU A100

### 2.1 Architettura Multi-GPU Distributiva

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     NODO LEONARDO BOOSTER                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   GPU 0     â”‚    GPU 1    â”‚    GPU 2    â”‚     GPU 3       â”‚
â”‚  (MASTER)   â”‚   (SLAVE)   â”‚   (SLAVE)   â”‚    (SLAVE)      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Mixtral     â”‚  Mixtral    â”‚  Mixtral    â”‚   Mixtral       â”‚
â”‚ ~45GB VRAM  â”‚ ~45GB VRAM  â”‚ ~45GB VRAM  â”‚  ~45GB VRAM     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Ollama      â”‚  Ollama     â”‚  Ollama     â”‚   Ollama        â”‚
â”‚ Port 39001  â”‚ Port 39002  â”‚ Port 39003  â”‚  Port 39004     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ CUDA_       â”‚ CUDA_       â”‚ CUDA_       â”‚  CUDA_          â”‚
â”‚ VISIBLE_    â”‚ VISIBLE_    â”‚ VISIBLE_    â”‚  VISIBLE_       â”‚
â”‚ DEVICES=0   â”‚ DEVICES=1   â”‚ DEVICES=2   â”‚  DEVICES=3      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2.2 **OGNI GPU HA UNA COPIA COMPLETA DEL MODELLO**

**Misconception comune**: "Il modello Ã¨ distribuito tra le GPU"
**RealtÃ **: **Ogni GPU carica una copia completa** di Mixtral:8x7b nella propria VRAM

**PerchÃ©?**
1. **Indipendenza**: Ogni GPU puÃ² elaborare richieste in modo completamente autonomo
2. **Latenza**: Nessuna comunicazione inter-GPU necessaria durante inference
3. **Resilienza**: Se una GPU fallisce, le altre continuano a funzionare
4. **SemplicitÃ **: Nessuna sincronizzazione complessa tra GPU

### 2.3 Strategia di Caricamento del Modello

#### **Fase 1: GPU 0 (MASTER) - Prima Copia**
```bash
# GPU 0 carica il modello da disco
CUDA_VISIBLE_DEVICES=0 ollama serve &
# Questo prende 26GB da disco â†’ 45GB VRAM GPU 0
```

**Tempo**: ~3-5 minuti (lettura da disco + ottimizzazioni CUDA)

#### **Fase 2: GPU 1,2,3 (SLAVES) - Copia da GPU 0**
```bash
# GPU 1,2,3 copiano il modello giÃ  ottimizzato da GPU 0
CUDA_VISIBLE_DEVICES=1 ollama serve &  # Copia da GPU 0 â†’ GPU 1
CUDA_VISIBLE_DEVICES=2 ollama serve &  # Copia da GPU 0 â†’ GPU 2  
CUDA_VISIBLE_DEVICES=3 ollama serve &  # Copia da GPU 0 â†’ GPU 3
```

**Tempo**: ~1-2 minuti ciascuna (copia ottimizzata inter-GPU via NVLink)

**Meccanismo**: Le GPU slave utilizzano **CUDA memory-to-memory copy** dalla GPU master, molto piÃ¹ veloce della rilettura da disco.

## 3. OLLAMA: UN'ISTANZA PER GPU

### 3.1 Architettura Multi-Processo

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    SISTEMA HOST                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Processo 1  â”‚  Processo 2  â”‚  Processo 3  â”‚Processo4â”‚
â”‚              â”‚              â”‚              â”‚         â”‚
â”‚ ollama serve â”‚ ollama serve â”‚ ollama serve â”‚ollama   â”‚
â”‚              â”‚              â”‚              â”‚serve    â”‚
â”‚ PID: 12345   â”‚ PID: 12346   â”‚ PID: 12347   â”‚PID:     â”‚
â”‚              â”‚              â”‚              â”‚12348    â”‚
â”‚ GPU: 0       â”‚ GPU: 1       â”‚ GPU: 2       â”‚GPU: 3   â”‚
â”‚ Port: 39001  â”‚ Port: 39002  â”‚ Port: 39003  â”‚Port:    â”‚
â”‚              â”‚              â”‚              â”‚39004    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 3.2 **Ogni GPU ha un processo Ollama dedicato**

```bash
# 4 processi Ollama separati e indipendenti
CUDA_VISIBLE_DEVICES=0 OLLAMA_HOST=127.0.0.1:39001 ollama serve &  # PID A
CUDA_VISIBLE_DEVICES=1 OLLAMA_HOST=127.0.0.1:39002 ollama serve &  # PID B  
CUDA_VISIBLE_DEVICES=2 OLLAMA_HOST=127.0.0.1:39003 ollama serve &  # PID C
CUDA_VISIBLE_DEVICES=3 OLLAMA_HOST=127.0.0.1:39004 ollama serve &  # PID D
```

**Caratteristiche**:
- **Processi indipendenti**: Crash di uno non influenza gli altri
- **Porte diverse**: Ogni processo ascolta su una porta dedicata
- **GPU dedicata**: Ogni processo vede solo "la sua" GPU via `CUDA_VISIBLE_DEVICES`
- **Cache separata**: Ogni processo ha la sua directory cache

### 3.3 Isolamento e Sicurezza

```bash
# Processo GPU 0
CUDA_VISIBLE_DEVICES=0          # Vede solo GPU 0
OLLAMA_CACHE_DIR="$WORK/.ollama/cache/gpu0"  # Cache dedicata
OLLAMA_HOST=127.0.0.1:39001     # Porta dedicata

# Processo GPU 1  
CUDA_VISIBLE_DEVICES=1          # Vede solo GPU 1
OLLAMA_CACHE_DIR="$WORK/.ollama/cache/gpu1"  # Cache separata
OLLAMA_HOST=127.0.0.1:39002     # Porta diversa
```

## 4. FLUSSO DI UTILIZZO MEMORIA

### 4.1 Analisi Utilizzo VRAM per GPU

```
GPU A100-SXM4-64GB: 64GB VRAM totali
â”œâ”€â”€ Sistema/Driver:     ~2GB   (riservato CUDA driver)
â”œâ”€â”€ Ollama overhead:    ~1GB   (processo, cache, buffer)  
â”œâ”€â”€ Mixtral modello:   ~45GB   (parametri + ottimizzazioni)
â”œâ”€â”€ Context buffer:     ~4GB   (prompt/response in elaborazione)
â”œâ”€â”€ Spazio libero:     ~12GB   (safety margin)
â””â”€â”€ TOTAL:             64GB
```

**Utilizzo effettivo**: ~70-75% VRAM per GPU (come da configurazione `OLLAMA_MAX_VRAM_USAGE=0.75`)

### 4.2 Memoria RAM Sistema (256GB allocati)

```
RAM Sistema 256GB:
â”œâ”€â”€ Sistema operativo:    ~8GB
â”œâ”€â”€ Python script:       ~4GB
â”œâ”€â”€ 4x Ollama processes: ~16GB  (4GB ciascuno)
â”œâ”€â”€ Buffer I/O:          ~8GB
â”œâ”€â”€ File temporanei:    ~20GB
â”œâ”€â”€ Spazio libero:     ~200GB   (per cache, temp files, etc.)
â””â”€â”€ TOTAL:             256GB
```

## 5. LOAD BALANCING E DISTRIBUZIONE RICHIESTE

### 5.1 Come il Python Script Distribuisce il Carico

```python
# File: ollama_ports.txt
39001,39002,39003,39004

# Python legge le porte e fa load balancing
class OllamaConnectionManager:
    def get_best_host(self) -> str:
        # Seleziona la GPU con minor carico
        return "http://127.0.0.1:39002"  # Esempio: GPU 1 meno occupata
```

### 5.2 Pattern di Distribuzione

```
Richiesta 1 â†’ GPU 0 (Port 39001) â†’ Mixtral inference â†’ Response 1
Richiesta 2 â†’ GPU 1 (Port 39002) â†’ Mixtral inference â†’ Response 2  
Richiesta 3 â†’ GPU 2 (Port 39003) â†’ Mixtral inference â†’ Response 3
Richiesta 4 â†’ GPU 3 (Port 39004) â†’ Mixtral inference â†’ Response 4
Richiesta 5 â†’ GPU 0 (Port 39001) â†’ ... (round-robin o load-based)
```

**Parallelismo**: 4 richieste possono essere elaborate **simultaneamente** (una per GPU)

## 6. VANTAGGI DI QUESTA ARCHITETTURA

### 6.1 **ScalabilitÃ  Lineare**
- 1 GPU = ~1 richiesta/secondo
- 4 GPU = ~4 richieste/secondo  
- Scaling quasi perfetto

### 6.2 **Fault Tolerance**
```
Se GPU 2 crasha:
â”œâ”€â”€ GPU 0, 1, 3 continuano a funzionare
â”œâ”€â”€ Throughput ridotto a 3/4 (75%)
â”œâ”€â”€ Nessuna perdita di dati
â””â”€â”€ Recovery automatico possibile
```

### 6.3 **Isolamento Completo**
- Nessuna condivisione di stato tra GPU
- Nessuna sincronizzazione necessaria
- Debug semplificato (log separati per GPU)

### 6.4 **Ottimizzazione Risorse**
- Ogni GPU utilizza la propria VRAM ottimalmente
- Nessun overhead di comunicazione inter-GPU
- Cache dedicata riduce I/O conflicts

## 7. CONFRONTO CON ALTERNATIVE

### 7.1 **Architettura Attuale vs. Distribuzione Tensor**

| Aspetto | **Multi-Istanza (Attuale)** | **Tensor Parallel** |
|---------|------------------------------|----------------------|
| Memoria modello | 4 Ã— 45GB = 180GB | 1 Ã— 45GB distribuito |
| Latenza | Bassa (no sync) | Media (sync inter-GPU) |
| Throughput | 4x parallelo | 1x (ma piÃ¹ veloce) |
| Fault tolerance | Alta | Bassa (single point) |
| ComplessitÃ  | Bassa | Alta |

### 7.2 **PerchÃ© Multi-Istanza Ã¨ Migliore per il Nostro Caso**

1. **Workload**: Molte richieste piccole/medie â†’ parallelismo > velocitÃ  singola
2. **Reliability**: Sistema di produzione â†’ fault tolerance critica  
3. **Sviluppo**: Meno complessitÃ  â†’ manutenzione piÃ¹ semplice
4. **Risorse**: 4Ã—64GB VRAM â†’ spazio abbondante per copie multiple

## 8. MONITORING E DIAGNOSTICA

### 8.1 Come Verificare il Caricamento

```bash
# Verifica VRAM utilizzata per GPU
nvidia-smi --query-gpu=index,memory.used,memory.total --format=csv

# Output atteso:
# 0, 45234 MiB, 65536 MiB  # GPU 0: ~45GB utilizzati
# 1, 45156 MiB, 65536 MiB  # GPU 1: ~45GB utilizzati  
# 2, 45198 MiB, 65536 MiB  # GPU 2: ~45GB utilizzati
# 3, 45167 MiB, 65536 MiB  # GPU 3: ~45GB utilizzati
```

### 8.2 Verifica Processi Ollama

```bash
ps aux | grep ollama

# Output atteso:
# user 12345 ollama serve (GPU 0, port 39001)
# user 12346 ollama serve (GPU 1, port 39002)
# user 12347 ollama serve (GPU 2, port 39003)  
# user 12348 ollama serve (GPU 3, port 39004)
```

### 8.3 Test Connectivity

```bash
# Test tutti gli endpoint
curl http://127.0.0.1:39001/api/tags  # GPU 0
curl http://127.0.0.1:39002/api/tags  # GPU 1
curl http://127.0.0.1:39003/api/tags  # GPU 2
curl http://127.0.0.1:39004/api/tags  # GPU 3
```

## RIEPILOGO ARCHITETTURALE

ðŸ”¹ **Modello**: Mixtral:8x7b caricato **4 volte** (una copia completa per GPU)
ðŸ”¹ **Storage**: Disco condiviso ($WORK/.ollama/models) â†’ 4x VRAM GPU
ðŸ”¹ **Ollama**: **4 processi indipendenti**, uno per GPU, porte separate  
ðŸ”¹ **Load Balancing**: Python script distribuisce richieste tra le 4 istanze
ðŸ”¹ **ScalabilitÃ **: 4x throughput parallelo con fault tolerance integrata

Questa architettura **sacrifica memoria per ottenere throughput, fault tolerance e semplicitÃ **, perfetta per workload di produzione con molte richieste parallele come l'analisi turistica.