Loading python/3.11.6--gcc--8.5.0
  Loading requirement: bzip2/1.0.8-gp5wcz5 libmd/1.0.4-wja3f5q
    libbsd/0.11.7-cgxjopl expat/2.5.0-bptl3xw ncurses/6.4-asx3jea
    readline/8.2-nyw6mp6 gdbm/1.23-fs6otck libiconv/1.17-d7yvx2s
    xz/5.4.1-hubmwr5 zlib-ng/2.1.4-6htiapk libxml2/2.10.3-5eeeokp
    pigz/2.7-bopr5vp zstd/1.5.5-gawytfl tar/1.34-amqus5s gettext/0.22.3-2g7elif
    libffi/3.4.4-6r7brdq libxcrypt/4.4.35-ss2rzin sqlite/3.43.2
    util-linux-uuid/2.38.1-jkdi7kv
✅ Ho scritto la porta in ollama_port.txt: 39000
▶ Avvio runner LLaMA sulla porta 39000...
▶ Nodo di esecuzione: lrdn1839.leonardo.local
▶ Working directory: /leonardo/home/userexternal/smattiol/LLM-Mob-As-Mobility-Interpreter
▶ Runner PID: 2045692
▶ Attesa che il server sia pronto...
✓ Runner LLaMA attivo dopo 4 secondi
✓ Server completamente pronto
▶ Lancio script Python alle Sat Aug  2 09:50:01 CEST 2025...
▶ Directory: /leonardo/home/userexternal/smattiol/LLM-Mob-As-Mobility-Interpreter
👉 Porta letta da ollama_port.txt: '39000'
👉 Provo a contattare http://127.0.0.1:39000/api/tags
📂 Working dir: /leonardo/home/userexternal/smattiol/LLM-Mob-As-Mobility-Interpreter
📄 Contenuto di ollama_port.txt: '39000'
🔄 Attesa Ollama su http://127.0.0.1:39000...
🔄 Tentativo 1/30: HTTP 404
⏳ Attendo 3s prima del prossimo tentativo...
🔄 Tentativo 2/30: HTTP 404
⏳ Attendo 3s prima del prossimo tentativo...
🔄 Tentativo 3/30: HTTP 404
⏳ Attendo 3s prima del prossimo tentativo...
🔄 Tentativo 4/30: HTTP 404
⏳ Attendo 3s prima del prossimo tentativo...
🔄 Tentativo 5/30: HTTP 404
⏳ Attendo 3s prima del prossimo tentativo...
🔄 Tentativo 6/30: HTTP 404
⏳ Attendo 3s prima del prossimo tentativo...
🔄 Tentativo 7/30: HTTP 404
⏳ Attendo 3s prima del prossimo tentativo...
🔄 Tentativo 8/30: HTTP 404
⏳ Attendo 3s prima del prossimo tentativo...
🔄 Tentativo 9/30: HTTP 404
⏳ Attendo 3s prima del prossimo tentativo...
🔄 Tentativo 10/30: HTTP 404
⏳ Attendo 3s prima del prossimo tentativo...
🔄 Tentativo 11/30: HTTP 404
⏳ Attendo 3s prima del prossimo tentativo...
🔄 Tentativo 12/30: HTTP 404
⏳ Attendo 3s prima del prossimo tentativo...
🔄 Tentativo 13/30: HTTP 404
⏳ Attendo 3s prima del prossimo tentativo...
🔄 Tentativo 14/30: HTTP 404
⏳ Attendo 3s prima del prossimo tentativo...
🔄 Tentativo 15/30: HTTP 404
⏳ Attendo 3s prima del prossimo tentativo...
🔄 Tentativo 16/30: HTTP 404
⏳ Attendo 3s prima del prossimo tentativo...
🔄 Tentativo 17/30: HTTP 404
⏳ Attendo 3s prima del prossimo tentativo...
🔄 Tentativo 18/30: HTTP 404
⏳ Attendo 3s prima del prossimo tentativo...
🔄 Tentativo 19/30: HTTP 404
⏳ Attendo 3s prima del prossimo tentativo...
🔄 Tentativo 20/30: HTTP 404
⏳ Attendo 3s prima del prossimo tentativo...
🔄 Tentativo 21/30: HTTP 404
⏳ Attendo 3s prima del prossimo tentativo...
🔄 Tentativo 22/30: HTTP 404
⏳ Attendo 3s prima del prossimo tentativo...
🔄 Tentativo 23/30: HTTP 404
⏳ Attendo 3s prima del prossimo tentativo...
🔄 Tentativo 24/30: HTTP 404
⏳ Attendo 3s prima del prossimo tentativo...
🔄 Tentativo 25/30: HTTP 404
⏳ Attendo 3s prima del prossimo tentativo...
🔄 Tentativo 26/30: HTTP 404
⏳ Attendo 3s prima del prossimo tentativo...
🔄 Tentativo 27/30: HTTP 404
⏳ Attendo 3s prima del prossimo tentativo...
🔄 Tentativo 28/30: HTTP 404
⏳ Attendo 3s prima del prossimo tentativo...
🔄 Tentativo 29/30: HTTP 404
⏳ Attendo 3s prima del prossimo tentativo...
🔄 Tentativo 30/30: HTTP 404
Traceback (most recent call last):
  File "/leonardo/home/userexternal/smattiol/LLM-Mob-As-Mobility-Interpreter/veronacard_mob_with_geom.py", line 85, in <module>
    raise RuntimeError("❌ Ollama non ha risposto dopo tutti i tentativi")
RuntimeError: ❌ Ollama non ha risposto dopo tutti i tentativi
❌ Script Python fallito con codice 1
▶ Chiusura runner alle Sat Aug  2 09:51:31 CEST 2025...
▶ Ultime righe del log runner:
llama_context: freq_base     = 500000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
llama_context:        CPU  output buffer size =     1.01 MiB
llama_kv_cache_unified: kv_size = 8192, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1, padding = 32
llama_kv_cache_unified:        CPU KV buffer size =  1024.00 MiB
llama_kv_cache_unified: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB
llama_context:        CPU compute buffer size =   560.01 MiB
llama_context: graph nodes  = 1094
llama_context: graph splits = 1
✅ Job completato!
