Loading python/3.11.6--gcc--8.5.0
  Loading requirement: bzip2/1.0.8-gp5wcz5 libmd/1.0.4-wja3f5q
    libbsd/0.11.7-cgxjopl expat/2.5.0-bptl3xw ncurses/6.4-asx3jea
    readline/8.2-nyw6mp6 gdbm/1.23-fs6otck libiconv/1.17-d7yvx2s
    xz/5.4.1-hubmwr5 zlib-ng/2.1.4-6htiapk libxml2/2.10.3-5eeeokp
    pigz/2.7-bopr5vp zstd/1.5.5-gawytfl tar/1.34-amqus5s gettext/0.22.3-2g7elif
    libffi/3.4.4-6r7brdq libxcrypt/4.4.35-ss2rzin sqlite/3.43.2
    util-linux-uuid/2.38.1-jkdi7kv
âœ… Ho scritto la porta in ollama_port.txt: 39000
â–¶ Avvio runner LLaMA sulla porta 39000...
â–¶ Nodo di esecuzione: lrdn1839.leonardo.local
â–¶ Working directory: /leonardo/home/userexternal/smattiol/LLM-Mob-As-Mobility-Interpreter
â–¶ Runner PID: 2045692
â–¶ Attesa che il server sia pronto...
âœ“ Runner LLaMA attivo dopo 4 secondi
âœ“ Server completamente pronto
â–¶ Lancio script Python alle Sat Aug  2 09:50:01 CEST 2025...
â–¶ Directory: /leonardo/home/userexternal/smattiol/LLM-Mob-As-Mobility-Interpreter
ğŸ‘‰ Porta letta da ollama_port.txt: '39000'
ğŸ‘‰ Provo a contattare http://127.0.0.1:39000/api/tags
ğŸ“‚ Working dir: /leonardo/home/userexternal/smattiol/LLM-Mob-As-Mobility-Interpreter
ğŸ“„ Contenuto di ollama_port.txt: '39000'
ğŸ”„ Attesa Ollama su http://127.0.0.1:39000...
ğŸ”„ Tentativo 1/30: HTTP 404
â³ Attendo 3s prima del prossimo tentativo...
ğŸ”„ Tentativo 2/30: HTTP 404
â³ Attendo 3s prima del prossimo tentativo...
ğŸ”„ Tentativo 3/30: HTTP 404
â³ Attendo 3s prima del prossimo tentativo...
ğŸ”„ Tentativo 4/30: HTTP 404
â³ Attendo 3s prima del prossimo tentativo...
ğŸ”„ Tentativo 5/30: HTTP 404
â³ Attendo 3s prima del prossimo tentativo...
ğŸ”„ Tentativo 6/30: HTTP 404
â³ Attendo 3s prima del prossimo tentativo...
ğŸ”„ Tentativo 7/30: HTTP 404
â³ Attendo 3s prima del prossimo tentativo...
ğŸ”„ Tentativo 8/30: HTTP 404
â³ Attendo 3s prima del prossimo tentativo...
ğŸ”„ Tentativo 9/30: HTTP 404
â³ Attendo 3s prima del prossimo tentativo...
ğŸ”„ Tentativo 10/30: HTTP 404
â³ Attendo 3s prima del prossimo tentativo...
ğŸ”„ Tentativo 11/30: HTTP 404
â³ Attendo 3s prima del prossimo tentativo...
ğŸ”„ Tentativo 12/30: HTTP 404
â³ Attendo 3s prima del prossimo tentativo...
ğŸ”„ Tentativo 13/30: HTTP 404
â³ Attendo 3s prima del prossimo tentativo...
ğŸ”„ Tentativo 14/30: HTTP 404
â³ Attendo 3s prima del prossimo tentativo...
ğŸ”„ Tentativo 15/30: HTTP 404
â³ Attendo 3s prima del prossimo tentativo...
ğŸ”„ Tentativo 16/30: HTTP 404
â³ Attendo 3s prima del prossimo tentativo...
ğŸ”„ Tentativo 17/30: HTTP 404
â³ Attendo 3s prima del prossimo tentativo...
ğŸ”„ Tentativo 18/30: HTTP 404
â³ Attendo 3s prima del prossimo tentativo...
ğŸ”„ Tentativo 19/30: HTTP 404
â³ Attendo 3s prima del prossimo tentativo...
ğŸ”„ Tentativo 20/30: HTTP 404
â³ Attendo 3s prima del prossimo tentativo...
ğŸ”„ Tentativo 21/30: HTTP 404
â³ Attendo 3s prima del prossimo tentativo...
ğŸ”„ Tentativo 22/30: HTTP 404
â³ Attendo 3s prima del prossimo tentativo...
ğŸ”„ Tentativo 23/30: HTTP 404
â³ Attendo 3s prima del prossimo tentativo...
ğŸ”„ Tentativo 24/30: HTTP 404
â³ Attendo 3s prima del prossimo tentativo...
ğŸ”„ Tentativo 25/30: HTTP 404
â³ Attendo 3s prima del prossimo tentativo...
ğŸ”„ Tentativo 26/30: HTTP 404
â³ Attendo 3s prima del prossimo tentativo...
ğŸ”„ Tentativo 27/30: HTTP 404
â³ Attendo 3s prima del prossimo tentativo...
ğŸ”„ Tentativo 28/30: HTTP 404
â³ Attendo 3s prima del prossimo tentativo...
ğŸ”„ Tentativo 29/30: HTTP 404
â³ Attendo 3s prima del prossimo tentativo...
ğŸ”„ Tentativo 30/30: HTTP 404
Traceback (most recent call last):
  File "/leonardo/home/userexternal/smattiol/LLM-Mob-As-Mobility-Interpreter/veronacard_mob_with_geom.py", line 85, in <module>
    raise RuntimeError("âŒ Ollama non ha risposto dopo tutti i tentativi")
RuntimeError: âŒ Ollama non ha risposto dopo tutti i tentativi
âŒ Script Python fallito con codice 1
â–¶ Chiusura runner alle Sat Aug  2 09:51:31 CEST 2025...
â–¶ Ultime righe del log runner:
llama_context: freq_base     = 500000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
llama_context:        CPU  output buffer size =     1.01 MiB
llama_kv_cache_unified: kv_size = 8192, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1, padding = 32
llama_kv_cache_unified:        CPU KV buffer size =  1024.00 MiB
llama_kv_cache_unified: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB
llama_context:        CPU compute buffer size =   560.01 MiB
llama_context: graph nodes  = 1094
llama_context: graph splits = 1
âœ… Job completato!
