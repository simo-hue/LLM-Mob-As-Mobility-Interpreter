🔍 DIAGNOSTICA OLLAMA E GPU
==========================
Job ID: 18185947
Nodo: lrdn2771.leonardo.local
Data: Sat Aug  2 14:07:59 CEST 2025
Loading python/3.11.6--gcc--8.5.0
  Loading requirement: bzip2/1.0.8-gp5wcz5 libmd/1.0.4-wja3f5q
    libbsd/0.11.7-cgxjopl expat/2.5.0-bptl3xw ncurses/6.4-asx3jea
    readline/8.2-nyw6mp6 gdbm/1.23-fs6otck libiconv/1.17-d7yvx2s
    xz/5.4.1-hubmwr5 zlib-ng/2.1.4-6htiapk libxml2/2.10.3-5eeeokp
    pigz/2.7-bopr5vp zstd/1.5.5-gawytfl tar/1.34-amqus5s gettext/0.22.3-2g7elif
    libffi/3.4.4-6r7brdq libxcrypt/4.4.35-ss2rzin sqlite/3.43.2
    util-linux-uuid/2.38.1-jkdi7kv
🔍 INFO GPU DETTAGLIATA:
Sat Aug  2 14:08:00 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.54.03              Driver Version: 535.54.03    CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA A100-SXM-64GB           On  | 00000000:C8:00.0 Off |                    0 |
| N/A   43C    P0              61W / 460W |      2MiB / 65536MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|  No running processes found                                                           |
+---------------------------------------------------------------------------------------+

CUDA_VISIBLE_DEVICES: 0

📁 TEST 1: Verifica modello
✅ Modello trovato: 4.6G	/leonardo/home/userexternal/smattiol/.ollama/models/blobs/sha256-667b0c1932bc6ffc593ed1d03f895bf2dc8dc6df21db3042284a6f4416b06a29
🚀 TEST 2: Avvio server
Server PID: 434034
⏳ TEST 3: Attesa server (max 60s)
✅ Server attivo dopo 3s
🔨 TEST 4: Creazione modello minimal
Creando modello llama3.1:8b-minimal...
Note: Unnecessary use of -X or --request, POST is already inferred.
*   Trying 127.0.0.1...
* TCP_NODELAY set
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0* Connected to 127.0.0.1 (127.0.0.1) port 39003 (#0)
> POST /api/create HTTP/1.1
> Host: 127.0.0.1:39003
> User-Agent: curl/7.61.1
> Accept: */*
> Content-Type: application/json
> Content-Length: 496
> 
} [496 bytes data]
* upload completely sent off: 496 out of 496 bytes
< HTTP/1.1 400 Bad Request
< Content-Type: application/json; charset=utf-8
< Date: Sat, 02 Aug 2025 12:08:04 GMT
< Content-Length: 55
< 
{ [55 bytes data]
100   551  100    55  100   496  55000   484k --:--:-- --:--:-- --:--:--  538k
* Connection #0 to host 127.0.0.1 left intact
{"error":"invalid character 'P' in string escape code"}✅ Modello creato
🧪 TEST 5: Test inferenza progressivi
Test 5a: Micro (1 token)
❌ Micro test FAIL (0s): {"error":"model 'llama3.1:8b-minimal' not found"}
Test 5b: Piccolo (5 token)
❌ Test piccolo FAIL (0s): {"error":"model 'llama3.1:8b-minimal' not found"}
Test 5c: Normale (20 token)
❌ Test normale FAIL (0s): {"error":"model 'llama3.1:8b-minimal' not found"}
📊 TEST 6: Stato finale sistema
Processo Ollama:
smattiol  434034 17.0  0.0 6748976 34624 ?       Sl   14:08   0:00 /leonardo/home/userexternal/smattiol/opt/ollama/bin/ollama serve

Memoria GPU finale:
memory.used [MiB], memory.total [MiB], utilization.gpu [%], temperature.gpu
6 MiB, 65536 MiB, 10 %, 44

Spazio disco modelli:
4.6G	/leonardo/home/userexternal/smattiol/.ollama/models/

Ultimi log server:
dlsym: cuMemGetInfo_v2 - 0x154c0a6a98b0
dlsym: cuCtxDestroy - 0x154c0a6f3f40
calling cuInit
calling cuDriverGetVersion
raw version 0x2ef4
CUDA driver version: 12.2
calling cuDeviceGetCount
device count 1
time=2025-08-02T14:08:04.052+02:00 level=DEBUG source=gpu.go:125 msg="detected GPUs" count=1 library=/usr/lib64/libcuda.so.535.54.03
[GPU-d797fbe8-5bb5-e338-ed04-3380653e286e] CUDA totalMem 64944mb
[GPU-d797fbe8-5bb5-e338-ed04-3380653e286e] CUDA freeMem 64462mb
[GPU-d797fbe8-5bb5-e338-ed04-3380653e286e] Compute Capability 8.0
time=2025-08-02T14:08:04.149+02:00 level=DEBUG source=amd_linux.go:419 msg="amdgpu driver not detected /sys/module/amdgpu"
releasing cuda driver library
time=2025-08-02T14:08:04.149+02:00 level=INFO source=types.go:130 msg="inference compute" id=GPU-d797fbe8-5bb5-e338-ed04-3380653e286e library=cuda variant=v12 compute=8.0 driver=12.2 name="NVIDIA A100-SXM-64GB" total="63.4 GiB" available="63.0 GiB"
[GIN] 2025/08/02 - 14:08:04 | 200 |    2.062128ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/02 - 14:08:04 | 400 |      79.548µs |       127.0.0.1 | POST     "/api/create"
[GIN] 2025/08/02 - 14:08:04 | 404 |    1.508668ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/08/02 - 14:08:04 | 404 |    1.508069ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/08/02 - 14:08:04 | 404 |    1.256282ms |       127.0.0.1 | POST     "/api/generate"

🏁 DIAGNOSTICA COMPLETATA
🧹 Cleanup...
