🚀 LLM-MOB PARALLEL PRODUCTION RUN
==================================
Job ID: 19248401
Nodo: lrdn0065.leonardo.local
Data: Thu Aug 21 18:30:25 CEST 2025
🔥 MODALITÀ: Elaborazione parallela su 4x A100

📦 Caricamento moduli e ambiente...
Loading python/3.11.6--gcc--8.5.0
  Loading requirement: bzip2/1.0.8-gp5wcz5 libmd/1.0.4-wja3f5q
    libbsd/0.11.7-cgxjopl expat/2.5.0-bptl3xw ncurses/6.4-asx3jea
    readline/8.2-nyw6mp6 gdbm/1.23-fs6otck libiconv/1.17-d7yvx2s
    xz/5.4.1-hubmwr5 zlib-ng/2.1.4-6htiapk libxml2/2.10.3-5eeeokp
    pigz/2.7-bopr5vp zstd/1.5.5-gawytfl tar/1.34-amqus5s gettext/0.22.3-2g7elif
    libffi/3.4.4-6r7brdq libxcrypt/4.4.35-ss2rzin sqlite/3.43.2
    util-linux-uuid/2.38.1-jkdi7kv
✅ Python: Python 3.11.6
✅ Virtual env: /leonardo_work/IscrC_LLM-Mob/venv
✅ CUDA: Cuda compilation tools, release 12.3, V12.3.103

🔍 INFO GPU DETTAGLIATE:
0, NVIDIA A100-SXM-64GB, 65536 MiB, 2 MiB, 0 %, 42
1, NVIDIA A100-SXM-64GB, 65536 MiB, 2 MiB, 0 %, 43
2, NVIDIA A100-SXM-64GB, 65536 MiB, 2 MiB, 0 %, 43
3, NVIDIA A100-SXM-64GB, 65536 MiB, 2 MiB, 0 %, 42
CUDA_VISIBLE_DEVICES: 0,1,2,3
GPU disponibili per il job: 4

⚙️ Configurazione Ollama per elaborazione parallela...
✅ Versione Ollama: 0.3.14

🚀 Avvio server Ollama per elaborazione parallela...
✅ Porta server: 39003
🧹 Pulizia processi precedenti...
🔥 Avvio con supporto 4x A100...
✅ Server PID: 836874
✅ Log file: ollama_parallel.log

⏳ Attesa avvio server multi-GPU (max 120s)...
✅ Server multi-GPU operativo dopo 18s

🔥 Preparazione modello per elaborazione parallela...
📋 Modelli disponibili:
  ✅ deepseek-coder:33b
  ✅ mixtral:8x7b
  ✅ llama3.1:8b
✅ Modello llama3.1:8b disponibile
🔥 Pre-caricamento modello su tutte le GPU...
📡 Warm-up GPU 0...
📡 Warm-up GPU 1...
📡 Warm-up GPU 2...
📡 Warm-up GPU 3...
slurmstepd: error: *** JOB 19248401 ON lrdn0065 CANCELLED AT 2025-08-21T18:43:50 DUE TO TIME LIMIT ***

🧹 CLEANUP PRODUZIONE PARALLELA...
⏱️ Tempo totale job: 805 secondi (0h 13m)
📊 Stato finale tutte le GPU:
index, memory.used [MiB], memory.total [MiB], utilization.gpu [%], temperature.gpu, power.draw [W]
0, 7252 MiB, 65536 MiB, 0 %, 43, 71.44 W
1, 6 MiB, 65536 MiB, 0 %, 43, 62.59 W
2, 6 MiB, 65536 MiB, 0 %, 43, 60.43 W
3, 6 MiB, 65536 MiB, 0 %, 42, 61.76 W
🔄 Shutdown graceful Ollama multi-GPU...
Unable to disable persistence mode for GPU 00000000:1D:00.0: Insufficient Permissions
✅ Cleanup completato
