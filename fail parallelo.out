ğŸš€ LLM-MOB PARALLEL PRODUCTION RUN - VERSIONE OTTIMIZZATA HPC
==============================================================
Job ID: 18855374
Nodo: lrdn1022.leonardo.local
Data: Wed Aug 13 13:48:57 CEST 2025
ğŸ’¡ MODALITÃ€: Processamento parallelo completo
ğŸ”§ CPUs disponibili: 32
ğŸ’¾ Memoria allocata: 128GB

ğŸ“¦ Caricamento moduli e ambiente...
Loading python/3.11.6--gcc--8.5.0
  Loading requirement: bzip2/1.0.8-gp5wcz5 libmd/1.0.4-wja3f5q
    libbsd/0.11.7-cgxjopl expat/2.5.0-bptl3xw ncurses/6.4-asx3jea
    readline/8.2-nyw6mp6 gdbm/1.23-fs6otck libiconv/1.17-d7yvx2s
    xz/5.4.1-hubmwr5 zlib-ng/2.1.4-6htiapk libxml2/2.10.3-5eeeokp
    pigz/2.7-bopr5vp zstd/1.5.5-gawytfl tar/1.34-amqus5s gettext/0.22.3-2g7elif
    libffi/3.4.4-6r7brdq libxcrypt/4.4.35-ss2rzin sqlite/3.43.2
    util-linux-uuid/2.38.1-jkdi7kv
âœ” Python: Python 3.11.6
âœ” Virtual env: /leonardo/home/userexternal/smattiol/LLM-Mob-As-Mobility-Interpreter/llm
âœ” CUDA: Cuda compilation tools, release 12.3, V12.3.103

ğŸ” INFO SISTEMA:
name, memory.total [MiB], memory.used [MiB], utilization.gpu [%], temperature.gpu
NVIDIA A100-SXM-64GB, 65536 MiB, 2 MiB, 0 %, 43
CUDA_VISIBLE_DEVICES: 0
CPU cores fisici: 32
Memoria totale: 502Gi

âš™ï¸  Configurazione Ollama per parallelismo...
âœ” Versione Ollama: 0.3.14

ğŸ”§ Configurazione parallelismo ottimale...
ğŸ“Š Configurazione calcolata:
   - File workers: 4 (processi paralleli)
   - LLM threads: 8 (per processo)
   - Batch size: 50 utenti
   - Max parallelismo totale: 32 richieste LLM

ğŸš€ Avvio server Ollama ottimizzato per parallelismo...
âœ” Porta server: 39003
ğŸ§¹ Pulizia processi precedenti...
âœ” Server PID: 3022721
âœ” Log file: ollama_parallel.log

â³ Attesa avvio server (max 60s)...
âœ… Server operativo dopo 10s

ğŸ”¥ Preparazione modello per produzione parallela...
ğŸ“‹ Modelli disponibili:
  âœ” deepseek-coder:33b
  âœ” qwen2.5:72b
  âœ” mixtral:8x7b
  âœ” llama3.1:70b
  âœ” llama3.1:8b
âœ… Modello llama3.1:8b disponibile
ğŸ”¥ Warm-up modello con test request...
âœ… Modello caricato in memoria

ğŸ¯ AVVIO PRODUZIONE PARALLELA - PROCESSAMENTO COMPLETO
======================================================
ğŸ“ Directory risultati: /leonardo/home/userexternal/smattiol/LLM-Mob-As-Mobility-Interpreter/results/
ğŸ“ Directory logs: /leonardo/home/userexternal/smattiol/LLM-Mob-As-Mobility-Interpreter/logs/
ğŸš€ AVVIO SCRIPT PARALLELO...
ğŸ”§ Parametri di esecuzione:
   --parallel-files 4
   --parallel-llm 8
   --batch-size 50
   --append (riprende da dove interrotto)

2025-08-13 13:49:48,073 [MainProcess-MainThread] INFO: ğŸ”§ Config finale: 4 file workers, 8 LLM threads, batch size 50
2025-08-13 13:49:48,074 [MainProcess-MainThread] INFO: ğŸ”— Ollama host: http://127.0.0.1:39003
2025-08-13 13:49:48,074 [MainProcess-MainThread] ERROR: âŒ Setup Ollama fallito: name 'wait_for_ollama' is not defined
ğŸ–¥ï¸  HPC Info: 32 cores, 502.9GB RAM
âš™ï¸  Config: 4 file workers, 8 LLM threads

ğŸ‰ PRODUZIONE PARALLELA COMPLETATA CON SUCCESSO!
â±ï¸  Tempo totale Python: 3 secondi (0h 0m)
ğŸš€ Speedup stimato: 4x rispetto a versione seriale

ğŸ“‹ REPORT FINALE PRODUZIONE PARALLELA
=====================================
â±ï¸  Tempo totale job: 51 secondi (0h 0m)
ğŸ”§ Versione Ollama: 0.3.14
ğŸ“Š ModalitÃ : Produzione parallela completa
/var/spool/slurmd/job18855374/slurm_script: line 389: 3022825 Terminated              monitor_progress
ğŸ”¢ Configurazione parallelismo:
   - File workers utilizzati: 4
   - LLM threads per worker: 8
   - Batch size: 50
âœ… Python success: true

ğŸ“„ RISULTATI GENERATI:
   ğŸ“Š Files totali: 0
   ğŸ“ Righe totali processate: ~0
   ğŸ’¾ Dimensione totale: 4.0K

   ğŸ“‹ Ultimi file generati:

ğŸ”§ STATO FINALE SISTEMA:
GPU:
memory.used [MiB], memory.total [MiB], utilization.gpu [%], temperature.gpu
8170 MiB, 65536 MiB, 0 %, 43

CPU e Memoria:
              total        used        free      shared  buff/cache   available
Mem:          502Gi       6.0Gi       482Gi       2.9Gi        14Gi       485Gi
Swap:            0B          0B          0B

 13:49:48 up 27 days, 20:13,  0 users,  load average: 1.57, 3.17, 3.70

ğŸ“Š LOG OLLAMA (ultimi 30 righe):
llm_load_print_meta: EOM token        = 128008 '<|eom_id|>'
llm_load_print_meta: EOG token        = 128008 '<|eom_id|>'
llm_load_print_meta: EOG token        = 128009 '<|eot_id|>'
llm_load_print_meta: max token length = 256
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A100-SXM-64GB, compute capability 8.0, VMM: yes
llm_load_tensors: ggml ctx size =    0.27 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
llm_load_tensors:        CPU buffer size =   281.81 MiB
llm_load_tensors:      CUDA0 buffer size =  4403.50 MiB
llama_new_context_with_model: n_ctx      = 16384
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: flash_attn = 0
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =  2048.00 MiB
llama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =     4.04 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =  1088.00 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =    40.01 MiB
llama_new_context_with_model: graph nodes  = 1030
llama_new_context_with_model: graph splits = 2
INFO [main] model loaded | tid="22654283812864" timestamp=1755085784
time=2025-08-13T13:49:45.033+02:00 level=INFO source=server.go:626 msg="llama runner started in 5.52 seconds"
[GIN] 2025/08/13 - 13:49:45 | 200 |  6.028892721s |       127.0.0.1 | POST     "/api/chat"

ğŸ“Š LOG PYTHON (ultimi errori/warning):
2025-08-13 12:02:01,522 [MainProcess-MainThread] ERROR: âŒ Setup Ollama fallito: name 'wait_for_ollama' is not defined
2025-08-13 13:49:48,074 [MainProcess-MainThread] ERROR: âŒ Setup Ollama fallito: name 'wait_for_ollama' is not defined

ğŸ“ˆ ANALISI PERFORMANCE:

ğŸ‰ JOB PRODUZIONE PARALLELA COMPLETATO CON SUCCESSO!
âœ… Tutti i dataset processati con ottimizzazione parallela
ğŸš€ Performance migliorate di ~4x

ğŸ• Fine job: Wed Aug 13 13:49:48 CEST 2025
======================================================

ğŸ§¹ CLEANUP PRODUZIONE PARALLELA...
ğŸ• Tempo totale job: 51 secondi (0h 0m)
ğŸ“Š Stato GPU finale:
memory.used [MiB], memory.total [MiB], utilization.gpu [%], temperature.gpu
8170 MiB, 65536 MiB, 0 %, 43
ğŸ“Š Utilizzo CPU finale:
top - 13:49:48 up 27 days, 20:13,  0 users,  load average: 1.57, 3.17, 3.70
Tasks: 627 total,   1 running, 626 sleeping,   0 stopped,   0 zombie
%Cpu(s):  0.0 us,  0.2 sy,  0.0 ni, 99.8 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st
MiB Mem : 514982.7 total, 494398.4 free,   6160.0 used,  14424.3 buff/cache
MiB Swap:      0.0 total,      0.0 free,      0.0 used. 497301.5 avail Mem 
ğŸ“„ Risultati generati: 0 files
ğŸ’¾ Dimensione risultati: 4.0K
ğŸ“Š Dettaglio per file:
ğŸ”„ Shutdown graceful Ollama...
âœ” Cleanup completato
