ğŸŒ HTTP Requests (8 simultanee)
                           â†“
                    ğŸ“¥ Request Queue  
                           â†“
                    ğŸ§  Llama Model (in VRAM)
                    â”œâ”€â”€ Tokenizer
                    â”œâ”€â”€ Embedding Layer
                    â”œâ”€â”€ Transformer Blocks (32 layers)
                    â”‚   â”œâ”€â”€ Multi-Head Attention (batch parallel)
                    â”‚   â””â”€â”€ Feed Forward (batch parallel)  
                    â””â”€â”€ Output Head
                           â†“
                    ğŸ“¤ Response Dispatcher
                           â†“
                    ğŸŒ HTTP Responses (8 simultanee)