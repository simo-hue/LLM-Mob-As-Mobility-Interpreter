LLM) [smattiol@login07 LLM-Mob-As-Mobility-Interpreter]$ tail -f slurm-18161325.out
Loading python/3.11.6--gcc--8.5.0
  Loading requirement: bzip2/1.0.8-gp5wcz5 libmd/1.0.4-wja3f5q
    libbsd/0.11.7-cgxjopl expat/2.5.0-bptl3xw ncurses/6.4-asx3jea
    readline/8.2-nyw6mp6 gdbm/1.23-fs6otck libiconv/1.17-d7yvx2s
    xz/5.4.1-hubmwr5 zlib-ng/2.1.4-6htiapk libxml2/2.10.3-5eeeokp
    pigz/2.7-bopr5vp zstd/1.5.5-gawytfl tar/1.34-amqus5s gettext/0.22.3-2g7elif
    libffi/3.4.4-6r7brdq libxcrypt/4.4.35-ss2rzin sqlite/3.43.2
    util-linux-uuid/2.38.1-jkdi7kv
/var/spool/slurmd/job18161325/slurm_script: line 19: /leonardo/home/userexternal/smattiol/LLM-Mob-As-Mobility-Interpreter/llm/bin/activate: No such file or directory
=== VERIFICA DIPENDENZE ===
Traceback (most recent call last):
  File "<string>", line 1, in <module>
ModuleNotFoundError: No module named 'pandas'
✗ pandas mancante
Traceback (most recent call last):
  File "<string>", line 1, in <module>
ModuleNotFoundError: No module named 'numpy'
✗ numpy mancante
Traceback (most recent call last):
  File "<string>", line 1, in <module>
ModuleNotFoundError: No module named 'requests'
✗ requests mancante
Traceback (most recent call last):
  File "<string>", line 1, in <module>
ModuleNotFoundError: No module named 'sklearn'
✗ scikit-learn mancante
Traceback (most recent call last):
  File "<string>", line 1, in <module>
ModuleNotFoundError: No module named 'tqdm'
✗ tqdm mancante
============================
=== CONFIGURAZIONE GPU ===
CUDA_VISIBLE_DEVICES: 0
name, memory.total [MiB], memory.free [MiB]
NVIDIA A100-SXM-64GB, 65536 MiB, 64942 MiB
==========================
Avvio Ollama server...
time=2025-08-01T16:23:34.319+02:00 level=INFO source=routes.go:1235 msg="server config" env="map[CUDA_VISIBLE_DEVICES:0 GPU_DEVICE_ORDINAL:0 HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/leonardo/home/userexternal/smattiol/.ollama/models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:0 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES:0 http_proxy: https_proxy: no_proxy:]"
time=2025-08-01T16:23:34.324+02:00 level=INFO source=images.go:476 msg="total blobs: 5"
time=2025-08-01T16:23:34.325+02:00 level=INFO source=images.go:483 msg="total unused blobs removed: 0"
time=2025-08-01T16:23:34.327+02:00 level=INFO source=routes.go:1288 msg="Listening on 127.0.0.1:11434 (version 0.9.3)"
time=2025-08-01T16:23:34.327+02:00 level=INFO source=gpu.go:217 msg="looking for compatible GPUs"
time=2025-08-01T16:23:34.501+02:00 level=INFO source=types.go:130 msg="inference compute" id=GPU-0826c9a6-2031-a4bb-1f18-cc36bbb7622e library=cuda variant=v12 compute=8.0 driver=12.2 name="NVIDIA A100-SXM-64GB" total="63.4 GiB" available="63.0 GiB"
Test connessione server...
[GIN] 2025/08/01 - 16:23:42 | 200 |    1.898728ms |       127.0.0.1 | GET      "/api/tags"
✓ Server pronto
Lancio script Python alle Fri Aug  1 16:23:42 CEST 2025...
Traceback (most recent call last):
  File "/leonardo/home/userexternal/smattiol/LLM-Mob-As-Mobility-Interpreter/veronacard_mob_with_geom.py", line 5, in <module>
    import pandas as pd
ModuleNotFoundError: No module named 'pandas'
Chiusura Ollama alle Fri Aug  1 16:23:42 CEST 2025...
Job completato!