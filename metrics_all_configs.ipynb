{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ad36ff5",
   "metadata": {},
   "source": [
    "# üìä Multi-Config Metrics Evaluation\n",
    "Questo notebook analizza tutte le configurazioni (`top1`, `top10`, `top10_wot`) per entrambi i dataset (`geolife`, `fsq`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7084e0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import ast\n",
    "import json\n",
    "import logging\n",
    "from sklearn.metrics import f1_score\n",
    "from pathlib import Path\n",
    "\n",
    "# Logger\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(\"multi_metrics\")\n",
    "\n",
    "# Funzione per parse sicuro della predizione\n",
    "def safe_parse_prediction(x):\n",
    "    try:\n",
    "        if isinstance(x, str):\n",
    "            return ast.literal_eval(x)\n",
    "        elif isinstance(x, list):\n",
    "            return x\n",
    "        else:\n",
    "            return []\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"‚ö†Ô∏è Parsing fallito per predizione: {x} con errore: {e}\")\n",
    "        return []\n",
    "\n",
    "# Funzione per analizzare una singola configurazione\n",
    "def analyze_config(path):\n",
    "    logger.info(f\"üìÇ Analisi cartella: {path}\")\n",
    "    \n",
    "    file_list = sorted([f for f in os.listdir(path) if f.endswith(\".csv\")])\n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    for file in file_list:\n",
    "        file_path = os.path.join(path, file)\n",
    "        try:\n",
    "            logger.info(f\"Leggo: {file_path}\")\n",
    "            temp_df = pd.read_csv(file_path)\n",
    "            df = pd.concat([df, temp_df], ignore_index=True)\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"‚ùå Errore lettura {file_path}: {e}\")\n",
    "    \n",
    "    if df.empty:\n",
    "        logger.warning(f\"Nessun dato trovato in {path}\")\n",
    "        return None\n",
    "\n",
    "    # Parsing e metrica\n",
    "    df['prediction'] = df['prediction'].apply(safe_parse_prediction)\n",
    "    df['hit@10'] = df.apply(lambda row: row['ground_truth'] in row['prediction'], axis=1)\n",
    "    acc_at_10 = df['hit@10'].mean()\n",
    "\n",
    "    # Predizione principale per F1\n",
    "    majority_preds = [row[0] if isinstance(row, list) and row else -1 for row in df['prediction']]\n",
    "    try:\n",
    "        df['ground_truth'] = df['ground_truth'].astype(int)\n",
    "        majority_preds = [int(p) for p in majority_preds]\n",
    "        f1 = f1_score(df['ground_truth'], majority_preds, average='macro')\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"‚ö†Ô∏è Errore F1 Score: {e}\")\n",
    "        f1 = 0.0\n",
    "\n",
    "    # Grafico\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.barplot(x=['Acc@10', 'F1'], y=[acc_at_10, f1])\n",
    "    plt.title(f\"üìä Metriche - {path}\")\n",
    "    plt.ylim(0, 1)\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"‚úÖ Acc@10: {round(acc_at_10, 4)}\")\n",
    "    print(f\"‚úÖ F1 Score: {round(f1, 4)}\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436c802b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# üìÅ Percorsi da analizzare\n",
    "configs = [\n",
    "    \"output/geolife/top1\",\n",
    "    \"output/geolife/top10\",\n",
    "    \"output/geolife/top10_wot\",\n",
    "    \"output/fsq/top1\",\n",
    "    \"output/fsq/top10\",\n",
    "    \"output/fsq/top10_wot\",\n",
    "]\n",
    "\n",
    "# üöÄ Avvia analisi\n",
    "for path in configs:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    analyze_config(path)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
