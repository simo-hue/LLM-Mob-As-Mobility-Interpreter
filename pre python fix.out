üöÄ LLM-MOB CON OLLAMA 0.3.14 AGGIORNATO
========================================
Job ID: 18548786
Nodo: lrdn0551.leonardo.local
Data: Wed Aug  6 08:13:24 CEST 2025
Loading python/3.11.6--gcc--8.5.0
  Loading requirement: bzip2/1.0.8-gp5wcz5 libmd/1.0.4-wja3f5q
    libbsd/0.11.7-cgxjopl expat/2.5.0-bptl3xw ncurses/6.4-asx3jea
    readline/8.2-nyw6mp6 gdbm/1.23-fs6otck libiconv/1.17-d7yvx2s
    xz/5.4.1-hubmwr5 zlib-ng/2.1.4-6htiapk libxml2/2.10.3-5eeeokp
    pigz/2.7-bopr5vp zstd/1.5.5-gawytfl tar/1.34-amqus5s gettext/0.22.3-2g7elif
    libffi/3.4.4-6r7brdq libxcrypt/4.4.35-ss2rzin sqlite/3.43.2
    util-linux-uuid/2.38.1-jkdi7kv
üîç INFO GPU:
name, memory.total [MiB], memory.used [MiB], utilization.gpu [%]
NVIDIA A100-SXM-64GB, 65536 MiB, 2 MiB, 0 %
CUDA_VISIBLE_DEVICES: 0
üì¶ Versione Ollama: 0.3.14
‚úÖ Porta: 39003
üöÄ Avvio server Ollama 0.3.x...
   Comando: OLLAMA_HOST=127.0.0.1:39003 /leonardo/home/userexternal/smattiol/opt/bin/ollama serve
   PID server: 2252544
‚è≥ Attesa server...
‚úÖ Server attivo dopo 10s
üì• Verifica modello...
Modelli disponibili:
  - llama3.1:8b
‚úÖ Modello llama3.1:8b gi√† disponibile
üß™ Test finale inferenza...
   Payload test: {
    "model": "llama3.1:8b",
    "prompt": "Hello",
    "stream": false,
    "options": {
        "num_predict": 5,
        "temperature": 0.1
    }
}
‚úÖ Test riuscito! Response: "Hello! How are you"
üéâ OLLAMA FUNZIONA CORRETTAMENTE!
‚úÖ Ollama pronto per lo script Python
üêç Preparazione script Python...
‚úÖ Uso modalit√† normale
üêç Avvio script Python...
üëâ Porta letta da ollama_port.txt: '39003'
üëâ Provo a contattare http://127.0.0.1:39003/api/tags
üìÇ Working dir: /leonardo/home/userexternal/smattiol/LLM-Mob-As-Mobility-Interpreter
üìÑ Contenuto di ollama_port.txt: '39003'
Traceback (most recent call last):
  File "veronacard_mob_with_geom.py", line 484, in <module>
    def list_outputs(visits_path: Path, out_dir: Path) -> list[Path]:
TypeError: 'type' object is not subscriptable
‚ùå Script Python fallito (exit code: 1)

üìä STATISTICHE FINALI
---------------------
Durata job: 45 secondi
Ollama version: 0.3.14
Ollama ready: true
Python success: false

Risultati generati:

Memoria GPU finale:
5548 MiB, 65536 MiB, 0 %

--- STATO PROCESSO OLLAMA ---
Server ancora attivo

--- ULTIMI LOG OLLAMA ---
DEBUG [update_slots] all slots are idle and system prompt is empty, clear the KV cache | tid="22888416935936" timestamp=1754460846
DEBUG [process_single_task] slot data | n_idle_slots=1 n_processing_slots=0 task_id=0 tid="22888416935936" timestamp=1754460847
time=2025-08-06T08:14:07.066+02:00 level=INFO source=server.go:626 msg="llama runner started in 5.52 seconds"
time=2025-08-06T08:14:07.066+02:00 level=DEBUG source=sched.go:462 msg="finished setting up runner" model=/leonardo/home/userexternal/smattiol/.ollama/models/blobs/sha256-667b0c1932bc6ffc593ed1d03f895bf2dc8dc6df21db3042284a6f4416b06a29
time=2025-08-06T08:14:07.067+02:00 level=DEBUG source=routes.go:239 msg="generate request" prompt="<|start_header_id|>user<|end_header_id|>\n\nHello<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n" images=[]
DEBUG [process_single_task] slot data | n_idle_slots=1 n_processing_slots=0 task_id=1 tid="22888416935936" timestamp=1754460847
DEBUG [launch_slot_with_data] slot is processing task | slot_id=0 task_id=2 tid="22888416935936" timestamp=1754460847
DEBUG [update_slots] slot progression | ga_i=0 n_past=0 n_past_se=0 n_prompt_tokens_processed=11 slot_id=0 task_id=2 tid="22888416935936" timestamp=1754460847
DEBUG [update_slots] kv cache rm [p0, end) | p0=0 slot_id=0 task_id=2 tid="22888416935936" timestamp=1754460847
DEBUG [print_timings] prompt eval time     =      21.93 ms /    11 tokens (    1.99 ms per token,   501.60 tokens per second) | n_prompt_tokens_processed=11 n_tokens_second=501.5959872321022 slot_id=0 t_prompt_processing=21.93 t_token=1.9936363636363637 task_id=2 tid="22888416935936" timestamp=1754460847
DEBUG [print_timings] generation eval time =      42.20 ms /     5 runs   (    8.44 ms per token,   118.49 tokens per second) | n_decoded=5 n_tokens_second=118.4946440420893 slot_id=0 t_token=8.4392 t_token_generation=42.196 task_id=2 tid="22888416935936" timestamp=1754460847
DEBUG [print_timings]           total time =      64.13 ms | slot_id=0 t_prompt_processing=21.93 t_token_generation=42.196 t_total=64.126 task_id=2 tid="22888416935936" timestamp=1754460847
DEBUG [update_slots] slot released | n_cache_tokens=16 n_ctx=2048 n_past=15 n_system_tokens=0 slot_id=0 task_id=2 tid="22888416935936" timestamp=1754460847 truncated=false
DEBUG [log_server_request] request | method="POST" params={} path="/completion" remote_addr="127.0.0.1" remote_port=59896 status=200 tid="22888404234240" timestamp=1754460847
DEBUG [process_single_task] slot data | n_idle_slots=1 n_processing_slots=0 task_id=10 tid="22888416935936" timestamp=1754460847
DEBUG [log_server_request] request | method="POST" params={} path="/tokenize" remote_addr="127.0.0.1" remote_port=59902 status=200 tid="22888402132992" timestamp=1754460847
[GIN] 2025/08/06 - 08:14:07 | 200 |    5.8693387s |       127.0.0.1 | POST     "/api/generate"
time=2025-08-06T08:14:07.175+02:00 level=DEBUG source=sched.go:466 msg="context for request finished"
time=2025-08-06T08:14:07.176+02:00 level=DEBUG source=sched.go:339 msg="runner with non-zero duration has gone idle, adding timer" modelPath=/leonardo/home/userexternal/smattiol/.ollama/models/blobs/sha256-667b0c1932bc6ffc593ed1d03f895bf2dc8dc6df21db3042284a6f4416b06a29 duration=30m0s
time=2025-08-06T08:14:07.176+02:00 level=DEBUG source=sched.go:357 msg="after processing request finished event" modelPath=/leonardo/home/userexternal/smattiol/.ollama/models/blobs/sha256-667b0c1932bc6ffc593ed1d03f895bf2dc8dc6df21db3042284a6f4416b06a29 refCount=0

‚ö†Ô∏è JOB COMPLETATO CON ERRORI
üßπ Cleanup server...
