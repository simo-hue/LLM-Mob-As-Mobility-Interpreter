2025/09/13 16:32:01 routes.go:1158: INFO server config env="map[CUDA_VISIBLE_DEVICES:0 GPU_DEVICE_ORDINAL:0,1 HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_DEBUG:true OLLAMA_FLASH_ATTENTION:true OLLAMA_GPU_OVERHEAD:1024 OLLAMA_HOST:http://127.0.0.1:39001 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:8h0m0s OLLAMA_LLM_LIBRARY:cuda_v12 OLLAMA_LOAD_TIMEOUT:30m0s OLLAMA_MAX_LOADED_MODELS:1 OLLAMA_MAX_QUEUE:10 OLLAMA_MODELS:/leonardo_work/IscrC_LLM-Mob/.ollama/models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://*] OLLAMA_SCHED_SPREAD:false OLLAMA_TMPDIR:/leonardo_work/IscrC_LLM-Mob/tmp_ollama_19945716 ROCR_VISIBLE_DEVICES:0,1 http_proxy: https_proxy: no_proxy:]"
time=2025-09-13T16:32:01.998+02:00 level=INFO source=images.go:754 msg="total blobs: 42"
time=2025-09-13T16:32:02.127+02:00 level=INFO source=images.go:761 msg="total unused blobs removed: 0"
time=2025-09-13T16:32:02.169+02:00 level=INFO source=routes.go:1205 msg="Listening on 127.0.0.1:39001 (version 0.3.14)"
time=2025-09-13T16:32:02.178+02:00 level=INFO source=common.go:135 msg="extracting embedded files" dir=/leonardo_work/IscrC_LLM-Mob/tmp_ollama_19945716/ollama696725503/runners
time=2025-09-13T16:32:02.193+02:00 level=DEBUG source=common.go:168 msg=extracting runner=cpu payload=linux/amd64/cpu/libggml.so.gz
time=2025-09-13T16:32:02.194+02:00 level=DEBUG source=common.go:168 msg=extracting runner=cpu payload=linux/amd64/cpu/libllama.so.gz
time=2025-09-13T16:32:02.194+02:00 level=DEBUG source=common.go:168 msg=extracting runner=cpu payload=linux/amd64/cpu/ollama_llama_server.gz
time=2025-09-13T16:32:02.194+02:00 level=DEBUG source=common.go:168 msg=extracting runner=cpu_avx payload=linux/amd64/cpu_avx/libggml.so.gz
time=2025-09-13T16:32:02.194+02:00 level=DEBUG source=common.go:168 msg=extracting runner=cpu_avx payload=linux/amd64/cpu_avx/libllama.so.gz
time=2025-09-13T16:32:02.194+02:00 level=DEBUG source=common.go:168 msg=extracting runner=cpu_avx payload=linux/amd64/cpu_avx/ollama_llama_server.gz
time=2025-09-13T16:32:02.194+02:00 level=DEBUG source=common.go:168 msg=extracting runner=cpu_avx2 payload=linux/amd64/cpu_avx2/libggml.so.gz
time=2025-09-13T16:32:02.194+02:00 level=DEBUG source=common.go:168 msg=extracting runner=cpu_avx2 payload=linux/amd64/cpu_avx2/libllama.so.gz
time=2025-09-13T16:32:02.194+02:00 level=DEBUG source=common.go:168 msg=extracting runner=cpu_avx2 payload=linux/amd64/cpu_avx2/ollama_llama_server.gz
time=2025-09-13T16:32:02.194+02:00 level=DEBUG source=common.go:168 msg=extracting runner=cuda_v11 payload=linux/amd64/cuda_v11/libggml.so.gz
time=2025-09-13T16:32:02.194+02:00 level=DEBUG source=common.go:168 msg=extracting runner=cuda_v11 payload=linux/amd64/cuda_v11/libllama.so.gz
time=2025-09-13T16:32:02.194+02:00 level=DEBUG source=common.go:168 msg=extracting runner=cuda_v11 payload=linux/amd64/cuda_v11/ollama_llama_server.gz
time=2025-09-13T16:32:02.194+02:00 level=DEBUG source=common.go:168 msg=extracting runner=cuda_v12 payload=linux/amd64/cuda_v12/libggml.so.gz
time=2025-09-13T16:32:02.194+02:00 level=DEBUG source=common.go:168 msg=extracting runner=cuda_v12 payload=linux/amd64/cuda_v12/libllama.so.gz
time=2025-09-13T16:32:02.194+02:00 level=DEBUG source=common.go:168 msg=extracting runner=cuda_v12 payload=linux/amd64/cuda_v12/ollama_llama_server.gz
time=2025-09-13T16:32:02.194+02:00 level=DEBUG source=common.go:168 msg=extracting runner=rocm_v60102 payload=linux/amd64/rocm_v60102/libggml.so.gz
time=2025-09-13T16:32:02.194+02:00 level=DEBUG source=common.go:168 msg=extracting runner=rocm_v60102 payload=linux/amd64/rocm_v60102/libllama.so.gz
time=2025-09-13T16:32:02.194+02:00 level=DEBUG source=common.go:168 msg=extracting runner=rocm_v60102 payload=linux/amd64/rocm_v60102/ollama_llama_server.gz
time=2025-09-13T16:32:50.363+02:00 level=DEBUG source=common.go:294 msg="availableServers : found" file=/leonardo_work/IscrC_LLM-Mob/tmp_ollama_19945716/ollama696725503/runners/cpu/ollama_llama_server
time=2025-09-13T16:32:50.363+02:00 level=DEBUG source=common.go:294 msg="availableServers : found" file=/leonardo_work/IscrC_LLM-Mob/tmp_ollama_19945716/ollama696725503/runners/cpu_avx/ollama_llama_server
time=2025-09-13T16:32:50.363+02:00 level=DEBUG source=common.go:294 msg="availableServers : found" file=/leonardo_work/IscrC_LLM-Mob/tmp_ollama_19945716/ollama696725503/runners/cpu_avx2/ollama_llama_server
time=2025-09-13T16:32:50.363+02:00 level=DEBUG source=common.go:294 msg="availableServers : found" file=/leonardo_work/IscrC_LLM-Mob/tmp_ollama_19945716/ollama696725503/runners/cuda_v11/ollama_llama_server
time=2025-09-13T16:32:50.363+02:00 level=DEBUG source=common.go:294 msg="availableServers : found" file=/leonardo_work/IscrC_LLM-Mob/tmp_ollama_19945716/ollama696725503/runners/cuda_v12/ollama_llama_server
time=2025-09-13T16:32:50.363+02:00 level=DEBUG source=common.go:294 msg="availableServers : found" file=/leonardo_work/IscrC_LLM-Mob/tmp_ollama_19945716/ollama696725503/runners/rocm_v60102/ollama_llama_server
time=2025-09-13T16:32:50.363+02:00 level=INFO source=common.go:49 msg="Dynamic LLM libraries" runners="[cpu_avx cpu_avx2 cuda_v11 cuda_v12 rocm_v60102 cpu]"
time=2025-09-13T16:32:50.363+02:00 level=DEBUG source=common.go:50 msg="Override detection logic by setting OLLAMA_LLM_LIBRARY"
time=2025-09-13T16:32:50.363+02:00 level=DEBUG source=sched.go:105 msg="starting llm scheduler"
time=2025-09-13T16:32:50.376+02:00 level=INFO source=gpu.go:221 msg="looking for compatible GPUs"
time=2025-09-13T16:32:50.426+02:00 level=DEBUG source=gpu.go:94 msg="searching for GPU discovery libraries for NVIDIA"
time=2025-09-13T16:32:50.434+02:00 level=DEBUG source=gpu.go:505 msg="Searching for GPU library" name=libcuda.so*
time=2025-09-13T16:32:50.434+02:00 level=DEBUG source=gpu.go:528 msg="gpu library search" globs="[/leonardo_work/IscrC_LLM-Mob/opt/lib/ollama/libcuda.so* /leonardo/prod/opt/compilers/cuda/12.3/none/lib64/libcuda.so* /leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/python-3.11.6-i5k3c6ggftqkzgqyymfbkynpgm2lgjtd/lib/libcuda.so* /leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/util-linux-uuid-2.38.1-jkdi7kvvma7367qdmvpkada4pyiafoud/lib/libcuda.so* /leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/sqlite-3.43.2-casyrltocz5edzjhs5vzlqhwkamn7y4a/lib/libcuda.so* /leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/libxcrypt-4.4.35-ss2rzin25ozjy4gyy3dack36njs6navg/lib/libcuda.so* /leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/libffi-3.4.4-6r7brdq5dnreoad6f7sn7ybjjvwdmvue/lib64/libcuda.so* /leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/libffi-3.4.4-6r7brdq5dnreoad6f7sn7ybjjvwdmvue/lib/libcuda.so* /leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/gettext-0.22.3-2g7elifkgxzpypbswqnjuu5hefn4mjts/lib/libcuda.so* /leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/zstd-1.5.5-gawytflrhedqdc2riwax7oduoqddx22s/lib/libcuda.so* /leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/libxml2-2.10.3-5eeeokp4kszufozbayq4bewwmyeuwy27/lib/libcuda.so* /leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/zlib-ng-2.1.4-6htiapkoa6fx2medhyabzo575skozuir/lib/libcuda.so* /leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/xz-5.4.1-hubmwr5wc5nf6zk3ghuaikxiejuyt6bi/lib/libcuda.so* /leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/libiconv-1.17-d7yvx2s6da4x2rfx44bc3perbb33rvuy/lib/libcuda.so* /leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/gdbm-1.23-fs6otcki47azeywcckquj2sy4mzsnzxg/lib/libcuda.so* /leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/readline-8.2-nyw6mp6b7dvizewrf7exopvap2q32s5j/lib/libcuda.so* /leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/ncurses-6.4-asx3jea367shsxjt6bdj2bu5olxll6ni/lib/libcuda.so* /leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/expat-2.5.0-bptl3xwbvbkxxoc5x3dhviorarv4dvxv/lib/libcuda.so* /leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/libbsd-0.11.7-cgxjopleu7se4y4cgi7oefbljgasr457/lib/libcuda.so* /leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/libmd-1.0.4-wja3f5q3w75tqtro333fptfnji7oqxiu/lib/libcuda.so* /leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/bzip2-1.0.8-gp5wcz5lksrbm2gqiqjppumrhjz6gahy/lib/libcuda.so* /usr/local/cuda*/targets/*/lib/libcuda.so* /usr/lib/*-linux-gnu/nvidia/current/libcuda.so* /usr/lib/*-linux-gnu/libcuda.so* /usr/lib/wsl/lib/libcuda.so* /usr/lib/wsl/drivers/*/libcuda.so* /opt/cuda/lib*/libcuda.so* /usr/local/cuda/lib*/libcuda.so* /usr/lib*/libcuda.so* /usr/local/lib*/libcuda.so*]"
time=2025-09-13T16:32:50.451+02:00 level=DEBUG source=gpu.go:562 msg="discovered GPU libraries" paths=[/usr/lib64/libcuda.so.535.54.03]
CUDA driver version: 12.2
time=2025-09-13T16:32:50.504+02:00 level=DEBUG source=gpu.go:129 msg="detected GPUs" count=1 library=/usr/lib64/libcuda.so.535.54.03
[GPU-8efa7ef6-1cbc-ec7b-6246-cd6e2bf7d1d4] CUDA totalMem 64944 mb
[GPU-8efa7ef6-1cbc-ec7b-6246-cd6e2bf7d1d4] CUDA freeMem 64462 mb
[GPU-8efa7ef6-1cbc-ec7b-6246-cd6e2bf7d1d4] Compute Capability 8.0
time=2025-09-13T16:32:50.651+02:00 level=DEBUG source=amd_linux.go:416 msg="amdgpu driver not detected /sys/module/amdgpu"
releasing cuda driver library
time=2025-09-13T16:32:50.651+02:00 level=INFO source=types.go:123 msg="inference compute" id=GPU-8efa7ef6-1cbc-ec7b-6246-cd6e2bf7d1d4 library=cuda variant=v12 compute=8.0 driver=12.2 name="NVIDIA A100-SXM-64GB" total="63.4 GiB" available="63.0 GiB"
[GIN] 2025/09/13 - 16:32:50 | 200 |  167.327294ms |       127.0.0.1 | GET      "/api/tags"
time=2025-09-13T16:32:51.111+02:00 level=DEBUG source=gpu.go:396 msg="updating system memory data" before.total="502.9 GiB" before.free="490.9 GiB" before.free_swap="0 B" now.total="502.9 GiB" now.free="490.9 GiB" now.free_swap="0 B"
CUDA driver version: 12.2
time=2025-09-13T16:32:51.196+02:00 level=DEBUG source=gpu.go:444 msg="updating cuda memory data" gpu=GPU-8efa7ef6-1cbc-ec7b-6246-cd6e2bf7d1d4 name="NVIDIA A100-SXM-64GB" overhead="0 B" before.total="63.4 GiB" before.free="63.0 GiB" now.total="63.4 GiB" now.free="63.0 GiB" now.used="482.8 MiB"
releasing cuda driver library
time=2025-09-13T16:32:51.226+02:00 level=DEBUG source=sched.go:224 msg="loading first model" model=/leonardo_work/IscrC_LLM-Mob/.ollama/models/blobs/sha256-2bada8a7450677000f678be90653b85d364de7db25eb5ea54136ada5f3933730
time=2025-09-13T16:32:51.228+02:00 level=DEBUG source=memory.go:103 msg=evaluating library=cuda gpu_count=1 available="[63.0 GiB]"
time=2025-09-13T16:32:51.228+02:00 level=INFO source=sched.go:714 msg="new model will fit in available VRAM in single GPU, loading" model=/leonardo_work/IscrC_LLM-Mob/.ollama/models/blobs/sha256-2bada8a7450677000f678be90653b85d364de7db25eb5ea54136ada5f3933730 gpu=GPU-8efa7ef6-1cbc-ec7b-6246-cd6e2bf7d1d4 parallel=1 available=67593371648 required="4.9 GiB"
time=2025-09-13T16:32:51.229+02:00 level=DEBUG source=gpu.go:396 msg="updating system memory data" before.total="502.9 GiB" before.free="490.9 GiB" before.free_swap="0 B" now.total="502.9 GiB" now.free="490.9 GiB" now.free_swap="0 B"
CUDA driver version: 12.2
time=2025-09-13T16:32:51.314+02:00 level=DEBUG source=gpu.go:444 msg="updating cuda memory data" gpu=GPU-8efa7ef6-1cbc-ec7b-6246-cd6e2bf7d1d4 name="NVIDIA A100-SXM-64GB" overhead="0 B" before.total="63.4 GiB" before.free="63.0 GiB" now.total="63.4 GiB" now.free="63.0 GiB" now.used="482.8 MiB"
releasing cuda driver library
time=2025-09-13T16:32:51.314+02:00 level=INFO source=server.go:105 msg="system memory" total="502.9 GiB" free="490.9 GiB" free_swap="0 B"
time=2025-09-13T16:32:51.314+02:00 level=DEBUG source=memory.go:103 msg=evaluating library=cuda gpu_count=1 available="[63.0 GiB]"
time=2025-09-13T16:32:51.314+02:00 level=INFO source=memory.go:326 msg="offload to cuda" layers.requested=-1 layers.model=29 layers.offload=29 layers.split="" memory.available="[63.0 GiB]" memory.gpu_overhead="1.0 KiB" memory.required.full="4.9 GiB" memory.required.partial="4.9 GiB" memory.required.kv="56.0 MiB" memory.required.allocations="[4.9 GiB]" memory.weights.total="3.7 GiB" memory.weights.repeating="3.3 GiB" memory.weights.nonrepeating="426.4 MiB" memory.graph.full="152.0 MiB" memory.graph.partial="578.4 MiB"
time=2025-09-13T16:32:51.314+02:00 level=DEBUG source=common.go:168 msg=extracting runner=cpu payload=linux/amd64/cpu/libggml.so.gz
time=2025-09-13T16:32:51.314+02:00 level=DEBUG source=common.go:168 msg=extracting runner=cpu payload=linux/amd64/cpu/libllama.so.gz
time=2025-09-13T16:32:51.314+02:00 level=DEBUG source=common.go:168 msg=extracting runner=cpu payload=linux/amd64/cpu/ollama_llama_server.gz
time=2025-09-13T16:32:51.314+02:00 level=DEBUG source=common.go:168 msg=extracting runner=cpu_avx payload=linux/amd64/cpu_avx/libggml.so.gz
time=2025-09-13T16:32:51.314+02:00 level=DEBUG source=common.go:168 msg=extracting runner=cpu_avx payload=linux/amd64/cpu_avx/libllama.so.gz
time=2025-09-13T16:32:51.314+02:00 level=DEBUG source=common.go:168 msg=extracting runner=cpu_avx payload=linux/amd64/cpu_avx/ollama_llama_server.gz
time=2025-09-13T16:32:51.314+02:00 level=DEBUG source=common.go:168 msg=extracting runner=cpu_avx2 payload=linux/amd64/cpu_avx2/libggml.so.gz
time=2025-09-13T16:32:51.314+02:00 level=DEBUG source=common.go:168 msg=extracting runner=cpu_avx2 payload=linux/amd64/cpu_avx2/libllama.so.gz
time=2025-09-13T16:32:51.314+02:00 level=DEBUG source=common.go:168 msg=extracting runner=cpu_avx2 payload=linux/amd64/cpu_avx2/ollama_llama_server.gz
time=2025-09-13T16:32:51.314+02:00 level=DEBUG source=common.go:168 msg=extracting runner=cuda_v11 payload=linux/amd64/cuda_v11/libggml.so.gz
time=2025-09-13T16:32:51.314+02:00 level=DEBUG source=common.go:168 msg=extracting runner=cuda_v11 payload=linux/amd64/cuda_v11/libllama.so.gz
time=2025-09-13T16:32:51.314+02:00 level=DEBUG source=common.go:168 msg=extracting runner=cuda_v11 payload=linux/amd64/cuda_v11/ollama_llama_server.gz
time=2025-09-13T16:32:51.314+02:00 level=DEBUG source=common.go:168 msg=extracting runner=cuda_v12 payload=linux/amd64/cuda_v12/libggml.so.gz
time=2025-09-13T16:32:51.314+02:00 level=DEBUG source=common.go:168 msg=extracting runner=cuda_v12 payload=linux/amd64/cuda_v12/libllama.so.gz
time=2025-09-13T16:32:51.314+02:00 level=DEBUG source=common.go:168 msg=extracting runner=cuda_v12 payload=linux/amd64/cuda_v12/ollama_llama_server.gz
time=2025-09-13T16:32:51.314+02:00 level=DEBUG source=common.go:168 msg=extracting runner=rocm_v60102 payload=linux/amd64/rocm_v60102/libggml.so.gz
time=2025-09-13T16:32:51.314+02:00 level=DEBUG source=common.go:168 msg=extracting runner=rocm_v60102 payload=linux/amd64/rocm_v60102/libllama.so.gz
time=2025-09-13T16:32:51.314+02:00 level=DEBUG source=common.go:168 msg=extracting runner=rocm_v60102 payload=linux/amd64/rocm_v60102/ollama_llama_server.gz
time=2025-09-13T16:32:51.316+02:00 level=DEBUG source=common.go:294 msg="availableServers : found" file=/leonardo_work/IscrC_LLM-Mob/tmp_ollama_19945716/ollama696725503/runners/cpu/ollama_llama_server
time=2025-09-13T16:32:51.316+02:00 level=DEBUG source=common.go:294 msg="availableServers : found" file=/leonardo_work/IscrC_LLM-Mob/tmp_ollama_19945716/ollama696725503/runners/cpu_avx/ollama_llama_server
time=2025-09-13T16:32:51.316+02:00 level=DEBUG source=common.go:294 msg="availableServers : found" file=/leonardo_work/IscrC_LLM-Mob/tmp_ollama_19945716/ollama696725503/runners/cpu_avx2/ollama_llama_server
time=2025-09-13T16:32:51.316+02:00 level=DEBUG source=common.go:294 msg="availableServers : found" file=/leonardo_work/IscrC_LLM-Mob/tmp_ollama_19945716/ollama696725503/runners/cuda_v11/ollama_llama_server
time=2025-09-13T16:32:51.316+02:00 level=DEBUG source=common.go:294 msg="availableServers : found" file=/leonardo_work/IscrC_LLM-Mob/tmp_ollama_19945716/ollama696725503/runners/cuda_v12/ollama_llama_server
time=2025-09-13T16:32:51.316+02:00 level=DEBUG source=common.go:294 msg="availableServers : found" file=/leonardo_work/IscrC_LLM-Mob/tmp_ollama_19945716/ollama696725503/runners/rocm_v60102/ollama_llama_server
time=2025-09-13T16:32:51.318+02:00 level=DEBUG source=common.go:294 msg="availableServers : found" file=/leonardo_work/IscrC_LLM-Mob/tmp_ollama_19945716/ollama696725503/runners/cpu/ollama_llama_server
time=2025-09-13T16:32:51.318+02:00 level=DEBUG source=common.go:294 msg="availableServers : found" file=/leonardo_work/IscrC_LLM-Mob/tmp_ollama_19945716/ollama696725503/runners/cpu_avx/ollama_llama_server
time=2025-09-13T16:32:51.318+02:00 level=DEBUG source=common.go:294 msg="availableServers : found" file=/leonardo_work/IscrC_LLM-Mob/tmp_ollama_19945716/ollama696725503/runners/cpu_avx2/ollama_llama_server
time=2025-09-13T16:32:51.318+02:00 level=DEBUG source=common.go:294 msg="availableServers : found" file=/leonardo_work/IscrC_LLM-Mob/tmp_ollama_19945716/ollama696725503/runners/cuda_v11/ollama_llama_server
time=2025-09-13T16:32:51.318+02:00 level=DEBUG source=common.go:294 msg="availableServers : found" file=/leonardo_work/IscrC_LLM-Mob/tmp_ollama_19945716/ollama696725503/runners/cuda_v12/ollama_llama_server
time=2025-09-13T16:32:51.318+02:00 level=DEBUG source=common.go:294 msg="availableServers : found" file=/leonardo_work/IscrC_LLM-Mob/tmp_ollama_19945716/ollama696725503/runners/rocm_v60102/ollama_llama_server
time=2025-09-13T16:32:51.318+02:00 level=INFO source=server.go:172 msg="user override" OLLAMA_LLM_LIBRARY=cuda_v12 path=/leonardo_work/IscrC_LLM-Mob/tmp_ollama_19945716/ollama696725503/runners/cuda_v12
time=2025-09-13T16:32:51.335+02:00 level=INFO source=server.go:388 msg="starting llama server" cmd="/leonardo_work/IscrC_LLM-Mob/tmp_ollama_19945716/ollama696725503/runners/cuda_v12/ollama_llama_server --model /leonardo_work/IscrC_LLM-Mob/.ollama/models/blobs/sha256-2bada8a7450677000f678be90653b85d364de7db25eb5ea54136ada5f3933730 --ctx-size 1024 --batch-size 256 --embedding --n-gpu-layers 29 --verbose --threads 56 --flash-attn --parallel 1 --port 45425"
time=2025-09-13T16:32:51.335+02:00 level=DEBUG source=server.go:405 msg=subprocess environment="[CUDA_VISIBLE_DEVICES=GPU-8efa7ef6-1cbc-ec7b-6246-cd6e2bf7d1d4 LD_LIBRARY_PATH=/leonardo_work/IscrC_LLM-Mob/opt/lib/ollama:/leonardo_work/IscrC_LLM-Mob/tmp_ollama_19945716/ollama696725503/runners/cuda_v12:/leonardo/prod/opt/compilers/cuda/12.3/none/lib64:/leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/python-3.11.6-i5k3c6ggftqkzgqyymfbkynpgm2lgjtd/lib:/leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/util-linux-uuid-2.38.1-jkdi7kvvma7367qdmvpkada4pyiafoud/lib:/leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/sqlite-3.43.2-casyrltocz5edzjhs5vzlqhwkamn7y4a/lib:/leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/libxcrypt-4.4.35-ss2rzin25ozjy4gyy3dack36njs6navg/lib:/leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/libffi-3.4.4-6r7brdq5dnreoad6f7sn7ybjjvwdmvue/lib64:/leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/libffi-3.4.4-6r7brdq5dnreoad6f7sn7ybjjvwdmvue/lib:/leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/gettext-0.22.3-2g7elifkgxzpypbswqnjuu5hefn4mjts/lib:/leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/zstd-1.5.5-gawytflrhedqdc2riwax7oduoqddx22s/lib:/leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/libxml2-2.10.3-5eeeokp4kszufozbayq4bewwmyeuwy27/lib:/leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/zlib-ng-2.1.4-6htiapkoa6fx2medhyabzo575skozuir/lib:/leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/xz-5.4.1-hubmwr5wc5nf6zk3ghuaikxiejuyt6bi/lib:/leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/libiconv-1.17-d7yvx2s6da4x2rfx44bc3perbb33rvuy/lib:/leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/gdbm-1.23-fs6otcki47azeywcckquj2sy4mzsnzxg/lib:/leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/readline-8.2-nyw6mp6b7dvizewrf7exopvap2q32s5j/lib:/leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/ncurses-6.4-asx3jea367shsxjt6bdj2bu5olxll6ni/lib:/leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/expat-2.5.0-bptl3xwbvbkxxoc5x3dhviorarv4dvxv/lib:/leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/libbsd-0.11.7-cgxjopleu7se4y4cgi7oefbljgasr457/lib:/leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/libmd-1.0.4-wja3f5q3w75tqtro333fptfnji7oqxiu/lib:/leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/bzip2-1.0.8-gp5wcz5lksrbm2gqiqjppumrhjz6gahy/lib ROCR_VISIBLE_DEVICES=0,1 CUDA_HOME=/leonardo/prod/opt/compilers/cuda/12.3/none PATH=/leonardo_work/IscrC_LLM-Mob/venv/bin:/leonardo/prod/opt/compilers/cuda/12.3/none/cuda-samples/bin/x86_64/linux/release:/leonardo/prod/opt/compilers/cuda/12.3/none/bin:/leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/python-3.11.6-i5k3c6ggftqkzgqyymfbkynpgm2lgjtd/bin:/leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/util-linux-uuid-2.38.1-jkdi7kvvma7367qdmvpkada4pyiafoud/bin:/leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/sqlite-3.43.2-casyrltocz5edzjhs5vzlqhwkamn7y4a/bin:/leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/gettext-0.22.3-2g7elifkgxzpypbswqnjuu5hefn4mjts/bin:/leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/tar-1.34-amqus5s63wb5oknfz4pfnvdqtwwe76iw/bin:/leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/zstd-1.5.5-gawytflrhedqdc2riwax7oduoqddx22s/bin:/leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/pigz-2.7-bopr5vpavkyl3lou5sap6iydgi3udtcm/bin:/leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/libxml2-2.10.3-5eeeokp4kszufozbayq4bewwmyeuwy27/bin:/leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/xz-5.4.1-hubmwr5wc5nf6zk3ghuaikxiejuyt6bi/bin:/leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/libiconv-1.17-d7yvx2s6da4x2rfx44bc3perbb33rvuy/bin:/leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/gdbm-1.23-fs6otcki47azeywcckquj2sy4mzsnzxg/bin:/leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/readline-8.2-nyw6mp6b7dvizewrf7exopvap2q32s5j/bin:/leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/ncurses-6.4-asx3jea367shsxjt6bdj2bu5olxll6ni/bin:/leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/expat-2.5.0-bptl3xwbvbkxxoc5x3dhviorarv4dvxv/bin:/leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/bzip2-1.0.8-gp5wcz5lksrbm2gqiqjppumrhjz6gahy/bin:/leonardo_work/IscrC_LLM-Mob/venv/bin:/leonardo/home/userexternal/smattiol/opt/ollama/bin:/cineca/bin:/leonardo/home/userexternal/smattiol/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/bin/remote-cli:/leonardo/home/userexternal/smattiol/opt/ollama/bin:/cineca/bin:/leonardo/home/userexternal/smattiol/.nvm/versions/node/v22.18.0/bin:/leonardo/home/userexternal/smattiol/opt/ollama/bin:/leonardo/home/userexternal/smattiol/.local/bin:/leonardo/home/userexternal/smattiol/bin:/cineca/bin:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/environment-modules-5.2.0-rz47odw4phlhzhhbz7b65nv5s5othgmi/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/leonardo/home/userexternal/smattiol/.vscode-server/extensions/ms-python.debugpy-2025.10.0-linux-x64/bundled/scripts/noConfigScripts GPU_DEVICE_ORDINAL=0,1]"
time=2025-09-13T16:32:51.352+02:00 level=INFO source=sched.go:449 msg="loaded runners" count=1
time=2025-09-13T16:32:51.352+02:00 level=INFO source=server.go:587 msg="waiting for llama runner to start responding"
time=2025-09-13T16:32:51.352+02:00 level=INFO source=server.go:621 msg="waiting for server to become available" status="llm server error"
INFO [main] starting c++ runner | tid="22560398077952" timestamp=1757773976
INFO [main] build info | build=10 commit="3a8c75e" tid="22560398077952" timestamp=1757773976
INFO [main] system info | n_threads=56 n_threads_batch=56 system_info="AVX = 1 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 0 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 0 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | " tid="22560398077952" timestamp=1757773976 total_threads=32
INFO [main] HTTP server listening | hostname="127.0.0.1" n_threads_http="31" port="45425" tid="22560398077952" timestamp=1757773976
llama_model_loader: loaded meta data with 34 key-value pairs and 339 tensors from /leonardo_work/IscrC_LLM-Mob/.ollama/models/blobs/sha256-2bada8a7450677000f678be90653b85d364de7db25eb5ea54136ada5f3933730 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 7B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5
llama_model_loader: - kv   5:                         general.size_label str              = 7B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-7...
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 7B
llama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen
llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-7B
llama_model_loader: - kv  12:                               general.tags arr[str,2]       = ["chat", "text-generation"]
llama_model_loader: - kv  13:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  14:                          qwen2.block_count u32              = 28
llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768
llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  22:                          general.file_type u32              = 15
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  33:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
time=2025-09-13T16:32:56.868+02:00 level=INFO source=server.go:621 msg="waiting for server to become available" status="llm server loading model"
llm_load_vocab: special tokens cache size = 22
llm_load_vocab: token to piece cache size = 0.9310 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = qwen2
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 152064
llm_load_print_meta: n_merges         = 151387
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 32768
llm_load_print_meta: n_embd           = 3584
llm_load_print_meta: n_layer          = 28
llm_load_print_meta: n_head           = 28
llm_load_print_meta: n_head_kv        = 4
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 7
llm_load_print_meta: n_embd_k_gqa     = 512
llm_load_print_meta: n_embd_v_gqa     = 512
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 18944
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 1000000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 32768
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = ?B
llm_load_print_meta: model ftype      = Q4_K - Medium
llm_load_print_meta: model params     = 7.62 B
llm_load_print_meta: model size       = 4.36 GiB (4.91 BPW) 
llm_load_print_meta: general.name     = Qwen2.5 7B Instruct
llm_load_print_meta: BOS token        = 151643 '<|endoftext|>'
llm_load_print_meta: EOS token        = 151645 '<|im_end|>'
llm_load_print_meta: PAD token        = 151643 '<|endoftext|>'
llm_load_print_meta: LF token         = 148848 'ÄĬ'
llm_load_print_meta: EOT token        = 151643 '<|endoftext|>'
llm_load_print_meta: EOG token        = 151643 '<|endoftext|>'
llm_load_print_meta: EOG token        = 151645 '<|im_end|>'
llm_load_print_meta: max token length = 256
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A100-SXM-64GB, compute capability 8.0, VMM: yes
llm_load_tensors: ggml ctx size =    0.30 MiB
llm_load_tensors: offloading 28 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 29/29 layers to GPU
llm_load_tensors:        CPU buffer size =   292.36 MiB
llm_load_tensors:      CUDA0 buffer size =  4168.09 MiB
time=2025-09-13T16:33:07.656+02:00 level=DEBUG source=server.go:632 msg="model load progress 0.07"
time=2025-09-13T16:33:07.906+02:00 level=DEBUG source=server.go:632 msg="model load progress 0.72"
time=2025-09-13T16:33:08.211+02:00 level=DEBUG source=server.go:632 msg="model load progress 1.00"
llama_new_context_with_model: n_ctx      = 1024
llama_new_context_with_model: n_batch    = 256
llama_new_context_with_model: n_ubatch   = 256
llama_new_context_with_model: flash_attn = 1
llama_new_context_with_model: freq_base  = 1000000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =    56.00 MiB
llama_new_context_with_model: KV self size  =   56.00 MiB, K (f16):   28.00 MiB, V (f16):   28.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =     0.59 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   152.00 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     4.50 MiB
llama_new_context_with_model: graph nodes  = 875
llama_new_context_with_model: graph splits = 2
DEBUG [initialize] initializing slots | n_slots=1 tid="22560398077952" timestamp=1757773988
DEBUG [initialize] new slot | n_ctx_slot=1024 slot_id=0 tid="22560398077952" timestamp=1757773988
INFO [main] model loaded | tid="22560398077952" timestamp=1757773988
DEBUG [update_slots] all slots are idle and system prompt is empty, clear the KV cache | tid="22560398077952" timestamp=1757773988
DEBUG [process_single_task] slot data | n_idle_slots=1 n_processing_slots=0 task_id=0 tid="22560398077952" timestamp=1757773988
time=2025-09-13T16:33:08.461+02:00 level=INFO source=server.go:626 msg="llama runner started in 17.11 seconds"
time=2025-09-13T16:33:08.462+02:00 level=DEBUG source=sched.go:462 msg="finished setting up runner" model=/leonardo_work/IscrC_LLM-Mob/.ollama/models/blobs/sha256-2bada8a7450677000f678be90653b85d364de7db25eb5ea54136ada5f3933730
time=2025-09-13T16:33:08.462+02:00 level=DEBUG source=routes.go:239 msg="generate request" prompt="<|im_start|>system\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n<|im_start|>user\nHi<|im_end|>\n<|im_start|>assistant\n" images=[]
DEBUG [process_single_task] slot data | n_idle_slots=1 n_processing_slots=0 task_id=1 tid="22560398077952" timestamp=1757773988
DEBUG [launch_slot_with_data] slot is processing task | slot_id=0 task_id=2 tid="22560398077952" timestamp=1757773988
DEBUG [update_slots] slot progression | ga_i=0 n_past=0 n_past_se=0 n_prompt_tokens_processed=30 slot_id=0 task_id=2 tid="22560398077952" timestamp=1757773988
DEBUG [update_slots] kv cache rm [p0, end) | p0=0 slot_id=0 task_id=2 tid="22560398077952" timestamp=1757773988
DEBUG [print_timings] prompt eval time     =      28.10 ms /    30 tokens (    0.94 ms per token,  1067.77 tokens per second) | n_prompt_tokens_processed=30 n_tokens_second=1067.767653758542 slot_id=0 t_prompt_processing=28.096 t_token=0.9365333333333333 task_id=2 tid="22560398077952" timestamp=1757773988
DEBUG [print_timings] generation eval time =       0.02 ms /     1 runs   (    0.02 ms per token, 62500.00 tokens per second) | n_decoded=1 n_tokens_second=62500.0 slot_id=0 t_token=0.016 t_token_generation=0.016 task_id=2 tid="22560398077952" timestamp=1757773988
DEBUG [print_timings]           total time =      28.11 ms | slot_id=0 t_prompt_processing=28.096 t_token_generation=0.016 t_total=28.112 task_id=2 tid="22560398077952" timestamp=1757773988
DEBUG [update_slots] slot released | n_cache_tokens=31 n_ctx=1024 n_past=30 n_system_tokens=0 slot_id=0 task_id=2 tid="22560398077952" timestamp=1757773988 truncated=false
DEBUG [log_server_request] request | method="POST" params={} path="/completion" remote_addr="127.0.0.1" remote_port=37042 status=200 tid="22560307306496" timestamp=1757773988
DEBUG [process_single_task] slot data | n_idle_slots=1 n_processing_slots=0 task_id=6 tid="22560398077952" timestamp=1757773988
DEBUG [log_server_request] request | method="POST" params={} path="/tokenize" remote_addr="127.0.0.1" remote_port=37054 status=200 tid="22560305205248" timestamp=1757773988
[GIN] 2025/09/13 - 16:33:08 | 200 | 17.670580404s |       127.0.0.1 | POST     "/api/generate"
time=2025-09-13T16:33:08.610+02:00 level=DEBUG source=sched.go:466 msg="context for request finished"
time=2025-09-13T16:33:08.620+02:00 level=DEBUG source=sched.go:339 msg="runner with non-zero duration has gone idle, adding timer" modelPath=/leonardo_work/IscrC_LLM-Mob/.ollama/models/blobs/sha256-2bada8a7450677000f678be90653b85d364de7db25eb5ea54136ada5f3933730 duration=8h0m0s
time=2025-09-13T16:33:08.620+02:00 level=DEBUG source=sched.go:357 msg="after processing request finished event" modelPath=/leonardo_work/IscrC_LLM-Mob/.ollama/models/blobs/sha256-2bada8a7450677000f678be90653b85d364de7db25eb5ea54136ada5f3933730 refCount=0
[GIN] 2025/09/13 - 16:36:44 | 200 |    7.746932ms |       127.0.0.1 | GET      "/api/tags"
time=2025-09-13T16:36:44.171+02:00 level=DEBUG source=sched.go:575 msg="evaluating already loaded" model=/leonardo_work/IscrC_LLM-Mob/.ollama/models/blobs/sha256-2bada8a7450677000f678be90653b85d364de7db25eb5ea54136ada5f3933730
DEBUG [process_single_task] slot data | n_idle_slots=1 n_processing_slots=0 task_id=7 tid="22560398077952" timestamp=1757774204
DEBUG [process_single_task] slot data | n_idle_slots=1 n_processing_slots=0 task_id=8 tid="22560398077952" timestamp=1757774204
DEBUG [log_server_request] request | method="POST" params={} path="/tokenize" remote_addr="127.0.0.1" remote_port=33870 status=200 tid="22560303104000" timestamp=1757774204
time=2025-09-13T16:36:44.257+02:00 level=DEBUG source=routes.go:1422 msg="chat request" images=0 prompt="<|im_start|>system\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n<|im_start|>user\nSay OK<|im_end|>\n<|im_start|>assistant\n"
DEBUG [process_single_task] slot data | n_idle_slots=1 n_processing_slots=0 task_id=9 tid="22560398077952" timestamp=1757774204
DEBUG [prefix_slot] slot with common prefix found | 0=["slot_id",0,"characters",115]
DEBUG [launch_slot_with_data] slot is processing task | slot_id=0 task_id=10 tid="22560398077952" timestamp=1757774204
DEBUG [update_slots] slot progression | ga_i=0 n_past=24 n_past_se=0 n_prompt_tokens_processed=31 slot_id=0 task_id=10 tid="22560398077952" timestamp=1757774204
DEBUG [update_slots] kv cache rm [p0, end) | p0=24 slot_id=0 task_id=10 tid="22560398077952" timestamp=1757774204
DEBUG [print_timings] prompt eval time     =      22.24 ms /    31 tokens (    0.72 ms per token,  1393.76 tokens per second) | n_prompt_tokens_processed=31 n_tokens_second=1393.7595539969427 slot_id=0 t_prompt_processing=22.242 t_token=0.7174838709677419 task_id=10 tid="22560398077952" timestamp=1757774204
DEBUG [print_timings] generation eval time =      35.45 ms /     2 runs   (   17.73 ms per token,    56.42 tokens per second) | n_decoded=2 n_tokens_second=56.41748942172073 slot_id=0 t_token=17.725 t_token_generation=35.45 task_id=10 tid="22560398077952" timestamp=1757774204
DEBUG [print_timings]           total time =      57.69 ms | slot_id=0 t_prompt_processing=22.242 t_token_generation=35.45 t_total=57.69200000000001 task_id=10 tid="22560398077952" timestamp=1757774204
DEBUG [update_slots] slot released | n_cache_tokens=33 n_ctx=1024 n_past=32 n_system_tokens=0 slot_id=0 task_id=10 tid="22560398077952" timestamp=1757774204 truncated=false
DEBUG [log_server_request] request | method="POST" params={} path="/completion" remote_addr="127.0.0.1" remote_port=33870 status=200 tid="22560303104000" timestamp=1757774204
[GIN] 2025/09/13 - 16:36:44 | 200 |  212.765873ms |       127.0.0.1 | POST     "/api/chat"
time=2025-09-13T16:36:44.357+02:00 level=DEBUG source=sched.go:407 msg="context for request finished"
time=2025-09-13T16:36:44.357+02:00 level=DEBUG source=sched.go:339 msg="runner with non-zero duration has gone idle, adding timer" modelPath=/leonardo_work/IscrC_LLM-Mob/.ollama/models/blobs/sha256-2bada8a7450677000f678be90653b85d364de7db25eb5ea54136ada5f3933730 duration=8h0m0s
time=2025-09-13T16:36:44.357+02:00 level=DEBUG source=sched.go:357 msg="after processing request finished event" modelPath=/leonardo_work/IscrC_LLM-Mob/.ollama/models/blobs/sha256-2bada8a7450677000f678be90653b85d364de7db25eb5ea54136ada5f3933730 refCount=0
[GIN] 2025/09/13 - 16:37:15 | 200 |    7.597047ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/09/13 - 16:37:15 | 200 |    7.090364ms |       127.0.0.1 | GET      "/api/tags"
time=2025-09-13T16:38:24.517+02:00 level=DEBUG source=sched.go:575 msg="evaluating already loaded" model=/leonardo_work/IscrC_LLM-Mob/.ollama/models/blobs/sha256-2bada8a7450677000f678be90653b85d364de7db25eb5ea54136ada5f3933730
time=2025-09-13T16:38:24.517+02:00 level=DEBUG source=sched.go:283 msg="resetting model to expire immediately to make room" modelPath=/leonardo_work/IscrC_LLM-Mob/.ollama/models/blobs/sha256-2bada8a7450677000f678be90653b85d364de7db25eb5ea54136ada5f3933730 refCount=0
time=2025-09-13T16:38:24.517+02:00 level=DEBUG source=sched.go:296 msg="waiting for pending requests to complete and unload to occur" modelPath=/leonardo_work/IscrC_LLM-Mob/.ollama/models/blobs/sha256-2bada8a7450677000f678be90653b85d364de7db25eb5ea54136ada5f3933730
time=2025-09-13T16:38:24.517+02:00 level=DEBUG source=sched.go:360 msg="runner expired event received" modelPath=/leonardo_work/IscrC_LLM-Mob/.ollama/models/blobs/sha256-2bada8a7450677000f678be90653b85d364de7db25eb5ea54136ada5f3933730
time=2025-09-13T16:38:24.517+02:00 level=DEBUG source=sched.go:375 msg="got lock to unload" modelPath=/leonardo_work/IscrC_LLM-Mob/.ollama/models/blobs/sha256-2bada8a7450677000f678be90653b85d364de7db25eb5ea54136ada5f3933730
time=2025-09-13T16:38:24.517+02:00 level=DEBUG source=gpu.go:396 msg="updating system memory data" before.total="502.9 GiB" before.free="490.9 GiB" before.free_swap="0 B" now.total="502.9 GiB" now.free="489.4 GiB" now.free_swap="0 B"
CUDA driver version: 12.2
time=2025-09-13T16:38:24.608+02:00 level=DEBUG source=gpu.go:444 msg="updating cuda memory data" gpu=GPU-8efa7ef6-1cbc-ec7b-6246-cd6e2bf7d1d4 name="NVIDIA A100-SXM-64GB" overhead="0 B" before.total="63.4 GiB" before.free="63.0 GiB" now.total="63.4 GiB" now.free="58.1 GiB" now.used="5.3 GiB"
releasing cuda driver library
time=2025-09-13T16:38:24.608+02:00 level=DEBUG source=server.go:1086 msg="stopping llama server"
time=2025-09-13T16:38:24.608+02:00 level=DEBUG source=server.go:1092 msg="waiting for llama server to exit"
time=2025-09-13T16:38:24.655+02:00 level=DEBUG source=server.go:1096 msg="llama server stopped"
time=2025-09-13T16:38:24.655+02:00 level=DEBUG source=sched.go:380 msg="runner released" modelPath=/leonardo_work/IscrC_LLM-Mob/.ollama/models/blobs/sha256-2bada8a7450677000f678be90653b85d364de7db25eb5ea54136ada5f3933730
time=2025-09-13T16:38:24.858+02:00 level=DEBUG source=gpu.go:396 msg="updating system memory data" before.total="502.9 GiB" before.free="489.4 GiB" before.free_swap="0 B" now.total="502.9 GiB" now.free="489.7 GiB" now.free_swap="0 B"
CUDA driver version: 12.2
time=2025-09-13T16:38:24.953+02:00 level=DEBUG source=gpu.go:444 msg="updating cuda memory data" gpu=GPU-8efa7ef6-1cbc-ec7b-6246-cd6e2bf7d1d4 name="NVIDIA A100-SXM-64GB" overhead="0 B" before.total="63.4 GiB" before.free="58.1 GiB" now.total="63.4 GiB" now.free="63.0 GiB" now.used="482.8 MiB"
releasing cuda driver library
time=2025-09-13T16:38:24.953+02:00 level=DEBUG source=sched.go:659 msg="gpu VRAM free memory converged after 0.44 seconds" model=/leonardo_work/IscrC_LLM-Mob/.ollama/models/blobs/sha256-2bada8a7450677000f678be90653b85d364de7db25eb5ea54136ada5f3933730
time=2025-09-13T16:38:24.953+02:00 level=DEBUG source=sched.go:384 msg="sending an unloaded event" modelPath=/leonardo_work/IscrC_LLM-Mob/.ollama/models/blobs/sha256-2bada8a7450677000f678be90653b85d364de7db25eb5ea54136ada5f3933730
time=2025-09-13T16:38:24.953+02:00 level=DEBUG source=sched.go:302 msg="unload completed" modelPath=/leonardo_work/IscrC_LLM-Mob/.ollama/models/blobs/sha256-2bada8a7450677000f678be90653b85d364de7db25eb5ea54136ada5f3933730
time=2025-09-13T16:38:24.953+02:00 level=DEBUG source=gpu.go:396 msg="updating system memory data" before.total="502.9 GiB" before.free="489.7 GiB" before.free_swap="0 B" now.total="502.9 GiB" now.free="489.7 GiB" now.free_swap="0 B"
CUDA driver version: 12.2
time=2025-09-13T16:38:25.035+02:00 level=DEBUG source=gpu.go:444 msg="updating cuda memory data" gpu=GPU-8efa7ef6-1cbc-ec7b-6246-cd6e2bf7d1d4 name="NVIDIA A100-SXM-64GB" overhead="0 B" before.total="63.4 GiB" before.free="63.0 GiB" now.total="63.4 GiB" now.free="63.0 GiB" now.used="482.8 MiB"
releasing cuda driver library
time=2025-09-13T16:38:25.063+02:00 level=DEBUG source=sched.go:224 msg="loading first model" model=/leonardo_work/IscrC_LLM-Mob/.ollama/models/blobs/sha256-2bada8a7450677000f678be90653b85d364de7db25eb5ea54136ada5f3933730
time=2025-09-13T16:38:25.063+02:00 level=DEBUG source=memory.go:103 msg=evaluating library=cuda gpu_count=1 available="[63.0 GiB]"
time=2025-09-13T16:38:25.064+02:00 level=INFO source=sched.go:714 msg="new model will fit in available VRAM in single GPU, loading" model=/leonardo_work/IscrC_LLM-Mob/.ollama/models/blobs/sha256-2bada8a7450677000f678be90653b85d364de7db25eb5ea54136ada5f3933730 gpu=GPU-8efa7ef6-1cbc-ec7b-6246-cd6e2bf7d1d4 parallel=1 available=67593371648 required="5.1 GiB"
time=2025-09-13T16:38:25.064+02:00 level=DEBUG source=gpu.go:396 msg="updating system memory data" before.total="502.9 GiB" before.free="489.7 GiB" before.free_swap="0 B" now.total="502.9 GiB" now.free="489.7 GiB" now.free_swap="0 B"
CUDA driver version: 12.2
time=2025-09-13T16:38:25.146+02:00 level=DEBUG source=gpu.go:444 msg="updating cuda memory data" gpu=GPU-8efa7ef6-1cbc-ec7b-6246-cd6e2bf7d1d4 name="NVIDIA A100-SXM-64GB" overhead="0 B" before.total="63.4 GiB" before.free="63.0 GiB" now.total="63.4 GiB" now.free="63.0 GiB" now.used="482.8 MiB"
releasing cuda driver library
time=2025-09-13T16:38:25.146+02:00 level=INFO source=server.go:105 msg="system memory" total="502.9 GiB" free="489.7 GiB" free_swap="0 B"
time=2025-09-13T16:38:25.146+02:00 level=DEBUG source=memory.go:103 msg=evaluating library=cuda gpu_count=1 available="[63.0 GiB]"
time=2025-09-13T16:38:25.147+02:00 level=INFO source=memory.go:326 msg="offload to cuda" layers.requested=-1 layers.model=29 layers.offload=29 layers.split="" memory.available="[63.0 GiB]" memory.gpu_overhead="1.0 KiB" memory.required.full="5.1 GiB" memory.required.partial="5.1 GiB" memory.required.kv="112.0 MiB" memory.required.allocations="[5.1 GiB]" memory.weights.total="3.8 GiB" memory.weights.repeating="3.3 GiB" memory.weights.nonrepeating="426.4 MiB" memory.graph.full="304.0 MiB" memory.graph.partial="730.4 MiB"
time=2025-09-13T16:38:25.147+02:00 level=DEBUG source=common.go:168 msg=extracting runner=cpu payload=linux/amd64/cpu/libggml.so.gz
time=2025-09-13T16:38:25.147+02:00 level=DEBUG source=common.go:168 msg=extracting runner=cpu payload=linux/amd64/cpu/libllama.so.gz
time=2025-09-13T16:38:25.147+02:00 level=DEBUG source=common.go:168 msg=extracting runner=cpu payload=linux/amd64/cpu/ollama_llama_server.gz
time=2025-09-13T16:38:25.147+02:00 level=DEBUG source=common.go:168 msg=extracting runner=cpu_avx payload=linux/amd64/cpu_avx/libggml.so.gz
time=2025-09-13T16:38:25.147+02:00 level=DEBUG source=common.go:168 msg=extracting runner=cpu_avx payload=linux/amd64/cpu_avx/libllama.so.gz
time=2025-09-13T16:38:25.147+02:00 level=DEBUG source=common.go:168 msg=extracting runner=cpu_avx payload=linux/amd64/cpu_avx/ollama_llama_server.gz
time=2025-09-13T16:38:25.147+02:00 level=DEBUG source=common.go:168 msg=extracting runner=cpu_avx2 payload=linux/amd64/cpu_avx2/libggml.so.gz
time=2025-09-13T16:38:25.147+02:00 level=DEBUG source=common.go:168 msg=extracting runner=cpu_avx2 payload=linux/amd64/cpu_avx2/libllama.so.gz
time=2025-09-13T16:38:25.147+02:00 level=DEBUG source=common.go:168 msg=extracting runner=cpu_avx2 payload=linux/amd64/cpu_avx2/ollama_llama_server.gz
time=2025-09-13T16:38:25.147+02:00 level=DEBUG source=common.go:168 msg=extracting runner=cuda_v11 payload=linux/amd64/cuda_v11/libggml.so.gz
time=2025-09-13T16:38:25.147+02:00 level=DEBUG source=common.go:168 msg=extracting runner=cuda_v11 payload=linux/amd64/cuda_v11/libllama.so.gz
time=2025-09-13T16:38:25.147+02:00 level=DEBUG source=common.go:168 msg=extracting runner=cuda_v11 payload=linux/amd64/cuda_v11/ollama_llama_server.gz
time=2025-09-13T16:38:25.147+02:00 level=DEBUG source=common.go:168 msg=extracting runner=cuda_v12 payload=linux/amd64/cuda_v12/libggml.so.gz
time=2025-09-13T16:38:25.147+02:00 level=DEBUG source=common.go:168 msg=extracting runner=cuda_v12 payload=linux/amd64/cuda_v12/libllama.so.gz
time=2025-09-13T16:38:25.147+02:00 level=DEBUG source=common.go:168 msg=extracting runner=cuda_v12 payload=linux/amd64/cuda_v12/ollama_llama_server.gz
time=2025-09-13T16:38:25.147+02:00 level=DEBUG source=common.go:168 msg=extracting runner=rocm_v60102 payload=linux/amd64/rocm_v60102/libggml.so.gz
time=2025-09-13T16:38:25.147+02:00 level=DEBUG source=common.go:168 msg=extracting runner=rocm_v60102 payload=linux/amd64/rocm_v60102/libllama.so.gz
time=2025-09-13T16:38:25.147+02:00 level=DEBUG source=common.go:168 msg=extracting runner=rocm_v60102 payload=linux/amd64/rocm_v60102/ollama_llama_server.gz
time=2025-09-13T16:38:25.149+02:00 level=DEBUG source=common.go:294 msg="availableServers : found" file=/leonardo_work/IscrC_LLM-Mob/tmp_ollama_19945716/ollama696725503/runners/cpu/ollama_llama_server
time=2025-09-13T16:38:25.149+02:00 level=DEBUG source=common.go:294 msg="availableServers : found" file=/leonardo_work/IscrC_LLM-Mob/tmp_ollama_19945716/ollama696725503/runners/cpu_avx/ollama_llama_server
time=2025-09-13T16:38:25.149+02:00 level=DEBUG source=common.go:294 msg="availableServers : found" file=/leonardo_work/IscrC_LLM-Mob/tmp_ollama_19945716/ollama696725503/runners/cpu_avx2/ollama_llama_server
time=2025-09-13T16:38:25.149+02:00 level=DEBUG source=common.go:294 msg="availableServers : found" file=/leonardo_work/IscrC_LLM-Mob/tmp_ollama_19945716/ollama696725503/runners/cuda_v11/ollama_llama_server
time=2025-09-13T16:38:25.149+02:00 level=DEBUG source=common.go:294 msg="availableServers : found" file=/leonardo_work/IscrC_LLM-Mob/tmp_ollama_19945716/ollama696725503/runners/cuda_v12/ollama_llama_server
time=2025-09-13T16:38:25.149+02:00 level=DEBUG source=common.go:294 msg="availableServers : found" file=/leonardo_work/IscrC_LLM-Mob/tmp_ollama_19945716/ollama696725503/runners/rocm_v60102/ollama_llama_server
time=2025-09-13T16:38:25.150+02:00 level=DEBUG source=common.go:294 msg="availableServers : found" file=/leonardo_work/IscrC_LLM-Mob/tmp_ollama_19945716/ollama696725503/runners/cpu/ollama_llama_server
time=2025-09-13T16:38:25.150+02:00 level=DEBUG source=common.go:294 msg="availableServers : found" file=/leonardo_work/IscrC_LLM-Mob/tmp_ollama_19945716/ollama696725503/runners/cpu_avx/ollama_llama_server
time=2025-09-13T16:38:25.150+02:00 level=DEBUG source=common.go:294 msg="availableServers : found" file=/leonardo_work/IscrC_LLM-Mob/tmp_ollama_19945716/ollama696725503/runners/cpu_avx2/ollama_llama_server
time=2025-09-13T16:38:25.150+02:00 level=DEBUG source=common.go:294 msg="availableServers : found" file=/leonardo_work/IscrC_LLM-Mob/tmp_ollama_19945716/ollama696725503/runners/cuda_v11/ollama_llama_server
time=2025-09-13T16:38:25.150+02:00 level=DEBUG source=common.go:294 msg="availableServers : found" file=/leonardo_work/IscrC_LLM-Mob/tmp_ollama_19945716/ollama696725503/runners/cuda_v12/ollama_llama_server
time=2025-09-13T16:38:25.150+02:00 level=DEBUG source=common.go:294 msg="availableServers : found" file=/leonardo_work/IscrC_LLM-Mob/tmp_ollama_19945716/ollama696725503/runners/rocm_v60102/ollama_llama_server
time=2025-09-13T16:38:25.150+02:00 level=INFO source=server.go:172 msg="user override" OLLAMA_LLM_LIBRARY=cuda_v12 path=/leonardo_work/IscrC_LLM-Mob/tmp_ollama_19945716/ollama696725503/runners/cuda_v12
time=2025-09-13T16:38:25.151+02:00 level=INFO source=server.go:388 msg="starting llama server" cmd="/leonardo_work/IscrC_LLM-Mob/tmp_ollama_19945716/ollama696725503/runners/cuda_v12/ollama_llama_server --model /leonardo_work/IscrC_LLM-Mob/.ollama/models/blobs/sha256-2bada8a7450677000f678be90653b85d364de7db25eb5ea54136ada5f3933730 --ctx-size 2048 --batch-size 512 --embedding --n-gpu-layers 29 --verbose --threads 32 --flash-attn --parallel 1 --port 33333"
time=2025-09-13T16:38:25.151+02:00 level=DEBUG source=server.go:405 msg=subprocess environment="[CUDA_VISIBLE_DEVICES=GPU-8efa7ef6-1cbc-ec7b-6246-cd6e2bf7d1d4 LD_LIBRARY_PATH=/leonardo_work/IscrC_LLM-Mob/opt/lib/ollama:/leonardo_work/IscrC_LLM-Mob/tmp_ollama_19945716/ollama696725503/runners/cuda_v12:/leonardo/prod/opt/compilers/cuda/12.3/none/lib64:/leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/python-3.11.6-i5k3c6ggftqkzgqyymfbkynpgm2lgjtd/lib:/leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/util-linux-uuid-2.38.1-jkdi7kvvma7367qdmvpkada4pyiafoud/lib:/leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/sqlite-3.43.2-casyrltocz5edzjhs5vzlqhwkamn7y4a/lib:/leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/libxcrypt-4.4.35-ss2rzin25ozjy4gyy3dack36njs6navg/lib:/leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/libffi-3.4.4-6r7brdq5dnreoad6f7sn7ybjjvwdmvue/lib64:/leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/libffi-3.4.4-6r7brdq5dnreoad6f7sn7ybjjvwdmvue/lib:/leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/gettext-0.22.3-2g7elifkgxzpypbswqnjuu5hefn4mjts/lib:/leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/zstd-1.5.5-gawytflrhedqdc2riwax7oduoqddx22s/lib:/leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/libxml2-2.10.3-5eeeokp4kszufozbayq4bewwmyeuwy27/lib:/leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/zlib-ng-2.1.4-6htiapkoa6fx2medhyabzo575skozuir/lib:/leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/xz-5.4.1-hubmwr5wc5nf6zk3ghuaikxiejuyt6bi/lib:/leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/libiconv-1.17-d7yvx2s6da4x2rfx44bc3perbb33rvuy/lib:/leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/gdbm-1.23-fs6otcki47azeywcckquj2sy4mzsnzxg/lib:/leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/readline-8.2-nyw6mp6b7dvizewrf7exopvap2q32s5j/lib:/leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/ncurses-6.4-asx3jea367shsxjt6bdj2bu5olxll6ni/lib:/leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/expat-2.5.0-bptl3xwbvbkxxoc5x3dhviorarv4dvxv/lib:/leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/libbsd-0.11.7-cgxjopleu7se4y4cgi7oefbljgasr457/lib:/leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/libmd-1.0.4-wja3f5q3w75tqtro333fptfnji7oqxiu/lib:/leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/bzip2-1.0.8-gp5wcz5lksrbm2gqiqjppumrhjz6gahy/lib ROCR_VISIBLE_DEVICES=0,1 CUDA_HOME=/leonardo/prod/opt/compilers/cuda/12.3/none PATH=/leonardo_work/IscrC_LLM-Mob/venv/bin:/leonardo/prod/opt/compilers/cuda/12.3/none/cuda-samples/bin/x86_64/linux/release:/leonardo/prod/opt/compilers/cuda/12.3/none/bin:/leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/python-3.11.6-i5k3c6ggftqkzgqyymfbkynpgm2lgjtd/bin:/leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/util-linux-uuid-2.38.1-jkdi7kvvma7367qdmvpkada4pyiafoud/bin:/leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/sqlite-3.43.2-casyrltocz5edzjhs5vzlqhwkamn7y4a/bin:/leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/gettext-0.22.3-2g7elifkgxzpypbswqnjuu5hefn4mjts/bin:/leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/tar-1.34-amqus5s63wb5oknfz4pfnvdqtwwe76iw/bin:/leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/zstd-1.5.5-gawytflrhedqdc2riwax7oduoqddx22s/bin:/leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/pigz-2.7-bopr5vpavkyl3lou5sap6iydgi3udtcm/bin:/leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/libxml2-2.10.3-5eeeokp4kszufozbayq4bewwmyeuwy27/bin:/leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/xz-5.4.1-hubmwr5wc5nf6zk3ghuaikxiejuyt6bi/bin:/leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/libiconv-1.17-d7yvx2s6da4x2rfx44bc3perbb33rvuy/bin:/leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/gdbm-1.23-fs6otcki47azeywcckquj2sy4mzsnzxg/bin:/leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/readline-8.2-nyw6mp6b7dvizewrf7exopvap2q32s5j/bin:/leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/ncurses-6.4-asx3jea367shsxjt6bdj2bu5olxll6ni/bin:/leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/expat-2.5.0-bptl3xwbvbkxxoc5x3dhviorarv4dvxv/bin:/leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/bzip2-1.0.8-gp5wcz5lksrbm2gqiqjppumrhjz6gahy/bin:/leonardo_work/IscrC_LLM-Mob/venv/bin:/leonardo/home/userexternal/smattiol/opt/ollama/bin:/cineca/bin:/leonardo/home/userexternal/smattiol/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/bin/remote-cli:/leonardo/home/userexternal/smattiol/opt/ollama/bin:/cineca/bin:/leonardo/home/userexternal/smattiol/.nvm/versions/node/v22.18.0/bin:/leonardo/home/userexternal/smattiol/opt/ollama/bin:/leonardo/home/userexternal/smattiol/.local/bin:/leonardo/home/userexternal/smattiol/bin:/cineca/bin:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/environment-modules-5.2.0-rz47odw4phlhzhhbz7b65nv5s5othgmi/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/leonardo/home/userexternal/smattiol/.vscode-server/extensions/ms-python.debugpy-2025.10.0-linux-x64/bundled/scripts/noConfigScripts GPU_DEVICE_ORDINAL=0,1]"
time=2025-09-13T16:38:25.151+02:00 level=INFO source=sched.go:449 msg="loaded runners" count=1
time=2025-09-13T16:38:25.151+02:00 level=INFO source=server.go:587 msg="waiting for llama runner to start responding"
time=2025-09-13T16:38:25.151+02:00 level=INFO source=server.go:621 msg="waiting for server to become available" status="llm server error"
INFO [main] starting c++ runner | tid="22982883241984" timestamp=1757774305
INFO [main] build info | build=10 commit="3a8c75e" tid="22982883241984" timestamp=1757774305
INFO [main] system info | n_threads=32 n_threads_batch=32 system_info="AVX = 1 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 0 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 0 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | " tid="22982883241984" timestamp=1757774305 total_threads=32
INFO [main] HTTP server listening | hostname="127.0.0.1" n_threads_http="31" port="33333" tid="22982883241984" timestamp=1757774305
llama_model_loader: loaded meta data with 34 key-value pairs and 339 tensors from /leonardo_work/IscrC_LLM-Mob/.ollama/models/blobs/sha256-2bada8a7450677000f678be90653b85d364de7db25eb5ea54136ada5f3933730 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 7B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5
llama_model_loader: - kv   5:                         general.size_label str              = 7B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-7...
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 7B
llama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen
llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-7B
llama_model_loader: - kv  12:                               general.tags arr[str,2]       = ["chat", "text-generation"]
llama_model_loader: - kv  13:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  14:                          qwen2.block_count u32              = 28
llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768
llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  22:                          general.file_type u32              = 15
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  33:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
time=2025-09-13T16:38:25.403+02:00 level=INFO source=server.go:621 msg="waiting for server to become available" status="llm server loading model"
llm_load_vocab: special tokens cache size = 22
llm_load_vocab: token to piece cache size = 0.9310 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = qwen2
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 152064
llm_load_print_meta: n_merges         = 151387
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 32768
llm_load_print_meta: n_embd           = 3584
llm_load_print_meta: n_layer          = 28
llm_load_print_meta: n_head           = 28
llm_load_print_meta: n_head_kv        = 4
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 7
llm_load_print_meta: n_embd_k_gqa     = 512
llm_load_print_meta: n_embd_v_gqa     = 512
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 18944
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 1000000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 32768
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = ?B
llm_load_print_meta: model ftype      = Q4_K - Medium
llm_load_print_meta: model params     = 7.62 B
llm_load_print_meta: model size       = 4.36 GiB (4.91 BPW) 
llm_load_print_meta: general.name     = Qwen2.5 7B Instruct
llm_load_print_meta: BOS token        = 151643 '<|endoftext|>'
llm_load_print_meta: EOS token        = 151645 '<|im_end|>'
llm_load_print_meta: PAD token        = 151643 '<|endoftext|>'
llm_load_print_meta: LF token         = 148848 'ÄĬ'
llm_load_print_meta: EOT token        = 151643 '<|endoftext|>'
llm_load_print_meta: EOG token        = 151643 '<|endoftext|>'
llm_load_print_meta: EOG token        = 151645 '<|im_end|>'
llm_load_print_meta: max token length = 256
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A100-SXM-64GB, compute capability 8.0, VMM: yes
llm_load_tensors: ggml ctx size =    0.30 MiB
llm_load_tensors: offloading 28 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 29/29 layers to GPU
llm_load_tensors:        CPU buffer size =   292.36 MiB
llm_load_tensors:      CUDA0 buffer size =  4168.09 MiB
time=2025-09-13T16:38:26.406+02:00 level=DEBUG source=server.go:632 msg="model load progress 0.53"
time=2025-09-13T16:38:26.670+02:00 level=DEBUG source=server.go:632 msg="model load progress 1.00"
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: flash_attn = 1
llama_new_context_with_model: freq_base  = 1000000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   112.00 MiB
llama_new_context_with_model: KV self size  =  112.00 MiB, K (f16):   56.00 MiB, V (f16):   56.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =     0.59 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   304.00 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =    11.01 MiB
llama_new_context_with_model: graph nodes  = 875
llama_new_context_with_model: graph splits = 2
DEBUG [initialize] initializing slots | n_slots=1 tid="22982883241984" timestamp=1757774306
DEBUG [initialize] new slot | n_ctx_slot=2048 slot_id=0 tid="22982883241984" timestamp=1757774306
INFO [main] model loaded | tid="22982883241984" timestamp=1757774306
DEBUG [update_slots] all slots are idle and system prompt is empty, clear the KV cache | tid="22982883241984" timestamp=1757774306
DEBUG [process_single_task] slot data | n_idle_slots=1 n_processing_slots=0 task_id=0 tid="22982883241984" timestamp=1757774306
time=2025-09-13T16:38:26.920+02:00 level=INFO source=server.go:626 msg="llama runner started in 1.77 seconds"
time=2025-09-13T16:38:26.921+02:00 level=DEBUG source=sched.go:462 msg="finished setting up runner" model=/leonardo_work/IscrC_LLM-Mob/.ollama/models/blobs/sha256-2bada8a7450677000f678be90653b85d364de7db25eb5ea54136ada5f3933730
DEBUG [process_single_task] slot data | n_idle_slots=1 n_processing_slots=0 task_id=1 tid="22982883241984" timestamp=1757774306
DEBUG [log_server_request] request | method="POST" params={} path="/tokenize" remote_addr="127.0.0.1" remote_port=34126 status=200 tid="22982876463104" timestamp=1757774306
time=2025-09-13T16:38:27.006+02:00 level=DEBUG source=routes.go:1422 msg="chat request" images=0 prompt="<|im_start|>system\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n<|im_start|>user\ntest<|im_end|>\n<|im_start|>assistant\n"
DEBUG [process_single_task] slot data | n_idle_slots=1 n_processing_slots=0 task_id=2 tid="22982883241984" timestamp=1757774307
DEBUG [launch_slot_with_data] slot is processing task | slot_id=0 task_id=3 tid="22982883241984" timestamp=1757774307
DEBUG [update_slots] slot progression | ga_i=0 n_past=0 n_past_se=0 n_prompt_tokens_processed=30 slot_id=0 task_id=3 tid="22982883241984" timestamp=1757774307
DEBUG [update_slots] kv cache rm [p0, end) | p0=0 slot_id=0 task_id=3 tid="22982883241984" timestamp=1757774307
DEBUG [print_timings] prompt eval time     =      28.14 ms /    30 tokens (    0.94 ms per token,  1066.02 tokens per second) | n_prompt_tokens_processed=30 n_tokens_second=1066.022315400469 slot_id=0 t_prompt_processing=28.142 t_token=0.9380666666666666 task_id=3 tid="22982883241984" timestamp=1757774307
DEBUG [print_timings] generation eval time =       0.02 ms /     1 runs   (    0.02 ms per token, 52631.58 tokens per second) | n_decoded=1 n_tokens_second=52631.57894736842 slot_id=0 t_token=0.019 t_token_generation=0.019 task_id=3 tid="22982883241984" timestamp=1757774307
DEBUG [print_timings]           total time =      28.16 ms | slot_id=0 t_prompt_processing=28.142 t_token_generation=0.019 t_total=28.160999999999998 task_id=3 tid="22982883241984" timestamp=1757774307
DEBUG [update_slots] slot released | n_cache_tokens=31 n_ctx=2048 n_past=30 n_system_tokens=0 slot_id=0 task_id=3 tid="22982883241984" timestamp=1757774307 truncated=false
DEBUG [log_server_request] request | method="POST" params={} path="/completion" remote_addr="127.0.0.1" remote_port=34136 status=200 tid="22982874361856" timestamp=1757774307
[GIN] 2025/09/13 - 16:38:27 | 200 |  2.534256551s |       127.0.0.1 | POST     "/api/chat"
time=2025-09-13T16:38:27.035+02:00 level=DEBUG source=sched.go:466 msg="context for request finished"
time=2025-09-13T16:38:27.035+02:00 level=DEBUG source=sched.go:339 msg="runner with non-zero duration has gone idle, adding timer" modelPath=/leonardo_work/IscrC_LLM-Mob/.ollama/models/blobs/sha256-2bada8a7450677000f678be90653b85d364de7db25eb5ea54136ada5f3933730 duration=8h0m0s
time=2025-09-13T16:38:27.035+02:00 level=DEBUG source=sched.go:357 msg="after processing request finished event" modelPath=/leonardo_work/IscrC_LLM-Mob/.ollama/models/blobs/sha256-2bada8a7450677000f678be90653b85d364de7db25eb5ea54136ada5f3933730 refCount=0
time=2025-09-13T16:38:29.670+02:00 level=WARN source=types.go:509 msg="invalid option provided" option=cache_type_k
time=2025-09-13T16:38:29.670+02:00 level=WARN source=types.go:509 msg="invalid option provided" option=num_gqa
time=2025-09-13T16:38:29.670+02:00 level=DEBUG source=sched.go:575 msg="evaluating already loaded" model=/leonardo_work/IscrC_LLM-Mob/.ollama/models/blobs/sha256-2bada8a7450677000f678be90653b85d364de7db25eb5ea54136ada5f3933730
time=2025-09-13T16:38:29.670+02:00 level=DEBUG source=sched.go:283 msg="resetting model to expire immediately to make room" modelPath=/leonardo_work/IscrC_LLM-Mob/.ollama/models/blobs/sha256-2bada8a7450677000f678be90653b85d364de7db25eb5ea54136ada5f3933730 refCount=0
time=2025-09-13T16:38:29.670+02:00 level=DEBUG source=sched.go:296 msg="waiting for pending requests to complete and unload to occur" modelPath=/leonardo_work/IscrC_LLM-Mob/.ollama/models/blobs/sha256-2bada8a7450677000f678be90653b85d364de7db25eb5ea54136ada5f3933730
time=2025-09-13T16:38:29.670+02:00 level=DEBUG source=sched.go:360 msg="runner expired event received" modelPath=/leonardo_work/IscrC_LLM-Mob/.ollama/models/blobs/sha256-2bada8a7450677000f678be90653b85d364de7db25eb5ea54136ada5f3933730
time=2025-09-13T16:38:29.670+02:00 level=DEBUG source=sched.go:375 msg="got lock to unload" modelPath=/leonardo_work/IscrC_LLM-Mob/.ollama/models/blobs/sha256-2bada8a7450677000f678be90653b85d364de7db25eb5ea54136ada5f3933730
time=2025-09-13T16:38:29.670+02:00 level=DEBUG source=gpu.go:396 msg="updating system memory data" before.total="502.9 GiB" before.free="489.7 GiB" before.free_swap="0 B" now.total="502.9 GiB" now.free="489.3 GiB" now.free_swap="0 B"
CUDA driver version: 12.2
time=2025-09-13T16:38:29.671+02:00 level=WARN source=types.go:509 msg="invalid option provided" option=cache_type_k
time=2025-09-13T16:38:29.671+02:00 level=WARN source=types.go:509 msg="invalid option provided" option=num_gqa
time=2025-09-13T16:38:29.806+02:00 level=DEBUG source=gpu.go:444 msg="updating cuda memory data" gpu=GPU-8efa7ef6-1cbc-ec7b-6246-cd6e2bf7d1d4 name="NVIDIA A100-SXM-64GB" overhead="0 B" before.total="63.4 GiB" before.free="63.0 GiB" now.total="63.4 GiB" now.free="57.9 GiB" now.used="5.5 GiB"
releasing cuda driver library
time=2025-09-13T16:38:29.806+02:00 level=DEBUG source=server.go:1086 msg="stopping llama server"
time=2025-09-13T16:38:29.806+02:00 level=DEBUG source=server.go:1092 msg="waiting for llama server to exit"
time=2025-09-13T16:38:29.876+02:00 level=DEBUG source=server.go:1096 msg="llama server stopped"
time=2025-09-13T16:38:29.876+02:00 level=DEBUG source=sched.go:380 msg="runner released" modelPath=/leonardo_work/IscrC_LLM-Mob/.ollama/models/blobs/sha256-2bada8a7450677000f678be90653b85d364de7db25eb5ea54136ada5f3933730
time=2025-09-13T16:38:30.057+02:00 level=DEBUG source=gpu.go:396 msg="updating system memory data" before.total="502.9 GiB" before.free="489.3 GiB" before.free_swap="0 B" now.total="502.9 GiB" now.free="489.9 GiB" now.free_swap="0 B"
CUDA driver version: 12.2
time=2025-09-13T16:38:30.201+02:00 level=DEBUG source=gpu.go:444 msg="updating cuda memory data" gpu=GPU-8efa7ef6-1cbc-ec7b-6246-cd6e2bf7d1d4 name="NVIDIA A100-SXM-64GB" overhead="0 B" before.total="63.4 GiB" before.free="57.9 GiB" now.total="63.4 GiB" now.free="63.0 GiB" now.used="482.8 MiB"
releasing cuda driver library
time=2025-09-13T16:38:30.202+02:00 level=DEBUG source=sched.go:659 msg="gpu VRAM free memory converged after 0.53 seconds" model=/leonardo_work/IscrC_LLM-Mob/.ollama/models/blobs/sha256-2bada8a7450677000f678be90653b85d364de7db25eb5ea54136ada5f3933730
time=2025-09-13T16:38:30.202+02:00 level=DEBUG source=sched.go:384 msg="sending an unloaded event" modelPath=/leonardo_work/IscrC_LLM-Mob/.ollama/models/blobs/sha256-2bada8a7450677000f678be90653b85d364de7db25eb5ea54136ada5f3933730
time=2025-09-13T16:38:30.202+02:00 level=DEBUG source=sched.go:302 msg="unload completed" modelPath=/leonardo_work/IscrC_LLM-Mob/.ollama/models/blobs/sha256-2bada8a7450677000f678be90653b85d364de7db25eb5ea54136ada5f3933730
time=2025-09-13T16:38:30.202+02:00 level=DEBUG source=gpu.go:396 msg="updating system memory data" before.total="502.9 GiB" before.free="489.9 GiB" before.free_swap="0 B" now.total="502.9 GiB" now.free="489.9 GiB" now.free_swap="0 B"
CUDA driver version: 12.2
time=2025-09-13T16:38:30.333+02:00 level=DEBUG source=gpu.go:444 msg="updating cuda memory data" gpu=GPU-8efa7ef6-1cbc-ec7b-6246-cd6e2bf7d1d4 name="NVIDIA A100-SXM-64GB" overhead="0 B" before.total="63.4 GiB" before.free="63.0 GiB" now.total="63.4 GiB" now.free="63.0 GiB" now.used="482.8 MiB"
releasing cuda driver library
time=2025-09-13T16:38:30.363+02:00 level=DEBUG source=sched.go:224 msg="loading first model" model=/leonardo_work/IscrC_LLM-Mob/.ollama/models/blobs/sha256-2bada8a7450677000f678be90653b85d364de7db25eb5ea54136ada5f3933730
time=2025-09-13T16:38:30.363+02:00 level=DEBUG source=memory.go:103 msg=evaluating library=cuda gpu_count=1 available="[63.0 GiB]"
time=2025-09-13T16:38:30.363+02:00 level=INFO source=sched.go:714 msg="new model will fit in available VRAM in single GPU, loading" model=/leonardo_work/IscrC_LLM-Mob/.ollama/models/blobs/sha256-2bada8a7450677000f678be90653b85d364de7db25eb5ea54136ada5f3933730 gpu=GPU-8efa7ef6-1cbc-ec7b-6246-cd6e2bf7d1d4 parallel=1 available=67593371648 required="1.3 GiB"
time=2025-09-13T16:38:30.364+02:00 level=DEBUG source=gpu.go:396 msg="updating system memory data" before.total="502.9 GiB" before.free="489.9 GiB" before.free_swap="0 B" now.total="502.9 GiB" now.free="489.9 GiB" now.free_swap="0 B"
CUDA driver version: 12.2
time=2025-09-13T16:38:30.494+02:00 level=DEBUG source=gpu.go:444 msg="updating cuda memory data" gpu=GPU-8efa7ef6-1cbc-ec7b-6246-cd6e2bf7d1d4 name="NVIDIA A100-SXM-64GB" overhead="0 B" before.total="63.4 GiB" before.free="63.0 GiB" now.total="63.4 GiB" now.free="63.0 GiB" now.used="482.8 MiB"
releasing cuda driver library
time=2025-09-13T16:38:30.494+02:00 level=INFO source=server.go:105 msg="system memory" total="502.9 GiB" free="489.9 GiB" free_swap="0 B"
time=2025-09-13T16:38:30.494+02:00 level=DEBUG source=memory.go:103 msg=evaluating library=cuda gpu_count=1 available="[63.0 GiB]"
time=2025-09-13T16:38:30.494+02:00 level=INFO source=memory.go:326 msg="offload to cuda" layers.requested=1 layers.model=29 layers.offload=1 layers.split="" memory.available="[63.0 GiB]" memory.gpu_overhead="1.0 KiB" memory.required.full="5.1 GiB" memory.required.partial="1.3 GiB" memory.required.kv="56.0 MiB" memory.required.allocations="[1.3 GiB]" memory.weights.total="3.7 GiB" memory.weights.repeating="3.3 GiB" memory.weights.nonrepeating="426.4 MiB" memory.graph.full="152.0 MiB" memory.graph.partial="578.4 MiB"
time=2025-09-13T16:38:30.495+02:00 level=DEBUG source=common.go:168 msg=extracting runner=cpu payload=linux/amd64/cpu/libggml.so.gz
time=2025-09-13T16:38:30.495+02:00 level=DEBUG source=common.go:168 msg=extracting runner=cpu payload=linux/amd64/cpu/libllama.so.gz
time=2025-09-13T16:38:30.495+02:00 level=DEBUG source=common.go:168 msg=extracting runner=cpu payload=linux/amd64/cpu/ollama_llama_server.gz
time=2025-09-13T16:38:30.495+02:00 level=DEBUG source=common.go:168 msg=extracting runner=cpu_avx payload=linux/amd64/cpu_avx/libggml.so.gz
time=2025-09-13T16:38:30.495+02:00 level=DEBUG source=common.go:168 msg=extracting runner=cpu_avx payload=linux/amd64/cpu_avx/libllama.so.gz
time=2025-09-13T16:38:30.495+02:00 level=DEBUG source=common.go:168 msg=extracting runner=cpu_avx payload=linux/amd64/cpu_avx/ollama_llama_server.gz
time=2025-09-13T16:38:30.495+02:00 level=DEBUG source=common.go:168 msg=extracting runner=cpu_avx2 payload=linux/amd64/cpu_avx2/libggml.so.gz
time=2025-09-13T16:38:30.495+02:00 level=DEBUG source=common.go:168 msg=extracting runner=cpu_avx2 payload=linux/amd64/cpu_avx2/libllama.so.gz
time=2025-09-13T16:38:30.495+02:00 level=DEBUG source=common.go:168 msg=extracting runner=cpu_avx2 payload=linux/amd64/cpu_avx2/ollama_llama_server.gz
time=2025-09-13T16:38:30.495+02:00 level=DEBUG source=common.go:168 msg=extracting runner=cuda_v11 payload=linux/amd64/cuda_v11/libggml.so.gz
time=2025-09-13T16:38:30.495+02:00 level=DEBUG source=common.go:168 msg=extracting runner=cuda_v11 payload=linux/amd64/cuda_v11/libllama.so.gz
time=2025-09-13T16:38:30.495+02:00 level=DEBUG source=common.go:168 msg=extracting runner=cuda_v11 payload=linux/amd64/cuda_v11/ollama_llama_server.gz
time=2025-09-13T16:38:30.495+02:00 level=DEBUG source=common.go:168 msg=extracting runner=cuda_v12 payload=linux/amd64/cuda_v12/libggml.so.gz
time=2025-09-13T16:38:30.495+02:00 level=DEBUG source=common.go:168 msg=extracting runner=cuda_v12 payload=linux/amd64/cuda_v12/libllama.so.gz
time=2025-09-13T16:38:30.495+02:00 level=DEBUG source=common.go:168 msg=extracting runner=cuda_v12 payload=linux/amd64/cuda_v12/ollama_llama_server.gz
time=2025-09-13T16:38:30.495+02:00 level=DEBUG source=common.go:168 msg=extracting runner=rocm_v60102 payload=linux/amd64/rocm_v60102/libggml.so.gz
time=2025-09-13T16:38:30.495+02:00 level=DEBUG source=common.go:168 msg=extracting runner=rocm_v60102 payload=linux/amd64/rocm_v60102/libllama.so.gz
time=2025-09-13T16:38:30.495+02:00 level=DEBUG source=common.go:168 msg=extracting runner=rocm_v60102 payload=linux/amd64/rocm_v60102/ollama_llama_server.gz
time=2025-09-13T16:38:30.497+02:00 level=DEBUG source=common.go:294 msg="availableServers : found" file=/leonardo_work/IscrC_LLM-Mob/tmp_ollama_19945716/ollama696725503/runners/cpu/ollama_llama_server
time=2025-09-13T16:38:30.497+02:00 level=DEBUG source=common.go:294 msg="availableServers : found" file=/leonardo_work/IscrC_LLM-Mob/tmp_ollama_19945716/ollama696725503/runners/cpu_avx/ollama_llama_server
time=2025-09-13T16:38:30.497+02:00 level=DEBUG source=common.go:294 msg="availableServers : found" file=/leonardo_work/IscrC_LLM-Mob/tmp_ollama_19945716/ollama696725503/runners/cpu_avx2/ollama_llama_server
time=2025-09-13T16:38:30.497+02:00 level=DEBUG source=common.go:294 msg="availableServers : found" file=/leonardo_work/IscrC_LLM-Mob/tmp_ollama_19945716/ollama696725503/runners/cuda_v11/ollama_llama_server
time=2025-09-13T16:38:30.497+02:00 level=DEBUG source=common.go:294 msg="availableServers : found" file=/leonardo_work/IscrC_LLM-Mob/tmp_ollama_19945716/ollama696725503/runners/cuda_v12/ollama_llama_server
time=2025-09-13T16:38:30.497+02:00 level=DEBUG source=common.go:294 msg="availableServers : found" file=/leonardo_work/IscrC_LLM-Mob/tmp_ollama_19945716/ollama696725503/runners/rocm_v60102/ollama_llama_server
time=2025-09-13T16:38:30.498+02:00 level=DEBUG source=common.go:294 msg="availableServers : found" file=/leonardo_work/IscrC_LLM-Mob/tmp_ollama_19945716/ollama696725503/runners/cpu/ollama_llama_server
time=2025-09-13T16:38:30.498+02:00 level=DEBUG source=common.go:294 msg="availableServers : found" file=/leonardo_work/IscrC_LLM-Mob/tmp_ollama_19945716/ollama696725503/runners/cpu_avx/ollama_llama_server
time=2025-09-13T16:38:30.498+02:00 level=DEBUG source=common.go:294 msg="availableServers : found" file=/leonardo_work/IscrC_LLM-Mob/tmp_ollama_19945716/ollama696725503/runners/cpu_avx2/ollama_llama_server
time=2025-09-13T16:38:30.498+02:00 level=DEBUG source=common.go:294 msg="availableServers : found" file=/leonardo_work/IscrC_LLM-Mob/tmp_ollama_19945716/ollama696725503/runners/cuda_v11/ollama_llama_server
time=2025-09-13T16:38:30.498+02:00 level=DEBUG source=common.go:294 msg="availableServers : found" file=/leonardo_work/IscrC_LLM-Mob/tmp_ollama_19945716/ollama696725503/runners/cuda_v12/ollama_llama_server
time=2025-09-13T16:38:30.498+02:00 level=DEBUG source=common.go:294 msg="availableServers : found" file=/leonardo_work/IscrC_LLM-Mob/tmp_ollama_19945716/ollama696725503/runners/rocm_v60102/ollama_llama_server
time=2025-09-13T16:38:30.498+02:00 level=INFO source=server.go:172 msg="user override" OLLAMA_LLM_LIBRARY=cuda_v12 path=/leonardo_work/IscrC_LLM-Mob/tmp_ollama_19945716/ollama696725503/runners/cuda_v12
time=2025-09-13T16:38:30.498+02:00 level=INFO source=server.go:388 msg="starting llama server" cmd="/leonardo_work/IscrC_LLM-Mob/tmp_ollama_19945716/ollama696725503/runners/cuda_v12/ollama_llama_server --model /leonardo_work/IscrC_LLM-Mob/.ollama/models/blobs/sha256-2bada8a7450677000f678be90653b85d364de7db25eb5ea54136ada5f3933730 --ctx-size 1024 --batch-size 256 --embedding --n-gpu-layers 1 --verbose --threads 56 --flash-attn --parallel 1 --port 46849"
time=2025-09-13T16:38:30.498+02:00 level=DEBUG source=server.go:405 msg=subprocess environment="[CUDA_VISIBLE_DEVICES=GPU-8efa7ef6-1cbc-ec7b-6246-cd6e2bf7d1d4 LD_LIBRARY_PATH=/leonardo_work/IscrC_LLM-Mob/opt/lib/ollama:/leonardo_work/IscrC_LLM-Mob/tmp_ollama_19945716/ollama696725503/runners/cuda_v12:/leonardo/prod/opt/compilers/cuda/12.3/none/lib64:/leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/python-3.11.6-i5k3c6ggftqkzgqyymfbkynpgm2lgjtd/lib:/leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/util-linux-uuid-2.38.1-jkdi7kvvma7367qdmvpkada4pyiafoud/lib:/leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/sqlite-3.43.2-casyrltocz5edzjhs5vzlqhwkamn7y4a/lib:/leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/libxcrypt-4.4.35-ss2rzin25ozjy4gyy3dack36njs6navg/lib:/leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/libffi-3.4.4-6r7brdq5dnreoad6f7sn7ybjjvwdmvue/lib64:/leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/libffi-3.4.4-6r7brdq5dnreoad6f7sn7ybjjvwdmvue/lib:/leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/gettext-0.22.3-2g7elifkgxzpypbswqnjuu5hefn4mjts/lib:/leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/zstd-1.5.5-gawytflrhedqdc2riwax7oduoqddx22s/lib:/leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/libxml2-2.10.3-5eeeokp4kszufozbayq4bewwmyeuwy27/lib:/leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/zlib-ng-2.1.4-6htiapkoa6fx2medhyabzo575skozuir/lib:/leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/xz-5.4.1-hubmwr5wc5nf6zk3ghuaikxiejuyt6bi/lib:/leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/libiconv-1.17-d7yvx2s6da4x2rfx44bc3perbb33rvuy/lib:/leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/gdbm-1.23-fs6otcki47azeywcckquj2sy4mzsnzxg/lib:/leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/readline-8.2-nyw6mp6b7dvizewrf7exopvap2q32s5j/lib:/leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/ncurses-6.4-asx3jea367shsxjt6bdj2bu5olxll6ni/lib:/leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/expat-2.5.0-bptl3xwbvbkxxoc5x3dhviorarv4dvxv/lib:/leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/libbsd-0.11.7-cgxjopleu7se4y4cgi7oefbljgasr457/lib:/leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/libmd-1.0.4-wja3f5q3w75tqtro333fptfnji7oqxiu/lib:/leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/bzip2-1.0.8-gp5wcz5lksrbm2gqiqjppumrhjz6gahy/lib ROCR_VISIBLE_DEVICES=0,1 CUDA_HOME=/leonardo/prod/opt/compilers/cuda/12.3/none PATH=/leonardo_work/IscrC_LLM-Mob/venv/bin:/leonardo/prod/opt/compilers/cuda/12.3/none/cuda-samples/bin/x86_64/linux/release:/leonardo/prod/opt/compilers/cuda/12.3/none/bin:/leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/python-3.11.6-i5k3c6ggftqkzgqyymfbkynpgm2lgjtd/bin:/leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/util-linux-uuid-2.38.1-jkdi7kvvma7367qdmvpkada4pyiafoud/bin:/leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/sqlite-3.43.2-casyrltocz5edzjhs5vzlqhwkamn7y4a/bin:/leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/gettext-0.22.3-2g7elifkgxzpypbswqnjuu5hefn4mjts/bin:/leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/tar-1.34-amqus5s63wb5oknfz4pfnvdqtwwe76iw/bin:/leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/zstd-1.5.5-gawytflrhedqdc2riwax7oduoqddx22s/bin:/leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/pigz-2.7-bopr5vpavkyl3lou5sap6iydgi3udtcm/bin:/leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/libxml2-2.10.3-5eeeokp4kszufozbayq4bewwmyeuwy27/bin:/leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/xz-5.4.1-hubmwr5wc5nf6zk3ghuaikxiejuyt6bi/bin:/leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/libiconv-1.17-d7yvx2s6da4x2rfx44bc3perbb33rvuy/bin:/leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/gdbm-1.23-fs6otcki47azeywcckquj2sy4mzsnzxg/bin:/leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/readline-8.2-nyw6mp6b7dvizewrf7exopvap2q32s5j/bin:/leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/ncurses-6.4-asx3jea367shsxjt6bdj2bu5olxll6ni/bin:/leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/expat-2.5.0-bptl3xwbvbkxxoc5x3dhviorarv4dvxv/bin:/leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/bzip2-1.0.8-gp5wcz5lksrbm2gqiqjppumrhjz6gahy/bin:/leonardo_work/IscrC_LLM-Mob/venv/bin:/leonardo/home/userexternal/smattiol/opt/ollama/bin:/cineca/bin:/leonardo/home/userexternal/smattiol/.vscode-server/cli/servers/Stable-f220831ea2d946c0dcb0f3eaa480eb435a2c1260/server/bin/remote-cli:/leonardo/home/userexternal/smattiol/opt/ollama/bin:/cineca/bin:/leonardo/home/userexternal/smattiol/.nvm/versions/node/v22.18.0/bin:/leonardo/home/userexternal/smattiol/opt/ollama/bin:/leonardo/home/userexternal/smattiol/.local/bin:/leonardo/home/userexternal/smattiol/bin:/cineca/bin:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/environment-modules-5.2.0-rz47odw4phlhzhhbz7b65nv5s5othgmi/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/leonardo/home/userexternal/smattiol/.vscode-server/extensions/ms-python.debugpy-2025.10.0-linux-x64/bundled/scripts/noConfigScripts GPU_DEVICE_ORDINAL=0,1]"
time=2025-09-13T16:38:30.499+02:00 level=INFO source=sched.go:449 msg="loaded runners" count=1
time=2025-09-13T16:38:30.499+02:00 level=DEBUG source=sched.go:575 msg="evaluating already loaded" model=/leonardo_work/IscrC_LLM-Mob/.ollama/models/blobs/sha256-2bada8a7450677000f678be90653b85d364de7db25eb5ea54136ada5f3933730
time=2025-09-13T16:38:30.499+02:00 level=INFO source=server.go:587 msg="waiting for llama runner to start responding"
time=2025-09-13T16:38:30.499+02:00 level=INFO source=server.go:621 msg="waiting for server to become available" status="llm server error"
INFO [main] starting c++ runner | tid="23436331696128" timestamp=1757774310
INFO [main] build info | build=10 commit="3a8c75e" tid="23436331696128" timestamp=1757774310
INFO [main] system info | n_threads=56 n_threads_batch=56 system_info="AVX = 1 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 0 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 0 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | " tid="23436331696128" timestamp=1757774310 total_threads=32
INFO [main] HTTP server listening | hostname="127.0.0.1" n_threads_http="31" port="46849" tid="23436331696128" timestamp=1757774310
llama_model_loader: loaded meta data with 34 key-value pairs and 339 tensors from /leonardo_work/IscrC_LLM-Mob/.ollama/models/blobs/sha256-2bada8a7450677000f678be90653b85d364de7db25eb5ea54136ada5f3933730 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 7B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5
llama_model_loader: - kv   5:                         general.size_label str              = 7B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-7...
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 7B
llama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen
llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-7B
llama_model_loader: - kv  12:                               general.tags arr[str,2]       = ["chat", "text-generation"]
llama_model_loader: - kv  13:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  14:                          qwen2.block_count u32              = 28
llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768
llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  22:                          general.file_type u32              = 15
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  33:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
time=2025-09-13T16:38:30.750+02:00 level=INFO source=server.go:621 msg="waiting for server to become available" status="llm server loading model"
llm_load_vocab: special tokens cache size = 22
llm_load_vocab: token to piece cache size = 0.9310 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = qwen2
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 152064
llm_load_print_meta: n_merges         = 151387
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 32768
llm_load_print_meta: n_embd           = 3584
llm_load_print_meta: n_layer          = 28
llm_load_print_meta: n_head           = 28
llm_load_print_meta: n_head_kv        = 4
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 7
llm_load_print_meta: n_embd_k_gqa     = 512
llm_load_print_meta: n_embd_v_gqa     = 512
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-06
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 18944
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 1000000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 32768
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = ?B
llm_load_print_meta: model ftype      = Q4_K - Medium
llm_load_print_meta: model params     = 7.62 B
llm_load_print_meta: model size       = 4.36 GiB (4.91 BPW) 
llm_load_print_meta: general.name     = Qwen2.5 7B Instruct
llm_load_print_meta: BOS token        = 151643 '<|endoftext|>'
llm_load_print_meta: EOS token        = 151645 '<|im_end|>'
llm_load_print_meta: PAD token        = 151643 '<|endoftext|>'
llm_load_print_meta: LF token         = 148848 'ÄĬ'
llm_load_print_meta: EOT token        = 151643 '<|endoftext|>'
llm_load_print_meta: EOG token        = 151643 '<|endoftext|>'
llm_load_print_meta: EOG token        = 151645 '<|im_end|>'
llm_load_print_meta: max token length = 256
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA A100-SXM-64GB, compute capability 8.0, VMM: yes
llm_load_tensors: ggml ctx size =    0.30 MiB
llm_load_tensors: offloading 1 repeating layers to GPU
llm_load_tensors: offloaded 1/29 layers to GPU
llm_load_tensors:        CPU buffer size =  4460.45 MiB
llm_load_tensors:      CUDA0 buffer size =   142.21 MiB
llama_new_context_with_model: n_ctx      = 1024
llama_new_context_with_model: n_batch    = 256
llama_new_context_with_model: n_ubatch   = 256
llama_new_context_with_model: flash_attn = 1
llama_new_context_with_model: freq_base  = 1000000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:  CUDA_Host KV buffer size =    54.00 MiB
llama_kv_cache_init:      CUDA0 KV buffer size =     2.00 MiB
llama_new_context_with_model: KV self size  =   56.00 MiB, K (f16):   28.00 MiB, V (f16):   28.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =     0.59 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   578.36 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     4.50 MiB
llama_new_context_with_model: graph nodes  = 875
llama_new_context_with_model: graph splits = 382
time=2025-09-13T16:38:31.754+02:00 level=DEBUG source=server.go:632 msg="model load progress 1.00"
time=2025-09-13T16:38:32.004+02:00 level=DEBUG source=server.go:635 msg="model load completed, waiting for server to become available" status="llm server loading model"
DEBUG [initialize] initializing slots | n_slots=1 tid="23436331696128" timestamp=1757774334
DEBUG [initialize] new slot | n_ctx_slot=1024 slot_id=0 tid="23436331696128" timestamp=1757774334
INFO [main] model loaded | tid="23436331696128" timestamp=1757774334
DEBUG [update_slots] all slots are idle and system prompt is empty, clear the KV cache | tid="23436331696128" timestamp=1757774334
DEBUG [process_single_task] slot data | n_idle_slots=1 n_processing_slots=0 task_id=0 tid="23436331696128" timestamp=1757774334
time=2025-09-13T16:38:54.551+02:00 level=INFO source=server.go:626 msg="llama runner started in 24.05 seconds"
time=2025-09-13T16:38:54.551+02:00 level=DEBUG source=sched.go:462 msg="finished setting up runner" model=/leonardo_work/IscrC_LLM-Mob/.ollama/models/blobs/sha256-2bada8a7450677000f678be90653b85d364de7db25eb5ea54136ada5f3933730
DEBUG [process_single_task] slot data | n_idle_slots=1 n_processing_slots=0 task_id=1 tid="23436331696128" timestamp=1757774334
DEBUG [process_single_task] slot data | n_idle_slots=1 n_processing_slots=0 task_id=2 tid="23436331696128" timestamp=1757774334
DEBUG [log_server_request] request | method="POST" params={} path="/tokenize" remote_addr="127.0.0.1" remote_port=55464 status=200 tid="23436216737792" timestamp=1757774334
time=2025-09-13T16:38:54.597+02:00 level=DEBUG source=routes.go:1422 msg="chat request" images=0 prompt="<|im_start|>system\nYou are a tourism prediction assistant in Verona, Italy.<|im_end|>\n<|im_start|>user\n\n            - Cluster: 3\n            - History: Casa Giulietta, Arena\n            - Current: Castelvecchio (Tue 10:30 (usual: 9-16h))\n\n            Nearby attractions within walking distance: Museo Lapidario (0.3km), Verona Tour (0.3km), Museo Radio (0.7km), San Zeno (0.7km), Sighseeing (0.8km), Museo Miniscalchi (0.9km), Torre Lamberti (0.9km), Centro Fotografia (0.9km), San Fermo (1.0km), Palazzo della Ragione (1.0km)\n\n            Predict exactly 5 most likely next destinations for this tourist\n\n            OUTPUT FORMAT: {\"prediction\": [\"poi1\", \"poi2\", \"poi3\", \"poi4\", \"poi5\"]}<|im_end|>\n<|im_start|>assistant\n"
DEBUG [process_single_task] slot data | n_idle_slots=1 n_processing_slots=0 task_id=3 tid="23436331696128" timestamp=1757774334
DEBUG [process_single_task] slot data | n_idle_slots=1 n_processing_slots=0 task_id=4 tid="23436331696128" timestamp=1757774334
DEBUG [launch_slot_with_data] slot is processing task | slot_id=0 task_id=5 tid="23436331696128" timestamp=1757774334
DEBUG [update_slots] slot progression | ga_i=0 n_past=0 n_past_se=0 n_prompt_tokens_processed=213 slot_id=0 task_id=5 tid="23436331696128" timestamp=1757774334
DEBUG [update_slots] kv cache rm [p0, end) | p0=0 slot_id=0 task_id=5 tid="23436331696128" timestamp=1757774334
DEBUG [log_server_request] request | method="POST" params={} path="/tokenize" remote_addr="127.0.0.1" remote_port=55458 status=200 tid="23436218839040" timestamp=1757774334
time=2025-09-13T16:38:54.684+02:00 level=DEBUG source=routes.go:1422 msg="chat request" images=0 prompt="<|im_start|>system\nYou are a tourism prediction assistant in Verona, Italy.<|im_end|>\n<|im_start|>user\n\n            - Cluster: 3\n            - History: San Fermo, Santa Anastasia, Teatro Romano, Duomo\n            - Current: Torre Lamberti (Mon 16:39 (usual: 12-15h))\n\n            Nearby attractions within walking distance: Sighseeing (0.0km), Centro Fotografia (0.1km), Museo Conte (0.1km), Casa Giulietta (0.1km), Palazzo della Ragione (0.2km), AMO (0.3km), Museo Miniscalchi (0.3km), Museo Radio (0.3km), Arena (0.5km), Verona Tour (0.6km)\n\n            Predict exactly 5 most likely next destinations for this tourist\n\n            OUTPUT FORMAT: {\"prediction\": [\"poi1\", \"poi2\", \"poi3\", \"poi4\", \"poi5\"]}<|im_end|>\n<|im_start|>assistant\n"
[GIN] 2025/09/13 - 16:39:47 | 200 |    8.622947ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/09/13 - 16:42:47 | 200 |   19.877238ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/09/13 - 16:45:48 | 200 |    7.707719ms |       127.0.0.1 | GET      "/api/tags"
