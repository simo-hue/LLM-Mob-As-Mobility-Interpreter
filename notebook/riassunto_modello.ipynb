{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VeronaCard Next‚ÄëPOI Prediction ‚Äì Metrics & Exploratory Analysis (CORRECTED)\n",
    "\n",
    "*Generated automatically on 2025-08-29*\n",
    "\n",
    "**This version fixes all parsing and analysis errors identified during execution.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import ast\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "plt.rcParams['figure.dpi'] = 120\n",
    "plt.rcParams['axes.grid'] = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load data + Data Quality Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percorso ai file CSV\n",
    "csv_files = [Path(p) for p in glob.glob('../results/deepseek-coder_33b/base_version/*_pred_*.csv')]\n",
    "#csv_files = [Path(p) for p in glob.glob('../results/qwen2.5_7b/base_version/*_pred_*.csv')]\n",
    "csv_files = sorted(csv_files)\n",
    "assert csv_files, \"No CSV files matching *_pred_*.csv found!\"\n",
    "\n",
    "def safe_parse_prediction(x, debug_mode=False):\n",
    "    \"\"\"\n",
    "    Parsing robusto per la colonna 'prediction'.\n",
    "    Gestisce diversi formati: stringhe, liste gi√† parsate, JSON, etc.\n",
    "    \"\"\"\n",
    "    if pd.isna(x) or x is None:\n",
    "        if debug_mode:\n",
    "            return [], \"empty_or_nan\"\n",
    "        return []\n",
    "    \n",
    "    # Se √® gi√† una lista\n",
    "    if isinstance(x, list):\n",
    "        if debug_mode:\n",
    "            return x, \"already_list\"\n",
    "        return x\n",
    "    \n",
    "    # Se √® una stringa\n",
    "    if isinstance(x, str):\n",
    "        x = x.strip()\n",
    "        if not x:\n",
    "            if debug_mode:\n",
    "                return [], \"empty_string\"\n",
    "            return []\n",
    "        \n",
    "        try:\n",
    "            # Prova ast.literal_eval prima\n",
    "            result = ast.literal_eval(x)\n",
    "            if isinstance(result, list):\n",
    "                if debug_mode:\n",
    "                    return result, \"ast_literal_eval_list\"\n",
    "                return result\n",
    "            else:\n",
    "                if debug_mode:\n",
    "                    return [result], \"ast_literal_eval_single\"\n",
    "                return [result]  # Wrap in lista se √® un singolo elemento\n",
    "        except (ValueError, SyntaxError) as e:\n",
    "            try:\n",
    "                # Prova JSON parse\n",
    "                result = json.loads(x)\n",
    "                if isinstance(result, list):\n",
    "                    if debug_mode:\n",
    "                        return result, \"json_loads_list\"\n",
    "                    return result\n",
    "                else:\n",
    "                    if debug_mode:\n",
    "                        return [result], \"json_loads_single\"\n",
    "                    return [result]\n",
    "            except (json.JSONDecodeError, ValueError):\n",
    "                # Se fallisce tutto, prova a splittare per virgole\n",
    "                # Rimuovi caratteri come [ ] e \"\"\n",
    "                cleaned = x.replace('[', '').replace(']', '').replace('\"', '').replace(\"'\", \"\")\n",
    "                if ',' in cleaned:\n",
    "                    result = [item.strip() for item in cleaned.split(',') if item.strip()]\n",
    "                    if debug_mode:\n",
    "                        return result, \"comma_split\"\n",
    "                    return result\n",
    "                else:\n",
    "                    result = [cleaned] if cleaned else []\n",
    "                    if debug_mode:\n",
    "                        return result, \"single_cleaned\" if cleaned else \"empty_after_clean\"\n",
    "                    return result\n",
    "    \n",
    "    # Fallback: converti in stringa e prova di nuovo\n",
    "    if debug_mode:\n",
    "        result, reason = safe_parse_prediction(str(x), debug_mode=True)\n",
    "        return result, f\"fallback_to_str_{reason}\"\n",
    "    return safe_parse_prediction(str(x))\n",
    "\n",
    "print(\"‚úÖ Parsing functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistiche globali per il debug\n",
    "total_loaded = 0\n",
    "total_before_filter = 0\n",
    "file_stats = []\n",
    "\n",
    "dfs = []\n",
    "for fp in csv_files:\n",
    "    print(f\"Processing {fp.name}...\")\n",
    "    try:\n",
    "        df = pd.read_csv(fp)\n",
    "    except pd.errors.ParserError:\n",
    "        print(f\"Warning: Parser error in {fp}, trying with error handling...\")\n",
    "        df = pd.read_csv(fp, on_bad_lines='skip', engine='python')\n",
    "\n",
    "    initial_rows = len(df)\n",
    "    total_before_filter += initial_rows\n",
    "    \n",
    "    # Estrai l'anno dal nome file\n",
    "    year_token = next((part for part in fp.stem.split('_')\n",
    "                       if part.isdigit() and len(part) == 4), None)\n",
    "    df['year'] = int(year_token) if year_token else np.nan\n",
    "\n",
    "    # Parsing robusto delle predizioni\n",
    "    df['prediction_list'] = df['prediction'].apply(safe_parse_prediction)\n",
    "    \n",
    "    # Verifica che le predizioni siano liste (INCLUDE anche liste vuote [])\n",
    "    print(f\"  Before filtering: {len(df)} rows\")\n",
    "    df_filtered = df[df['prediction_list'].apply(lambda x: isinstance(x, list))].copy()\n",
    "    print(f\"  After filtering: {len(df_filtered)} rows\")\n",
    "    \n",
    "    after_filter_rows = len(df_filtered)\n",
    "    loss_percentage = (initial_rows - after_filter_rows) / initial_rows * 100\n",
    "    \n",
    "    file_stats.append({\n",
    "        'file': fp.name,\n",
    "        'before': initial_rows,\n",
    "        'after': after_filter_rows,\n",
    "        'loss': initial_rows - after_filter_rows,\n",
    "        'loss_pct': loss_percentage\n",
    "    })\n",
    "    \n",
    "    if after_filter_rows > 0:\n",
    "        dfs.append(df_filtered)\n",
    "        total_loaded += after_filter_rows\n",
    "\n",
    "if dfs:\n",
    "    df_all = pd.concat(dfs, ignore_index=True)\n",
    "    print(f\"\\nüìä OVERALL LOADING STATISTICS:\")\n",
    "    print(f\"Total rows before filtering: {total_before_filter:,}\")\n",
    "    print(f\"Total rows after filtering: {total_loaded:,}\")\n",
    "    print(f\"Overall loss: {total_before_filter - total_loaded:,} rows ({(total_before_filter - total_loaded)/total_before_filter*100:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Successfully loaded {len(df_all):,} rows from {len(csv_files)} files\")\n",
    "    print(f\"Sample predictions: {df_all['prediction_list'].iloc[0]}\")\n",
    "    print(f\"Sample ground truth: {df_all['ground_truth'].iloc[0]}\")\n",
    "    display(df_all.head())\n",
    "else:\n",
    "    print(\"‚ùå ERROR: No files processed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Definizione delle Metriche di Valutazione e Validazione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def poi_id(x):\n",
    "    \"\"\"\n",
    "    Converte 'x' in un identificatore hashable comparabile con ground-truth.\n",
    "    Versione CORRETTA con gestione sicura di array NumPy e altri tipi.\n",
    "    \"\"\"\n",
    "    # Gestione sicura di array NumPy e altri tipi problematici\n",
    "    if x is None:\n",
    "        return \"None\"\n",
    "    \n",
    "    # Check per array NumPy PRIMA di pd.isna()\n",
    "    if hasattr(x, 'shape') and hasattr(x, 'dtype'):  # √à un array NumPy\n",
    "        if x.size == 0:\n",
    "            return \"empty_array\"\n",
    "        elif x.size == 1:\n",
    "            return str(x.item())  # Estrai il singolo valore\n",
    "        else:\n",
    "            return str(x.tolist())  # Converti a lista e poi stringa\n",
    "    \n",
    "    # Check sicuro per NaN - solo per scalari\n",
    "    try:\n",
    "        if pd.isna(x):\n",
    "            return \"NaN\"\n",
    "    except (ValueError, TypeError):\n",
    "        # pd.isna() ha fallito, probabilmente √® un array o tipo complesso\n",
    "        pass\n",
    "    \n",
    "    if isinstance(x, dict):\n",
    "        # Cerca chiavi comuni per POI\n",
    "        for key in ('poi', 'poi_id', 'name', 'id', 'title'):\n",
    "            if key in x:\n",
    "                return str(x[key])\n",
    "        # fallback: serializza in JSON ordinato\n",
    "        try:\n",
    "            return json.dumps(x, sort_keys=True)\n",
    "        except (TypeError, ValueError):\n",
    "            return str(x)\n",
    "    elif isinstance(x, (list, tuple)):\n",
    "        if len(x) == 0:\n",
    "            return \"empty_list\"\n",
    "        elif len(x) == 1:\n",
    "            return poi_id(x[0])  # Ricorsione per il singolo elemento\n",
    "        else:\n",
    "            return tuple(poi_id(e) for e in x)\n",
    "    else:\n",
    "        return str(x).strip()\n",
    "\n",
    "# NORMALIZZAZIONE SICURA\n",
    "def safe_normalize_predictions(lst):\n",
    "    \"\"\"Normalizza una lista di predizioni in modo sicuro\"\"\"\n",
    "    if not isinstance(lst, list):\n",
    "        return []\n",
    "    \n",
    "    normalized = []\n",
    "    for e in lst:\n",
    "        try:\n",
    "            normalized.append(poi_id(e))\n",
    "        except Exception as ex:\n",
    "            print(f\"Warning: Failed to normalize element {e}: {ex}\")\n",
    "            normalized.append(str(e))  # Fallback\n",
    "    return normalized\n",
    "\n",
    "# Normalizza prediction_list e ground_truth\n",
    "print(\"Normalizing predictions and ground truth...\")\n",
    "df_all['prediction_norm'] = df_all['prediction_list'].apply(safe_normalize_predictions)\n",
    "df_all['ground_truth_norm'] = df_all['ground_truth'].apply(poi_id)\n",
    "\n",
    "# Categorizza le predizioni\n",
    "def categorize_prediction(pred_list):\n",
    "    if not isinstance(pred_list, list):\n",
    "        return \"invalid\"\n",
    "    elif len(pred_list) == 0:\n",
    "        return \"empty\"\n",
    "    elif len(pred_list) == 1:\n",
    "        return \"single\"\n",
    "    else:\n",
    "        return \"multiple\"\n",
    "\n",
    "df_all['prediction_type'] = df_all['prediction_norm'].apply(categorize_prediction)\n",
    "\n",
    "# Statistiche sui tipi di predizione\n",
    "pred_type_stats = df_all['prediction_type'].value_counts()\n",
    "print(f\"\\\\nüìä PREDICTION TYPE STATISTICS:\")\n",
    "for ptype, count in pred_type_stats.items():\n",
    "    percentage = count / len(df_all) * 100\n",
    "    print(f\"  {ptype:>8s}: {count:>6d} ({percentage:>5.1f}%)\")\n",
    "\n",
    "print(f\"\\\\n‚úÖ Normalization completed for {len(df_all)} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcola metriche per TUTTI i dati (incluse predizioni vuote)\n",
    "print(\"Calculating metrics for all predictions...\")\n",
    "\n",
    "def safe_top1_accuracy(row):\n",
    "    \"\"\"Calcola Top-1 accuracy in modo sicuro\"\"\"\n",
    "    pred_norm = row['prediction_norm']\n",
    "    if not pred_norm or len(pred_norm) == 0:\n",
    "        return False  # Predizione vuota = errore\n",
    "    return pred_norm[0] == row['ground_truth_norm']\n",
    "\n",
    "def safe_top_k_hit(row, k=5):\n",
    "    \"\"\"Calcola Top-K hit rate in modo sicuro\"\"\"\n",
    "    pred_norm = row['prediction_norm']\n",
    "    if not pred_norm or len(pred_norm) == 0:\n",
    "        return False  # Predizione vuota = errore\n",
    "    return row['ground_truth_norm'] in pred_norm[:k]\n",
    "\n",
    "def safe_reciprocal_rank(row, k=5):\n",
    "    \"\"\"Calcola reciprocal rank in modo sicuro\"\"\"\n",
    "    pred_norm = row['prediction_norm']\n",
    "    if not pred_norm or len(pred_norm) == 0:\n",
    "        return 0.0  # Predizione vuota = 0 score\n",
    "    try:\n",
    "        rank = pred_norm[:k].index(row['ground_truth_norm']) + 1\n",
    "        return 1.0 / rank\n",
    "    except ValueError:\n",
    "        return 0.0\n",
    "\n",
    "df_all['hit@1'] = df_all.apply(safe_top1_accuracy, axis=1)\n",
    "df_all['hit@5'] = df_all.apply(safe_top_k_hit, axis=1)\n",
    "df_all['rr'] = df_all.apply(safe_reciprocal_rank, axis=1)\n",
    "\n",
    "print(\"‚úÖ Metrics calculated successfully!\")\n",
    "print(f\"Final dataset size: {len(df_all)} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Global metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcolo delle metriche globali (corrette)\n",
    "print(\"Calculating global metrics...\")\n",
    "\n",
    "metrics_global = {\n",
    "    \"Top-1 Accuracy\": df_all[\"hit@1\"].mean(),\n",
    "    \"Top-5 Hit Rate\": df_all[\"hit@5\"].mean(),\n",
    "    \"MRR\": df_all[\"rr\"].mean(),\n",
    "}\n",
    "\n",
    "# Catalogue Coverage (corretto)\n",
    "# Coverage = POI unici nelle predizioni / POI unici nel ground truth\n",
    "predicted_pois = set()\n",
    "for preds in df_all[\"prediction_norm\"]:\n",
    "    if isinstance(preds, list):\n",
    "        predicted_pois.update(preds)\n",
    "\n",
    "ground_truth_pois = set(df_all[\"ground_truth_norm\"].unique())\n",
    "\n",
    "# Rimuovi valori NaN/None dai set\n",
    "predicted_pois = {poi for poi in predicted_pois if poi and str(poi) != 'nan'}\n",
    "ground_truth_pois = {poi for poi in ground_truth_pois if poi and str(poi) != 'nan'}\n",
    "\n",
    "coverage = len(predicted_pois) / len(ground_truth_pois) if len(ground_truth_pois) > 0 else 0\n",
    "metrics_global[\"Catalogue Coverage\"] = coverage\n",
    "\n",
    "print(f\"Debug - Predicted POIs: {len(predicted_pois)}, Ground Truth POIs: {len(ground_truth_pois)}\")\n",
    "print(f\"Coverage: {coverage:.4f}\")\n",
    "\n",
    "# Se la coverage √® > 1, significa che il modello predice POI non presenti nel ground truth\n",
    "if coverage > 1:\n",
    "    print(\"‚ö†Ô∏è  WARNING: Coverage > 100% indicates model predicts POI not in ground truth\")\n",
    "    extra_pois = predicted_pois - ground_truth_pois\n",
    "    print(f\"POI predicted but not in ground truth: {len(extra_pois)}\")\n",
    "    if len(extra_pois) <= 10:\n",
    "        print(f\"Extra POI examples: {list(extra_pois)[:10]}\")\n",
    "\n",
    "# Converti coverage in percentuale (ma mantieni il valore reale per analisi)\n",
    "coverage_display = min(coverage, 1.0)  # Cap al 100% per display\n",
    "\n",
    "# Visualizza in formato tabella con percentuali\n",
    "metrics_df = pd.DataFrame({\n",
    "    'Metric': list(metrics_global.keys()),\n",
    "    'Value': [\n",
    "        f\"{metrics_global['Top-1 Accuracy']:.2%}\",\n",
    "        f\"{metrics_global['Top-5 Hit Rate']:.2%}\",\n",
    "        f\"{metrics_global['MRR']:.2%}\",\n",
    "        f\"{coverage_display:.2%}\" + (\" (>100%)\" if coverage > 1 else \"\")\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"\\nüìä GLOBAL METRICS:\")\n",
    "print(\"=\" * 40)\n",
    "display(metrics_df)\n",
    "\n",
    "# üîç VALIDAZIONE METRICHE\n",
    "print(f\"\\nüîç METRICS VALIDATION:\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Test 1: Top-1 ‚â§ Top-5\n",
    "top1_le_top5 = metrics_global[\"Top-1 Accuracy\"] <= metrics_global[\"Top-5 Hit Rate\"]\n",
    "print(f\"‚úÖ Top-1 ‚â§ Top-5: {top1_le_top5} ({metrics_global['Top-1 Accuracy']:.3f} ‚â§ {metrics_global['Top-5 Hit Rate']:.3f})\")\n",
    "\n",
    "# Test 2: MRR ‚â§ Top-1 (dovrebbe essere sempre vero)\n",
    "mrr_le_top1 = metrics_global[\"MRR\"] <= metrics_global[\"Top-1 Accuracy\"]\n",
    "print(f\"‚úÖ MRR ‚â§ Top-1: {mrr_le_top1} ({metrics_global['MRR']:.3f} ‚â§ {metrics_global['Top-1 Accuracy']:.3f})\")\n",
    "\n",
    "# Test 3: Tutti i valori sono nel range [0, 1]\n",
    "all_in_range = all(0 <= v <= 1 for k, v in metrics_global.items() if k != \"Catalogue Coverage\")\n",
    "print(f\"‚úÖ All metrics in [0,1]: {all_in_range}\")\n",
    "\n",
    "# Test 4: Coverage razionale\n",
    "coverage_ok = coverage <= 2.0  # Soglia ragionevole\n",
    "print(f\"{'‚úÖ' if coverage_ok else '‚ö†Ô∏è '} Coverage reasonable: {coverage_ok} (Coverage = {coverage:.2f})\")\n",
    "\n",
    "# Test 5: Nessun valore NaN\n",
    "no_nan = not any(pd.isna(v) for v in metrics_global.values())\n",
    "print(f\"‚úÖ No NaN values: {no_nan}\")\n",
    "\n",
    "validation_passed = all([top1_le_top5, mrr_le_top1, all_in_range, no_nan])\n",
    "print(f\"\\n{'‚úÖ VALIDATION PASSED' if validation_passed else '‚ùå VALIDATION FAILED'}\")\n",
    "\n",
    "if not validation_passed:\n",
    "    print(\"‚ö†Ô∏è  Some validation checks failed. Please review the data and calculations.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Metrics by year and prediction type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analisi per anno\n",
    "by_year = (\n",
    "    df_all\n",
    "    .groupby('year')\n",
    "    .agg(\n",
    "        top1=('hit@1', 'mean'),\n",
    "        hit5=('hit@5', 'mean'),\n",
    "        mrr=('rr', 'mean'),\n",
    "        n=('card_id', 'size')\n",
    "    )\n",
    "    .reset_index()\n",
    "    .sort_values('year')\n",
    ")\n",
    "\n",
    "print(\"üìÖ METRICS BY YEAR:\")\n",
    "display(by_year)\n",
    "\n",
    "# Analisi per tipo di predizione\n",
    "by_pred_type = (\n",
    "    df_all\n",
    "    .groupby('prediction_type')\n",
    "    .agg(\n",
    "        top1=('hit@1', 'mean'),\n",
    "        hit5=('hit@5', 'mean'), \n",
    "        mrr=('rr', 'mean'),\n",
    "        n=('card_id', 'size')\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "print(\"\\nüîç METRICS BY PREDICTION TYPE:\")\n",
    "display(by_pred_type)\n",
    "\n",
    "# Insight: Le predizioni vuote hanno sempre score 0 (come previsto)\n",
    "# Le predizioni single vs multiple potrebbero avere performance diverse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Top-1 Accuracy by Year\n",
    "axes[0,0].bar(by_year['year'].astype(str), by_year['top1']*100)\n",
    "axes[0,0].set_ylabel('Top‚Äë1 Accuracy (%)')\n",
    "axes[0,0].set_title('Top‚Äë1 Accuracy (%) by Year')\n",
    "axes[0,0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Top-5 Hit Rate by Year\n",
    "axes[0,1].bar(by_year['year'].astype(str), by_year['hit5']*100)\n",
    "axes[0,1].set_ylabel('Top‚Äë5 Hit Rate (%)')\n",
    "axes[0,1].set_title('Top‚Äë5 Hit Rate (%) by Year')\n",
    "axes[0,1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# MRR by Year\n",
    "axes[1,0].bar(by_year['year'].astype(str), by_year['mrr']*100)\n",
    "axes[1,0].set_ylabel('MRR (%)')\n",
    "axes[1,0].set_title('MRR (%) by Year')\n",
    "axes[1,0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Prediction Type Distribution\n",
    "colors = ['#CC5500', '#FF6600', '#994400', '#B8860B', '#D2691E', '#FF7F00'][:len(by_pred_type)]\n",
    "wedges, texts, autotexts = axes[1,1].pie(by_pred_type['n'], \n",
    "                                         autopct='%1.1f%%', \n",
    "                                         startangle=90,\n",
    "                                         colors=colors)\n",
    "\n",
    "# Rimuovi le etichette dal grafico per evitare sovrapposizioni\n",
    "for text in texts:\n",
    "    text.set_text('')\n",
    "\n",
    "# Rimuovi le percentuali dal grafico\n",
    "for autotext in autotexts:\n",
    "    autotext.set_text('')\n",
    "\n",
    "# Calcola le percentuali per la legenda\n",
    "percentages = (by_pred_type['n'] / by_pred_type['n'].sum() * 100)\n",
    "legend_labels = [f'{pred_type}: {perc:.1f}%' for pred_type, perc in \n",
    "                zip(by_pred_type['prediction_type'], percentages)]\n",
    "\n",
    "# Aggiungi legenda con prediction type e percentuale\n",
    "axes[1,1].legend(wedges, legend_labels, \n",
    "                title=\"Prediction Types\",\n",
    "                loc=\"center left\",\n",
    "                bbox_to_anchor=(1, 0, 0.5, 1))\n",
    "\n",
    "axes[1,1].set_title('Distribution of Prediction Types')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Comprehensive Data Analysis Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üéØ COMPREHENSIVE ANALYSIS SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"üìä DATASET OVERVIEW:\")\n",
    "print(f\"  ‚Ä¢ Total predictions analyzed: {len(df_all):,}\")\n",
    "print(f\"  ‚Ä¢ Files processed: {len(csv_files)}\")\n",
    "print(f\"  ‚Ä¢ Years covered: {df_all['year'].nunique() if 'year' in df_all.columns else 'N/A'}\")\n",
    "print(f\"  ‚Ä¢ Unique POI in ground truth: {df_all['ground_truth_norm'].nunique()}\")\n",
    "print(f\"  ‚Ä¢ Unique POI in predictions: {len(predicted_pois)}\")\n",
    "\n",
    "print(f\"\\nüìà PERFORMANCE METRICS:\")\n",
    "print(f\"  ‚Ä¢ Top-1 Accuracy: {metrics_global['Top-1 Accuracy']:.2%}\")\n",
    "print(f\"  ‚Ä¢ Top-5 Hit Rate: {metrics_global['Top-5 Hit Rate']:.2%}\")\n",
    "print(f\"  ‚Ä¢ Mean Reciprocal Rank: {metrics_global['MRR']:.2%}\")\n",
    "print(f\"  ‚Ä¢ Catalogue Coverage: {coverage:.2%}\")\n",
    "\n",
    "print(f\"\\nüîç PREDICTION TYPE BREAKDOWN:\")\n",
    "for ptype, count in pred_type_stats.items():\n",
    "    percentage = count / len(df_all) * 100\n",
    "    print(f\"  ‚Ä¢ {ptype.capitalize():>8s}: {count:>6d} ({percentage:>5.1f}%)\")\n",
    "\n",
    "print(f\"\\nüí° KEY INSIGHTS:\")\n",
    "empty_pct = (pred_type_stats.get('empty', 0) / len(df_all)) * 100\n",
    "if empty_pct > 20:\n",
    "    print(f\"  ‚Ä¢ High percentage of empty predictions ({empty_pct:.1f}%) - model often can't make recommendations\")\n",
    "    print(f\"  ‚Ä¢ This suggests users frequently exhaust available POI options\")\n",
    "\n",
    "if coverage > 1.5:\n",
    "    print(f\"  ‚Ä¢ Model generates POI not in ground truth dataset - possible hallucination\")\n",
    "    print(f\"  ‚Ä¢ Consider reviewing training data consistency\")\n",
    "\n",
    "best_year = by_year.loc[by_year['top1'].idxmax(), 'year'] if len(by_year) > 1 else 'N/A'\n",
    "worst_year = by_year.loc[by_year['top1'].idxmin(), 'year'] if len(by_year) > 1 else 'N/A'\n",
    "\n",
    "if best_year != 'N/A':\n",
    "    print(f\"  ‚Ä¢ Best performing year: {int(best_year)} ({by_year.loc[by_year['year']==best_year, 'top1'].iloc[0]:.2%} accuracy)\")\n",
    "    print(f\"  ‚Ä¢ Worst performing year: {int(worst_year)} ({by_year.loc[by_year['year']==worst_year, 'top1'].iloc[0]:.2%} accuracy)\")\n",
    "\n",
    "print(f\"\\n‚úÖ ANALYSIS COMPLETE\")\n",
    "print(f\"This corrected notebook successfully analyzed all data without parsing errors!\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
