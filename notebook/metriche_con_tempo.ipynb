{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VeronaCard Next‑POI Prediction – Robust Analysis\n",
    "\n",
    "*Enhanced version with robust CSV loading - Generated automatically on 2025-08-27*\n",
    "\n",
    "## 🎯 Overview\n",
    "\n",
    "This notebook provides a **comprehensive evaluation** of the next-POI prediction model with **enhanced data loading capabilities** that handle malformed CSV files robustly.\n",
    "\n",
    "### 🔧 Key Improvements\n",
    "\n",
    "- **🛡️ Robust CSV Parsing**: Handles escaped quotes, malformed entries, and mixed quote types\n",
    "- **📊 Advanced Error Recovery**: Multiple fallback strategies for prediction parsing\n",
    "- **🔍 Data Quality Checks**: Automatic validation and cleaning of input data\n",
    "- **📈 Enhanced Visualizations**: Improved charts with better styling and annotations\n",
    "- **⚡ Performance Optimized**: Efficient processing for large datasets\n",
    "\n",
    "### 📋 Analysis Pipeline\n",
    "\n",
    "1. **🔄 Robust Data Loading**: Multi-strategy CSV parsing with error handling\n",
    "2. **📊 Comprehensive Metrics**: Top-1 Accuracy, Top-5 Hit Rate, MRR, Coverage\n",
    "3. **📈 Advanced Visualizations**: Interactive charts and performance dashboards\n",
    "4. **🔍 Deep Error Analysis**: Pattern recognition and failure mode analysis\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from pathlib import Path\n",
    "import glob, ast, os, json, re, csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting parameters\n",
    "plt.rcParams['figure.dpi'] = 120\n",
    "plt.rcParams['axes.grid'] = True\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "# Configuration\n",
    "DATA_DIR = \"../results/mixtral_8x7b/with_geom/\" \n",
    "# DATA_DIR = \"../results_mixtral_8x7b_with_geom/\" \n",
    "# DATA_DIR = \"../result_run_completa_with_geom_srv_univr/\"\n",
    "# DATA_DIR = \"../results/\"\n",
    "\n",
    "print(f\"📁 Loading data from: {DATA_DIR}\")\n",
    "print(f\"🔧 Enhanced robust parsing enabled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 🛡️ Robust Data Loading\n",
    "\n",
    "This section implements a multi-strategy approach to handle various CSV formatting issues:\n",
    "\n",
    "- **Escaped quotes handling**: `\"\"Basilica di Sant'Anastasia\"\"` → `'Basilica di Sant\\'Anastasia'`\n",
    "- **Mixed quote types**: Single and double quotes in the same string\n",
    "- **Malformed JSON**: Fallback parsing strategies\n",
    "- **Missing data**: Graceful handling of empty or corrupt entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def robust_parse_prediction(x, debug=False):\n",
    "    \"\"\"\n",
    "    Ultra-robust prediction parsing with multiple fallback strategies\n",
    "    \n",
    "    This function handles the specific issue in the CSV files where POI names\n",
    "    contain escaped quotes like: \"\"Basilica di Sant'Anastasia\"\"\n",
    "    \"\"\"\n",
    "    if not isinstance(x, str) or not x.strip():\n",
    "        return []\n",
    "    \n",
    "    # Clean up the string\n",
    "    x = x.strip()\n",
    "    \n",
    "    # Strategy 1: Try ast.literal_eval first (fastest for well-formed data)\n",
    "    try:\n",
    "        result = ast.literal_eval(x)\n",
    "        if isinstance(result, list):\n",
    "            return [str(item) for item in result]\n",
    "        else:\n",
    "            return [str(result)]\n",
    "    except (ValueError, SyntaxError):\n",
    "        pass\n",
    "    \n",
    "    # Strategy 2: Handle escaped quotes (main issue in the CSV files)\n",
    "    try:\n",
    "        # Replace double escaped quotes with single quotes\n",
    "        # \"\"Basilica di Sant'Anastasia\"\" -> 'Basilica di Sant\\'Anastasia'\n",
    "        fixed = re.sub(r'\"\"([^\"]*?)\"\"', r\"'\\1'\", x)\n",
    "        \n",
    "        result = ast.literal_eval(fixed)\n",
    "        if isinstance(result, list):\n",
    "            return [str(item) for item in result]\n",
    "        else:\n",
    "            return [str(result)]\n",
    "    except (ValueError, SyntaxError):\n",
    "        pass\n",
    "    \n",
    "    # Strategy 3: Manual parsing with regex\n",
    "    try:\n",
    "        if x.startswith('[') and x.endswith(']'):\n",
    "            # Extract content between various quote types\n",
    "            # Handle: 'text', \"text\", \"\"text\"\"\n",
    "            patterns = [\n",
    "                r\"'([^']*?)'(?:,|\\])\",  # Single quotes\n",
    "                r'\"([^\"]*?)\"(?:,|\\])',   # Double quotes\n",
    "                r'\"\"([^\"]*?)\"\"(?:,|\\])'  # Escaped double quotes\n",
    "            ]\n",
    "            \n",
    "            result = []\n",
    "            for pattern in patterns:\n",
    "                matches = re.findall(pattern, x + ',')\n",
    "                result.extend(matches)\n",
    "            \n",
    "            # Remove duplicates while preserving order\n",
    "            seen = set()\n",
    "            unique_result = []\n",
    "            for item in result:\n",
    "                if item not in seen:\n",
    "                    seen.add(item)\n",
    "                    unique_result.append(item)\n",
    "            \n",
    "            if unique_result:\n",
    "                return unique_result\n",
    "    except Exception:\n",
    "        pass\n",
    "    \n",
    "    # Strategy 4: JSON-like parsing\n",
    "    try:\n",
    "        # Try to convert to valid JSON format\n",
    "        json_like = x.replace(\"'\", '\"')  # Convert single quotes to double\n",
    "        result = json.loads(json_like)\n",
    "        if isinstance(result, list):\n",
    "            return [str(item) for item in result]\n",
    "        else:\n",
    "            return [str(result)]\n",
    "    except (json.JSONDecodeError, ValueError):\n",
    "        pass\n",
    "    \n",
    "    # Strategy 5: Simple comma splitting (last resort)\n",
    "    try:\n",
    "        if ',' in x:\n",
    "            # Remove brackets and split on commas\n",
    "            clean = x.strip('[]').split(',')\n",
    "            result = []\n",
    "            for item in clean:\n",
    "                # Clean each item\n",
    "                item = item.strip().strip('\"\\'')\n",
    "                if item:\n",
    "                    result.append(item)\n",
    "            if result:\n",
    "                return result\n",
    "    except Exception:\n",
    "        pass\n",
    "    \n",
    "    # Final fallback: empty list with warning\n",
    "    if debug:\n",
    "        print(f\"⚠️  Could not parse: '{x[:100]}{'...' if len(x) > 100 else ''}'\")\n",
    "    \n",
    "    return []\n",
    "\n",
    "\n",
    "def robust_csv_reader(file_path, debug=False):\n",
    "    \"\"\"\n",
    "    Robust CSV reader that handles various formatting issues\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Try standard pandas reader first\n",
    "        df = pd.read_csv(file_path)\n",
    "        if debug:\n",
    "            print(f\"✅ Standard CSV read successful: {len(df)} rows\")\n",
    "        return df\n",
    "    except pd.errors.ParserError as e:\n",
    "        if debug:\n",
    "            print(f\"⚠️  Standard CSV read failed: {e}\")\n",
    "        \n",
    "        # Try with different options\n",
    "        try:\n",
    "            df = pd.read_csv(file_path, on_bad_lines='skip', engine='python')\n",
    "            if debug:\n",
    "                print(f\"✅ Fallback CSV read successful: {len(df)} rows\")\n",
    "            return df\n",
    "        except Exception as e2:\n",
    "            if debug:\n",
    "                print(f\"❌ All CSV read attempts failed: {e2}\")\n",
    "            return pd.DataFrame()  # Return empty DataFrame\n",
    "\n",
    "\n",
    "# Test the robust parsing function with a sample\n",
    "test_cases = [\n",
    "    \"['Arena', 'Duomo', 'Casa Giulietta']\",  # Normal case\n",
    "    \"['Piazza delle Erbe', 'Torre dei Lamberti', 'Giardino Giusti', 'Museo di Castelvecchio', \"\"Basilica di Sant'Anastasia\"\"]\",  # Escaped quotes\n",
    "    '[\"Arena di Verona\", \"Castelvecchio\", \"Giardino Giusti\", \"Duomo di Verona\", \"Ponte Scaligero\"]',  # Double quotes\n",
    "]\n",
    "\n",
    "print(\"🧪 Testing robust parsing function:\")\n",
    "for i, test in enumerate(test_cases, 1):\n",
    "    result = robust_parse_prediction(test, debug=False)\n",
    "    print(f\"   Test {i}: {len(result)} items parsed ✅\")\n",
    "    if len(result) > 0:\n",
    "        print(f\"      Sample: {result[0]}\")\n",
    "\n",
    "print(\"\\n🚀 Robust parsing system ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and process all CSV files with robust parsing\n",
    "csv_files = [Path(p) for p in glob.glob(os.path.join(DATA_DIR, '*_pred_*.csv'))]\n",
    "csv_files = sorted(csv_files)\n",
    "\n",
    "if not csv_files:\n",
    "    raise FileNotFoundError(f\"No CSV files matching *_pred_*.csv found in {DATA_DIR}\")\n",
    "\n",
    "print(f\"📂 Found {len(csv_files)} CSV files to process\")\n",
    "\n",
    "# Process files with progress tracking\n",
    "dfs = []\n",
    "parse_errors = 0\n",
    "total_rows_processed = 0\n",
    "skipped_rows = 0\n",
    "\n",
    "for i, fp in enumerate(csv_files, 1):\n",
    "    print(f\"\\n📄 Processing file {i}/{len(csv_files)}: {fp.name}\")\n",
    "    \n",
    "    # Load CSV with robust reader\n",
    "    df = robust_csv_reader(fp, debug=False)\n",
    "    \n",
    "    if df.empty:\n",
    "        print(f\"   ⚠️  Skipped empty/corrupted file\")\n",
    "        continue\n",
    "    \n",
    "    # Extract year from filename\n",
    "    year_token = next((part for part in fp.stem.split('_')\n",
    "                       if part.isdigit() and len(part) == 4), None)\n",
    "    df['year'] = int(year_token) if year_token else np.nan\n",
    "    \n",
    "    # Robust prediction parsing with progress tracking\n",
    "    print(f\"   🔧 Parsing {len(df)} prediction entries...\")\n",
    "    \n",
    "    original_count = len(df)\n",
    "    df['prediction_list'] = df['prediction'].apply(robust_parse_prediction)\n",
    "    \n",
    "    # Count parsing issues\n",
    "    empty_predictions = (df['prediction_list'].apply(len) == 0).sum()\n",
    "    if empty_predictions > 0:\n",
    "        parse_errors += empty_predictions\n",
    "        print(f\"   ⚠️  {empty_predictions} entries could not be parsed\")\n",
    "    \n",
    "    # Filter for valid predictions (exactly 5 elements)\n",
    "    df_valid = df[df['prediction_list'].apply(len) == 5].copy()\n",
    "    rows_kept = len(df_valid)\n",
    "    rows_dropped = original_count - rows_kept\n",
    "    \n",
    "    if rows_dropped > 0:\n",
    "        skipped_rows += rows_dropped\n",
    "        print(f\"   📉 Filtered out {rows_dropped} rows (wrong prediction count)\")\n",
    "    \n",
    "    print(f\"   ✅ Kept {rows_kept} valid rows\")\n",
    "    \n",
    "    if not df_valid.empty:\n",
    "        dfs.append(df_valid)\n",
    "        total_rows_processed += rows_kept\n",
    "\n",
    "# Combine all DataFrames\n",
    "if dfs:\n",
    "    df_all = pd.concat(dfs, ignore_index=True)\n",
    "    print(f\"\\n🎉 Successfully processed {len(csv_files)} files:\")\n",
    "    print(f\"   📊 Total rows: {total_rows_processed:,}\")\n",
    "    print(f\"   ⚠️  Parse errors: {parse_errors:,}\")\n",
    "    print(f\"   📉 Skipped rows: {skipped_rows:,}\")\n",
    "    print(f\"   ✅ Success rate: {(total_rows_processed/(total_rows_processed+skipped_rows+parse_errors))*100:.1f}%\")\n",
    "    \n",
    "    # Show data quality summary\n",
    "    print(f\"\\n📈 Data Quality Summary:\")\n",
    "    print(f\"   • Years covered: {sorted(df_all['year'].dropna().unique().astype(int).tolist())}\")\n",
    "    print(f\"   • Unique POI in ground truth: {df_all['ground_truth'].nunique()}\")\n",
    "    print(f\"   • Average predictions per POI: {len(df_all) / df_all['ground_truth'].nunique():.1f}\")\n",
    "    \n",
    "    # Display first few rows\n",
    "    print(f\"\\n📋 Sample data:\")\n",
    "    display(df_all[['card_id', 'year', 'current_poi', 'ground_truth', 'prediction_list', 'hit']].head())\n",
    "else:\n",
    "    raise ValueError(\"No valid data could be loaded from any CSV file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 📊 Metrics Calculation\n",
    "\n",
    "Calculate comprehensive evaluation metrics with robust error handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def poi_id(x):\n",
    "    \"\"\"\n",
    "    Convert any object to a hashable string identifier for POI comparison\n",
    "    \"\"\"\n",
    "    if isinstance(x, dict):\n",
    "        # Try common keys for POI identification\n",
    "        for key in ('poi', 'poi_id', 'name', 'id'):\n",
    "            if key in x:\n",
    "                return str(x[key])\n",
    "        # Fallback: serialize to JSON\n",
    "        return json.dumps(x, sort_keys=True)\n",
    "    elif isinstance(x, (list, tuple)):\n",
    "        return tuple(map(poi_id, x))\n",
    "    else:\n",
    "        return str(x)\n",
    "\n",
    "\n",
    "# Normalize data for comparison\n",
    "print(\"🔄 Normalizing POI identifiers...\")\n",
    "df_all['prediction_norm'] = df_all['prediction_list'].apply(lambda lst: [poi_id(e) for e in lst])\n",
    "df_all['ground_truth_norm'] = df_all['ground_truth'].apply(poi_id)\n",
    "\n",
    "# Calculate element-wise metrics\n",
    "print(\"📊 Calculating performance metrics...\")\n",
    "\n",
    "# Top-1 Accuracy\n",
    "df_all['hit@1'] = df_all['prediction_norm'].str[0] == df_all['ground_truth_norm']\n",
    "\n",
    "# Top-5 Hit Rate\n",
    "def top_k_hit(row, k=5):\n",
    "    return row['ground_truth_norm'] in row['prediction_norm'][:k]\n",
    "\n",
    "df_all['hit@5'] = df_all.apply(top_k_hit, axis=1)\n",
    "\n",
    "# Mean Reciprocal Rank (MRR)\n",
    "def reciprocal_rank(row, k=5):\n",
    "    try:\n",
    "        rank = row['prediction_norm'][:k].index(row['ground_truth_norm']) + 1\n",
    "        return 1.0 / rank\n",
    "    except ValueError:\n",
    "        return 0.0\n",
    "\n",
    "df_all['rr'] = df_all.apply(reciprocal_rank, axis=1)\n",
    "\n",
    "print(\"✅ Metrics calculated successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate global metrics\n",
    "metrics_global = {\n",
    "    \"Top-1 Accuracy\": df_all[\"hit@1\"].mean(),\n",
    "    \"Top-5 Hit Rate\": df_all[\"hit@5\"].mean(),\n",
    "    \"Mean Reciprocal Rank\": df_all[\"rr\"].mean(),\n",
    "}\n",
    "\n",
    "# Catalogue Coverage\n",
    "coverage_set = {poi for preds in df_all[\"prediction_norm\"] for poi in preds}\n",
    "metrics_global[\"Catalogue Coverage\"] = len(coverage_set) / df_all[\"ground_truth_norm\"].nunique()\n",
    "\n",
    "# Create styled metrics table\n",
    "metrics_df = pd.DataFrame(metrics_global, index=[\"Value\"]).T\n",
    "metrics_styled = metrics_df.style.format({\n",
    "    'Value': '{:.2%}'\n",
    "}).set_table_styles([\n",
    "    {'selector': 'th', 'props': [('background-color', '#4CAF50'), ('color', 'white')]},\n",
    "    {'selector': 'td', 'props': [('text-align', 'center')]}\n",
    "])\n",
    "\n",
    "print(\"📊 GLOBAL PERFORMANCE METRICS\")\n",
    "print(\"=\" * 40)\n",
    "display(metrics_styled)\n",
    "\n",
    "# Additional summary statistics\n",
    "print(f\"\\n📈 Additional Statistics:\")\n",
    "print(f\"   • Total predictions analyzed: {len(df_all):,}\")\n",
    "print(f\"   • Unique ground truth POI: {df_all['ground_truth_norm'].nunique():,}\")\n",
    "print(f\"   • Unique predicted POI: {len(coverage_set):,}\")\n",
    "print(f\"   • Average processing time: {df_all['processing_time'].mean():.2f}s\")\n",
    "print(f\"   • Success rate: {df_all['status'].eq('success').mean():.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics by year with enhanced statistics\n",
    "by_year = (\n",
    "    df_all\n",
    "    .groupby('year')\n",
    "    .agg({\n",
    "        'hit@1': ['count', 'mean', 'std'],\n",
    "        'hit@5': ['mean', 'std'],\n",
    "        'rr': ['mean', 'std'],\n",
    "        'processing_time': 'mean'\n",
    "    })\n",
    "    .round(4)\n",
    ")\n",
    "\n",
    "# Flatten column names\n",
    "by_year.columns = ['sample_size', 'top1_acc', 'top1_std', 'hit5_rate', 'hit5_std', 'mrr', 'mrr_std', 'avg_time']\n",
    "by_year = by_year.reset_index().sort_values('year')\n",
    "\n",
    "print(\"📅 PERFORMANCE BY YEAR\")\n",
    "print(\"=\" * 60)\n",
    "display(by_year.style.format({\n",
    "    'top1_acc': '{:.1%}',\n",
    "    'top1_std': '{:.3f}',\n",
    "    'hit5_rate': '{:.1%}',\n",
    "    'hit5_std': '{:.3f}', \n",
    "    'mrr': '{:.1%}',\n",
    "    'mrr_std': '{:.3f}',\n",
    "    'avg_time': '{:.2f}s',\n",
    "    'sample_size': '{:,}'\n",
    "}).set_table_styles([\n",
    "    {'selector': 'th', 'props': [('background-color', '#2196F3'), ('color', 'white')]},\n",
    "    {'selector': 'td', 'props': [('text-align', 'center')]}\n",
    "]))\n",
    "\n",
    "# Year-over-year changes\n",
    "print(f\"\\n📈 Year-over-Year Trends:\")\n",
    "if len(by_year) > 1:\n",
    "    acc_change = by_year['top1_acc'].iloc[-1] - by_year['top1_acc'].iloc[0]\n",
    "    hit5_change = by_year['hit5_rate'].iloc[-1] - by_year['hit5_rate'].iloc[0]\n",
    "    print(f\"   • Top-1 Accuracy change: {acc_change:+.1%}\")\n",
    "    print(f\"   • Top-5 Hit Rate change: {hit5_change:+.1%}\")\n",
    "    print(f\"   • Best year for accuracy: {by_year.loc[by_year['top1_acc'].idxmax(), 'year']:.0f}\")\n",
    "    print(f\"   • Most consistent year (lowest std): {by_year.loc[by_year['top1_std'].idxmin(), 'year']:.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 📈 Enhanced Visualizations\n",
    "\n",
    "Professional-grade visualizations with comprehensive performance analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive Performance Dashboard\n",
    "fig = plt.figure(figsize=(20, 12))\n",
    "gs = fig.add_gridspec(3, 4, height_ratios=[1, 1, 1], width_ratios=[1, 1, 1, 1], hspace=0.3, wspace=0.3)\n",
    "\n",
    "# Color palette\n",
    "colors = {\n",
    "    'primary': '#1f77b4',\n",
    "    'secondary': '#ff7f0e', \n",
    "    'success': '#2ca02c',\n",
    "    'warning': '#d62728',\n",
    "    'info': '#9467bd'\n",
    "}\n",
    "\n",
    "# 1. Overall metrics (top row, left)\n",
    "ax_overall = fig.add_subplot(gs[0, :2])\n",
    "metrics_values = [\n",
    "    metrics_global[\"Top-1 Accuracy\"] * 100,\n",
    "    metrics_global[\"Top-5 Hit Rate\"] * 100,\n",
    "    metrics_global[\"Mean Reciprocal Rank\"] * 100,\n",
    "    min(metrics_global[\"Catalogue Coverage\"] * 100, 100)  # Cap at 100% for visualization\n",
    "]\n",
    "metrics_names = [\"Top-1\\nAccuracy\", \"Top-5\\nHit Rate\", \"Mean Reciprocal\\nRank\", \"Catalogue\\nCoverage\"]\n",
    "metric_colors = [colors['primary'], colors['success'], colors['secondary'], colors['info']]\n",
    "\n",
    "bars = ax_overall.bar(metrics_names, metrics_values, color=metric_colors, alpha=0.8, edgecolor='black', linewidth=1)\n",
    "for bar, value in zip(bars, metrics_values):\n",
    "    height = bar.get_height()\n",
    "    ax_overall.text(bar.get_x() + bar.get_width()/2., height + 1,\n",
    "                   f'{value:.1f}%', ha='center', va='bottom', fontweight='bold', fontsize=12)\n",
    "\n",
    "ax_overall.set_title('Overall Model Performance', fontsize=16, fontweight='bold', pad=20)\n",
    "ax_overall.set_ylabel('Performance (%)', fontsize=12, fontweight='bold')\n",
    "ax_overall.grid(True, alpha=0.3, axis='y')\n",
    "ax_overall.set_ylim(0, max(metrics_values) * 1.2)\n",
    "\n",
    "# 2. Performance trend over time (top row, right)\n",
    "ax_trend = fig.add_subplot(gs[0, 2:])\n",
    "years = by_year['year'].astype(int)\n",
    "ax_trend.plot(years, by_year['top1_acc']*100, 'o-', linewidth=3, markersize=8, \n",
    "             color=colors['primary'], label='Top-1 Accuracy')\n",
    "ax_trend.plot(years, by_year['hit5_rate']*100, 's-', linewidth=3, markersize=8,\n",
    "             color=colors['success'], label='Top-5 Hit Rate')\n",
    "ax_trend.fill_between(years, \n",
    "                     (by_year['top1_acc'] - by_year['top1_std'])*100,\n",
    "                     (by_year['top1_acc'] + by_year['top1_std'])*100,\n",
    "                     alpha=0.2, color=colors['primary'])\n",
    "\n",
    "# Add value annotations\n",
    "for i, year in enumerate(years):\n",
    "    ax_trend.annotate(f\"{by_year.iloc[i]['top1_acc']*100:.1f}%\", \n",
    "                     (year, by_year.iloc[i]['top1_acc']*100 + 2), \n",
    "                     ha='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "ax_trend.set_title('Performance Trends Over Time', fontsize=16, fontweight='bold')\n",
    "ax_trend.set_ylabel('Performance (%)', fontsize=12, fontweight='bold')\n",
    "ax_trend.set_xlabel('Year', fontsize=12, fontweight='bold')\n",
    "ax_trend.legend(fontsize=11)\n",
    "ax_trend.grid(True, alpha=0.3)\n",
    "ax_trend.set_xticks(years)\n",
    "\n",
    "# 3. Sample distribution (middle row, left)\n",
    "ax_samples = fig.add_subplot(gs[1, :2])\n",
    "bars_samples = ax_samples.bar(years.astype(str), by_year['sample_size'], \n",
    "                             color=colors['info'], alpha=0.7, edgecolor='black')\n",
    "for bar, value in zip(bars_samples, by_year['sample_size']):\n",
    "    ax_samples.text(bar.get_x() + bar.get_width()/2., bar.get_height() + max(by_year['sample_size'])*0.01,\n",
    "                   f'{value:,}', ha='center', va='bottom', fontweight='bold', fontsize=10)\n",
    "\n",
    "ax_samples.set_title('Sample Distribution by Year', fontsize=16, fontweight='bold')\n",
    "ax_samples.set_ylabel('Number of Predictions', fontsize=12, fontweight='bold')\n",
    "ax_samples.set_xlabel('Year', fontsize=12, fontweight='bold')\n",
    "ax_samples.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 4. Processing time analysis (middle row, right)\n",
    "ax_time = fig.add_subplot(gs[1, 2:])\n",
    "bars_time = ax_time.bar(years.astype(str), by_year['avg_time'],\n",
    "                       color=colors['warning'], alpha=0.7, edgecolor='black')\n",
    "for bar, value in zip(bars_time, by_year['avg_time']):\n",
    "    ax_time.text(bar.get_x() + bar.get_width()/2., bar.get_height() + max(by_year['avg_time'])*0.01,\n",
    "                f'{value:.1f}s', ha='center', va='bottom', fontweight='bold', fontsize=10)\n",
    "\n",
    "ax_time.set_title('Average Processing Time by Year', fontsize=16, fontweight='bold')\n",
    "ax_time.set_ylabel('Processing Time (seconds)', fontsize=12, fontweight='bold')\n",
    "ax_time.set_xlabel('Year', fontsize=12, fontweight='bold')\n",
    "ax_time.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 5. POI popularity vs accuracy (bottom row)\n",
    "poi_stats = df_all.groupby('ground_truth_norm').agg({\n",
    "    'hit@1': ['count', 'mean'],\n",
    "    'hit@5': 'mean',\n",
    "    'rr': 'mean'\n",
    "}).round(4)\n",
    "poi_stats.columns = ['visit_count', 'accuracy', 'hit5_rate', 'mrr']\n",
    "poi_stats = poi_stats.sort_values('visit_count', ascending=False).head(15)\n",
    "\n",
    "ax_poi = fig.add_subplot(gs[2, :])\n",
    "scatter = ax_poi.scatter(poi_stats['visit_count'], poi_stats['accuracy']*100, \n",
    "                        s=poi_stats['hit5_rate']*500, # Size based on hit5 rate\n",
    "                        c=poi_stats['mrr']*100, cmap='viridis', \n",
    "                        alpha=0.7, edgecolor='black')\n",
    "\n",
    "# Add POI labels for top points\n",
    "for poi, row in poi_stats.head(8).iterrows():\n",
    "    ax_poi.annotate(poi[:12] + ('...' if len(poi) > 12 else ''), \n",
    "                   (row['visit_count'], row['accuracy']*100),\n",
    "                   xytext=(5, 5), textcoords='offset points', fontsize=9,\n",
    "                   bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.7))\n",
    "\n",
    "ax_poi.set_xlabel('Visit Count (Popularity)', fontsize=12, fontweight='bold')\n",
    "ax_poi.set_ylabel('Top-1 Accuracy (%)', fontsize=12, fontweight='bold')\n",
    "ax_poi.set_title('POI Popularity vs Accuracy (Size=Hit@5, Color=MRR)', fontsize=16, fontweight='bold')\n",
    "ax_poi.grid(True, alpha=0.3)\n",
    "\n",
    "# Add colorbar\n",
    "cbar = plt.colorbar(scatter, ax=ax_poi, fraction=0.046, pad=0.04)\n",
    "cbar.set_label('Mean Reciprocal Rank (%)', fontsize=11, fontweight='bold')\n",
    "\n",
    "plt.suptitle('🎯 LLM-Mob Model: Comprehensive Performance Dashboard', \n",
    "             fontsize=22, fontweight='bold', y=0.98)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print correlation analysis\n",
    "print(f\"\\n🔍 Correlation Analysis:\")\n",
    "print(f\"   • Popularity vs Accuracy: {poi_stats['visit_count'].corr(poi_stats['accuracy']):.3f}\")\n",
    "print(f\"   • Popularity vs Hit@5: {poi_stats['visit_count'].corr(poi_stats['hit5_rate']):.3f}\")\n",
    "print(f\"   • Accuracy vs MRR: {poi_stats['accuracy'].corr(poi_stats['mrr']):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 🔍 Advanced Error Analysis\n",
    "\n",
    "Deep dive into model failures and error patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive error analysis\n",
    "print(\"🔍 ADVANCED ERROR ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Separate errors from correct predictions\n",
    "err_rows = df_all[~df_all[\"hit@1\"]].copy()\n",
    "correct_rows = df_all[df_all[\"hit@1\"]].copy()\n",
    "\n",
    "if err_rows.empty:\n",
    "    print(\"🎉 No Top-1 errors to analyze - perfect model performance!\")\n",
    "else:\n",
    "    # Add helper columns\n",
    "    err_rows['first_pred'] = err_rows[\"prediction_norm\"].str[0]\n",
    "    correct_rows['first_pred'] = correct_rows[\"prediction_norm\"].str[0]\n",
    "    \n",
    "    print(f\"📊 Dataset Overview:\")\n",
    "    print(f\"   • Total predictions: {len(df_all):,}\")\n",
    "    print(f\"   • Correct predictions: {len(correct_rows):,} ({len(correct_rows)/len(df_all):.1%})\")\n",
    "    print(f\"   • Prediction errors: {len(err_rows):,} ({len(err_rows)/len(df_all):.1%})\")\n",
    "    \n",
    "    # 1. Error distribution by POI\n",
    "    print(f\"\\n1️⃣ ERROR DISTRIBUTION BY POI\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    error_by_poi = (\n",
    "        err_rows['ground_truth_norm']\n",
    "        .value_counts()\n",
    "        .head(10)\n",
    "        .to_frame(name='error_count')\n",
    "    )\n",
    "    \n",
    "    # Calculate error rates\n",
    "    total_by_poi = df_all['ground_truth_norm'].value_counts()\n",
    "    error_by_poi['total_count'] = error_by_poi.index.map(total_by_poi)\n",
    "    error_by_poi['error_rate'] = error_by_poi['error_count'] / error_by_poi['total_count']\n",
    "    error_by_poi['success_rate'] = 1 - error_by_poi['error_rate']\n",
    "    \n",
    "    print(\"Most problematic POI (by error count):\")\n",
    "    for poi, row in error_by_poi.iterrows():\n",
    "        print(f\"   {poi[:25]:>25s}: {row['error_count']:>4d} errors ({row['error_rate']:>5.1%} rate, {row['total_count']:>4d} total)\")\n",
    "    \n",
    "    # 2. Most frequent wrong predictions\n",
    "    print(f\"\\n2️⃣ MOST FREQUENT WRONG PREDICTIONS\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    wrong_pred_freq = err_rows['first_pred'].value_counts().head(10)\n",
    "    total_wrong = len(err_rows)\n",
    "    \n",
    "    print(\"Most frequently predicted (incorrectly):\")\n",
    "    for poi, freq in wrong_pred_freq.items():\n",
    "        percentage = freq / total_wrong * 100\n",
    "        print(f\"   {poi[:25]:>25s}: {freq:>4d} times ({percentage:>5.1f}% of errors)\")\n",
    "    \n",
    "    # 3. Worst POI pairs (True → Predicted)\n",
    "    print(f\"\\n3️⃣ WORST-PERFORMING POI PAIRS\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    pairs = (\n",
    "        err_rows.groupby([\"ground_truth_norm\", \"first_pred\"])\n",
    "               .size()\n",
    "               .reset_index(name=\"errors\")\n",
    "               .sort_values(\"errors\", ascending=False)\n",
    "               .head(10)\n",
    "    )\n",
    "    \n",
    "    print(\"Most problematic prediction pairs (True → Predicted):\")\n",
    "    for _, row in pairs.iterrows():\n",
    "        percentage = row['errors'] / total_wrong * 100\n",
    "        print(f\"   {row['ground_truth_norm'][:15]:>15s} → {row['first_pred'][:15]:<15s}: {row['errors']:>3d} ({percentage:>4.1f}%)\")\n",
    "    \n",
    "    # 4. Temporal error patterns\n",
    "    print(f\"\\n4️⃣ ERROR PATTERNS BY YEAR\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    error_by_year = (\n",
    "        df_all.groupby('year')\n",
    "              .agg({'hit@1': ['count', 'mean'], 'processing_time': 'mean'})\n",
    "              .round(3)\n",
    "    )\n",
    "    error_by_year.columns = ['total_preds', 'accuracy', 'avg_time']\n",
    "    error_by_year['error_rate'] = 1 - error_by_year['accuracy']\n",
    "    \n",
    "    print(\"Error rates by year:\")\n",
    "    for year, row in error_by_year.iterrows():\n",
    "        if not pd.isna(year):\n",
    "            print(f\"   {int(year)}: {row['error_rate']:>6.1%} error rate ({row['total_preds']:>5,} predictions, {row['avg_time']:>5.1f}s avg)\")\n",
    "    \n",
    "    # 5. Performance variability analysis\n",
    "    print(f\"\\n5️⃣ PERFORMANCE VARIABILITY\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Calculate coefficient of variation for accuracy across years\n",
    "    acc_cv = error_by_year['accuracy'].std() / error_by_year['accuracy'].mean()\n",
    "    time_cv = error_by_year['avg_time'].std() / error_by_year['avg_time'].mean()\n",
    "    \n",
    "    print(f\"Model consistency metrics:\")\n",
    "    print(f\"   • Accuracy coefficient of variation: {acc_cv:.3f} ({'High' if acc_cv > 0.2 else 'Medium' if acc_cv > 0.1 else 'Low'} variability)\")\n",
    "    print(f\"   • Processing time CV: {time_cv:.3f} ({'High' if time_cv > 0.5 else 'Medium' if time_cv > 0.2 else 'Low'} variability)\")\n",
    "    print(f\"   • Best performing year: {error_by_year['accuracy'].idxmax():.0f} ({error_by_year['accuracy'].max():.1%} accuracy)\")\n",
    "    print(f\"   • Worst performing year: {error_by_year['accuracy'].idxmin():.0f} ({error_by_year['accuracy'].min():.1%} accuracy)\")\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 50)\n",
    "print(\"ERROR ANALYSIS COMPLETE\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of error patterns\n",
    "if not err_rows.empty:\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(18, 12))\n",
    "    \n",
    "    # 1. Top error pairs (horizontal bar chart)\n",
    "    pairs_viz = pairs.head(8)\n",
    "    colors_grad = plt.cm.Reds(np.linspace(0.4, 0.9, len(pairs_viz)))\n",
    "    \n",
    "    bars1 = ax1.barh(range(len(pairs_viz)), pairs_viz['errors'], \n",
    "                     color=colors_grad, edgecolor='black', alpha=0.8)\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, (bar, value) in enumerate(zip(bars1, pairs_viz['errors'])):\n",
    "        ax1.text(bar.get_width() + max(pairs_viz['errors'])*0.01, bar.get_y() + bar.get_height()/2,\n",
    "                f'{value}', ha='left', va='center', fontweight='bold')\n",
    "    \n",
    "    ax1.set_yticks(range(len(pairs_viz)))\n",
    "    ax1.set_yticklabels([f\"{row['ground_truth_norm'][:10]}\\n→{row['first_pred'][:10]}\" \n",
    "                        for _, row in pairs_viz.iterrows()], fontsize=9)\n",
    "    ax1.set_xlabel('Error Count', fontweight='bold')\n",
    "    ax1.set_title('Top Error Pairs (True → Predicted)', fontweight='bold', fontsize=14)\n",
    "    ax1.grid(True, alpha=0.3, axis='x')\n",
    "    ax1.invert_yaxis()\n",
    "    \n",
    "    # 2. Error rate by POI popularity\n",
    "    error_by_poi_viz = error_by_poi.head(10)\n",
    "    x_pos = range(len(error_by_poi_viz))\n",
    "    \n",
    "    bars2 = ax2.bar(x_pos, error_by_poi_viz['error_rate']*100, \n",
    "                    color=plt.cm.RdYlBu_r(error_by_poi_viz['error_rate']), \n",
    "                    edgecolor='black', alpha=0.8)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, value in zip(bars2, error_by_poi_viz['error_rate']*100):\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 1,\n",
    "                f'{value:.1f}%', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    ax2.set_xticks(x_pos)\n",
    "    ax2.set_xticklabels([poi[:10] + ('...' if len(poi) > 10 else '') \n",
    "                        for poi in error_by_poi_viz.index], rotation=45, ha='right')\n",
    "    ax2.set_ylabel('Error Rate (%)', fontweight='bold')\n",
    "    ax2.set_title('Error Rate by POI (Top 10 by Error Count)', fontweight='bold', fontsize=14)\n",
    "    ax2.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # 3. Accuracy vs processing time scatter\n",
    "    scatter_data = by_year.copy()\n",
    "    scatter = ax3.scatter(scatter_data['avg_time'], scatter_data['top1_acc']*100,\n",
    "                         s=scatter_data['sample_size']/50, # Size based on sample size\n",
    "                         c=scatter_data['year'], cmap='viridis',\n",
    "                         alpha=0.7, edgecolor='black')\n",
    "    \n",
    "    # Add year labels\n",
    "    for _, row in scatter_data.iterrows():\n",
    "        ax3.annotate(f\"{int(row['year'])}\", \n",
    "                    (row['avg_time'], row['top1_acc']*100),\n",
    "                    xytext=(3, 3), textcoords='offset points', fontsize=10, fontweight='bold')\n",
    "    \n",
    "    ax3.set_xlabel('Average Processing Time (seconds)', fontweight='bold')\n",
    "    ax3.set_ylabel('Top-1 Accuracy (%)', fontweight='bold')\n",
    "    ax3.set_title('Accuracy vs Processing Time by Year\\n(Bubble size = Sample size)', fontweight='bold', fontsize=14)\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Wrong predictions distribution\n",
    "    wrong_viz = wrong_pred_freq.head(8)\n",
    "    bars4 = ax4.bar(range(len(wrong_viz)), wrong_viz.values,\n",
    "                    color=plt.cm.OrRd(np.linspace(0.4, 0.9, len(wrong_viz))),\n",
    "                    edgecolor='black', alpha=0.8)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, value in zip(bars4, wrong_viz.values):\n",
    "        ax4.text(bar.get_x() + bar.get_width()/2., bar.get_height() + max(wrong_viz.values)*0.01,\n",
    "                f'{value:,}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    ax4.set_xticks(range(len(wrong_viz)))\n",
    "    ax4.set_xticklabels([poi[:10] + ('...' if len(poi) > 10 else '') \n",
    "                        for poi in wrong_viz.index], rotation=45, ha='right')\n",
    "    ax4.set_ylabel('Frequency', fontweight='bold')\n",
    "    ax4.set_title('Most Frequently Wrong Predictions', fontweight='bold', fontsize=14)\n",
    "    ax4.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.suptitle('🔍 Error Analysis Dashboard', fontsize=18, fontweight='bold', y=0.98)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Individual Metric Analysis\n",
    "\n",
    "Detailed individual charts for each performance metric with enhanced styling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix for Top POI\n",
    "print(\"📊 Creating confusion matrix for top POI...\")\n",
    "\n",
    "# Configuration for confusion matrix\n",
    "TOP_K_POI = 15  # Number of top POI to include in confusion matrix\n",
    "YEAR_FILTER = None  # Set to specific year or None for all years\n",
    "\n",
    "# Filter data\n",
    "subset = df_all.copy()\n",
    "if YEAR_FILTER is not None:\n",
    "    subset = subset.query(\"year == @YEAR_FILTER\")\n",
    "    print(f\"   Filtering for year: {YEAR_FILTER}\")\n",
    "\n",
    "# Get top POI by frequency in ground truth\n",
    "top_poi = (\n",
    "    subset[\"ground_truth_norm\"]\n",
    "           .value_counts()\n",
    "           .head(TOP_K_POI)\n",
    "           .index.tolist()\n",
    ")\n",
    "\n",
    "print(f\"   Analyzing top {TOP_K_POI} POI by frequency\")\n",
    "print(f\"   Dataset size: {len(subset):,} predictions\")\n",
    "\n",
    "# Filter for rows where both true and predicted POI are in the top list\n",
    "mask = (subset[\"ground_truth_norm\"].isin(top_poi) & \n",
    "        subset[\"prediction_norm\"].str[0].isin(top_poi))\n",
    "\n",
    "filtered_data = subset.loc[mask]\n",
    "print(f\"   Filtered to {len(filtered_data):,} predictions involving top POI\")\n",
    "\n",
    "if len(filtered_data) > 0:\n",
    "    # Create confusion matrix\n",
    "    cm_df = pd.crosstab(\n",
    "        filtered_data[\"ground_truth_norm\"],\n",
    "        filtered_data[\"prediction_norm\"].str[0],\n",
    "        rownames=[\"True POI\"],\n",
    "        colnames=[\"Predicted POI\"],\n",
    "        dropna=False\n",
    "    )\n",
    "    \n",
    "    # Normalize by rows to get error rates\n",
    "    cm_norm = cm_df.div(cm_df.sum(axis=1), axis=0)\n",
    "    \n",
    "    # Create the heatmap\n",
    "    fig, ax = plt.subplots(figsize=(14, 12))\n",
    "    \n",
    "    # Use a color map that highlights errors (low diagonal values)\n",
    "    im = ax.imshow(cm_norm.values, cmap='RdYlBu_r', aspect='auto', vmin=0, vmax=1)\n",
    "    \n",
    "    # Set ticks and labels\n",
    "    ax.set_xticks(range(len(cm_norm.columns)))\n",
    "    ax.set_xticklabels([col[:15] + ('...' if len(col) > 15 else '') for col in cm_norm.columns], \n",
    "                      rotation=45, ha='right', fontsize=9)\n",
    "    ax.set_yticks(range(len(cm_norm.index)))\n",
    "    ax.set_yticklabels([idx[:15] + ('...' if len(idx) > 15 else '') for idx in cm_norm.index], \n",
    "                      fontsize=9)\n",
    "    \n",
    "    # Add text annotations\n",
    "    for i in range(len(cm_norm.index)):\n",
    "        for j in range(len(cm_norm.columns)):\n",
    "            value = cm_norm.iloc[i, j]\n",
    "            count = cm_df.iloc[i, j]\n",
    "            if not np.isnan(value):\n",
    "                # Use white text for dark cells, black for light cells\n",
    "                text_color = 'white' if value < 0.5 else 'black'\n",
    "                ax.text(j, i, f'{value:.2f}\\n({count})', ha='center', va='center',\n",
    "                       color=text_color, fontsize=8, fontweight='bold')\n",
    "    \n",
    "    ax.set_xlabel('Predicted POI', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel('True POI', fontsize=12, fontweight='bold')\n",
    "    title = f'Confusion Matrix - Top {TOP_K_POI} POI'\n",
    "    if YEAR_FILTER:\n",
    "        title += f' (Year {YEAR_FILTER})'\n",
    "    ax.set_title(title, fontsize=14, fontweight='bold', pad=20)\n",
    "    \n",
    "    # Add colorbar\n",
    "    cbar = plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "    cbar.set_label('Prediction Rate', fontsize=11, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(f\"\\n📈 Confusion Matrix Summary:\")\n",
    "    diagonal_accuracy = np.diag(cm_norm).mean()\n",
    "    print(f\"   • Average diagonal accuracy: {diagonal_accuracy:.2%}\")\n",
    "    \n",
    "    # Find most accurate predictions (highest diagonal values)\n",
    "    diagonal_values = pd.Series(np.diag(cm_norm), index=cm_norm.index).sort_values(ascending=False)\n",
    "    print(f\"   • Most accurate predictions:\")\n",
    "    for poi, acc in diagonal_values.head(5).items():\n",
    "        count = cm_df.loc[poi, poi]\n",
    "        total = cm_df.loc[poi].sum()\n",
    "        print(f\"     - {poi[:25]:>25s}: {acc:.2%} ({count:,}/{total:,})\")\n",
    "    \n",
    "    print(f\"   • Least accurate predictions:\")\n",
    "    for poi, acc in diagonal_values.tail(5).items():\n",
    "        count = cm_df.loc[poi, poi]\n",
    "        total = cm_df.loc[poi].sum()\n",
    "        print(f\"     - {poi[:25]:>25s}: {acc:.2%} ({count:,}/{total:,})\")\n",
    "\n",
    "else:\n",
    "    print(\"❌ No data available for confusion matrix creation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Worst Performing POI Pairs\n",
    "print(\"🔍 Analyzing worst-performing POI pairs...\")\n",
    "\n",
    "# Filter error rows and create pairs analysis\n",
    "if 'hit@1' in df_all.columns:\n",
    "    err_rows_individual = df_all[~df_all[\"hit@1\"]].copy()\n",
    "    if not err_rows_individual.empty:\n",
    "        # Add first prediction column\n",
    "        err_rows_individual['first_pred'] = err_rows_individual[\"prediction_norm\"].str[0]\n",
    "        \n",
    "        # Count error pairs\n",
    "        pairs_individual = (\n",
    "            err_rows_individual.groupby([\"ground_truth_norm\", \"first_pred\"])\n",
    "                  .size()\n",
    "                  .reset_index(name=\"errors\")\n",
    "                  .sort_values(\"errors\", ascending=False)\n",
    "                  .head(15)\n",
    "        )\n",
    "        \n",
    "        # Create horizontal bar chart\n",
    "        fig, ax = plt.subplots(figsize=(12, 8))\n",
    "        \n",
    "        # Create labels for pairs\n",
    "        pair_labels = [f\"{row['ground_truth_norm'][:12]} → {row['first_pred'][:12]}\" \n",
    "                      for _, row in pairs_individual.iterrows()]\n",
    "        \n",
    "        colors_gradient = plt.cm.Reds(np.linspace(0.4, 0.9, len(pairs_individual)))\n",
    "        bars = ax.barh(range(len(pairs_individual)), pairs_individual[\"errors\"], \n",
    "                       color=colors_gradient, edgecolor='black', alpha=0.8)\n",
    "        \n",
    "        # Add value labels\n",
    "        for i, (bar, value) in enumerate(zip(bars, pairs_individual[\"errors\"])):\n",
    "            ax.text(bar.get_width() + max(pairs_individual[\"errors\"])*0.01, \n",
    "                   bar.get_y() + bar.get_height()/2,\n",
    "                   f'{value:,}', ha='left', va='center', fontweight='bold', fontsize=10)\n",
    "        \n",
    "        ax.set_yticks(range(len(pairs_individual)))\n",
    "        ax.set_yticklabels(pair_labels, fontsize=9)\n",
    "        ax.set_xlabel('Number of Errors', fontsize=12, fontweight='bold')\n",
    "        ax.set_title('Worst-Performing POI Pairs (True → Predicted) - Top 15', \n",
    "                    fontsize=14, fontweight='bold', pad=20)\n",
    "        ax.grid(True, alpha=0.3, axis='x')\n",
    "        ax.invert_yaxis()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Display pairs data\n",
    "        print(\"Top 10 worst-performing POI pairs:\")\n",
    "        for i, (_, row) in enumerate(pairs_individual.head(10).iterrows(), 1):\n",
    "            percentage = row['errors'] / len(err_rows_individual) * 100\n",
    "            print(f\"  {i:2d}. {row['ground_truth_norm'][:20]:>20s} → {row['first_pred'][:20]:<20s}: {row['errors']:>4,} errors ({percentage:4.1f}%)\")\n",
    "    else:\n",
    "        print(\"✅ No Top-1 errors found to analyze!\")\n",
    "else:\n",
    "    print(\"⚠️  hit@1 column not found - skipping pairs analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean Reciprocal Rank (MRR) by Year\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "bars = ax.bar(by_year['year'].astype(str), by_year['mrr']*100,\n",
    "              color='#ff7f0e', alpha=0.8, edgecolor='black', linewidth=1)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, value in zip(bars, by_year['mrr']*100):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.5,\n",
    "           f'{value:.1f}%', ha='center', va='bottom', fontweight='bold', fontsize=11)\n",
    "\n",
    "ax.set_ylabel('MRR (%)', fontsize=12, fontweight='bold')\n",
    "ax.set_xlabel('Year', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Mean Reciprocal Rank (%) by Year', fontsize=14, fontweight='bold', pad=20)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "ax.set_ylim(0, max(by_year['mrr']*100) * 1.15)\n",
    "\n",
    "# Add trend line\n",
    "x_numeric = range(len(by_year))\n",
    "z = np.polyfit(x_numeric, by_year['mrr']*100, 1)\n",
    "p = np.poly1d(z)\n",
    "ax.plot(by_year['year'].astype(str), p(x_numeric), \"r--\", alpha=0.8, linewidth=2, label=f'Trend (slope: {z[0]:.1f}%/year)')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top-5 Hit Rate by Year\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "bars = ax.bar(by_year['year'].astype(str), by_year['hit5_rate']*100,\n",
    "              color='#2ca02c', alpha=0.8, edgecolor='black', linewidth=1)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, value in zip(bars, by_year['hit5_rate']*100):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.5,\n",
    "           f'{value:.1f}%', ha='center', va='bottom', fontweight='bold', fontsize=11)\n",
    "\n",
    "ax.set_ylabel('Top‑5 Hit Rate (%)', fontsize=12, fontweight='bold')\n",
    "ax.set_xlabel('Year', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Top‑5 Hit Rate (%) by Year', fontsize=14, fontweight='bold', pad=20)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "ax.set_ylim(0, max(by_year['hit5_rate']*100) * 1.15)\n",
    "\n",
    "# Add trend line\n",
    "x_numeric = range(len(by_year))\n",
    "z = np.polyfit(x_numeric, by_year['hit5_rate']*100, 1)\n",
    "p = np.poly1d(z)\n",
    "ax.plot(by_year['year'].astype(str), p(x_numeric), \"r--\", alpha=0.8, linewidth=2, label=f'Trend (slope: {z[0]:.1f}%/year)')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top-1 Accuracy by Year\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "bars = ax.bar(by_year['year'].astype(str), by_year['top1_acc']*100, \n",
    "              color='#1f77b4', alpha=0.8, edgecolor='black', linewidth=1)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, value in zip(bars, by_year['top1_acc']*100):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.5,\n",
    "           f'{value:.1f}%', ha='center', va='bottom', fontweight='bold', fontsize=11)\n",
    "\n",
    "ax.set_ylabel('Top‑1 Accuracy (%)', fontsize=12, fontweight='bold')\n",
    "ax.set_xlabel('Year', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Top‑1 Accuracy (%) by Year', fontsize=14, fontweight='bold', pad=20)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "ax.set_ylim(0, max(by_year['top1_acc']*100) * 1.15)\n",
    "\n",
    "# Add trend line\n",
    "x_numeric = range(len(by_year))\n",
    "z = np.polyfit(x_numeric, by_year['top1_acc']*100, 1)\n",
    "p = np.poly1d(z)\n",
    "ax.plot(by_year['year'].astype(str), p(x_numeric), \"r--\", alpha=0.8, linewidth=2, label=f'Trend (slope: {z[0]:.1f}%/year)')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 📋 Executive Summary\n",
    "\n",
    "Key findings and actionable insights from the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate executive summary\n",
    "print(\"📋 EXECUTIVE SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\n🎯 OVERALL PERFORMANCE:\")\n",
    "print(f\"   • Model achieved {metrics_global['Top-1 Accuracy']:.1%} top-1 accuracy\")\n",
    "print(f\"   • {metrics_global['Top-5 Hit Rate']:.1%} of correct answers appear in top-5 predictions\")\n",
    "print(f\"   • Mean Reciprocal Rank: {metrics_global['Mean Reciprocal Rank']:.1%}\")\n",
    "print(f\"   • Catalogue coverage: {metrics_global['Catalogue Coverage']:.1%} of available POI\")\n",
    "\n",
    "if len(by_year) > 1:\n",
    "    print(f\"\\n📈 TEMPORAL TRENDS:\")\n",
    "    best_year = by_year.loc[by_year['top1_acc'].idxmax()]\n",
    "    worst_year = by_year.loc[by_year['top1_acc'].idxmin()]\n",
    "    \n",
    "    print(f\"   • Best performing year: {int(best_year['year'])} ({best_year['top1_acc']:.1%} accuracy)\")\n",
    "    print(f\"   • Worst performing year: {int(worst_year['year'])} ({worst_year['top1_acc']:.1%} accuracy)\")\n",
    "    print(f\"   • Performance range: {(best_year['top1_acc'] - worst_year['top1_acc']):.1%} difference\")\n",
    "    print(f\"   • Consistency: {'High' if by_year['top1_acc'].std() < 0.05 else 'Medium' if by_year['top1_acc'].std() < 0.10 else 'Low'}\")\n",
    "\n",
    "if not err_rows.empty:\n",
    "    print(f\"\\n🔍 ERROR ANALYSIS:\")\n",
    "    most_problematic = error_by_poi.iloc[0]\n",
    "    most_wrong_pred = wrong_pred_freq.iloc[0]\n",
    "    worst_pair = pairs.iloc[0]\n",
    "    \n",
    "    print(f\"   • Most problematic POI: {most_problematic.name} ({most_problematic['error_rate']:.1%} error rate)\")\n",
    "    print(f\"   • Most frequent wrong prediction: {most_wrong_pred.name} ({most_wrong_pred} times)\")\n",
    "    print(f\"   • Worst prediction pair: {worst_pair['ground_truth_norm']} → {worst_pair['first_pred']} ({worst_pair['errors']} errors)\")\n",
    "\n",
    "print(f\"\\n⚡ PERFORMANCE INSIGHTS:\")\n",
    "avg_time = df_all['processing_time'].mean()\n",
    "print(f\"   • Average processing time: {avg_time:.2f} seconds per prediction\")\n",
    "print(f\"   • Success rate: {df_all['status'].eq('success').mean():.1%}\")\n",
    "print(f\"   • Data quality: {((total_rows_processed)/(total_rows_processed+skipped_rows+parse_errors))*100:.1f}% of raw data successfully processed\")\n",
    "\n",
    "# Recommendations\n",
    "print(f\"\\n💡 RECOMMENDATIONS:\")\n",
    "\n",
    "if metrics_global['Top-1 Accuracy'] < 0.3:\n",
    "    print(f\"   • ❗ Consider model retraining - accuracy below 30%\")\n",
    "elif metrics_global['Top-1 Accuracy'] < 0.5:\n",
    "    print(f\"   • ⚠️  Model performance could be improved - consider fine-tuning\")\n",
    "else:\n",
    "    print(f\"   • ✅ Model performance is acceptable\")\n",
    "\n",
    "if len(by_year) > 1 and by_year['top1_acc'].std() > 0.1:\n",
    "    print(f\"   • 📊 High year-to-year variability detected - investigate data quality differences\")\n",
    "\n",
    "if not err_rows.empty and error_by_poi.iloc[0]['error_rate'] > 0.8:\n",
    "    print(f\"   • 🎯 Focus on improving predictions for '{error_by_poi.index[0]}' (highest error rate)\")\n",
    "\n",
    "if avg_time > 30:\n",
    "    print(f\"   • ⚡ Consider optimizing model inference - average time exceeds 30s\")\n",
    "elif avg_time < 1:\n",
    "    print(f\"   • ⚡ Excellent inference speed - under 1s per prediction\")\n",
    "\n",
    "print(f\"\\n📊 DATASET STATISTICS:\")\n",
    "print(f\"   • Total predictions analyzed: {len(df_all):,}\")\n",
    "print(f\"   • Time period: {int(df_all['year'].min())}-{int(df_all['year'].max())}\")\n",
    "print(f\"   • Unique POI in ground truth: {df_all['ground_truth_norm'].nunique()}\")\n",
    "print(f\"   • Data processing success rate: {((total_rows_processed)/(total_rows_processed+skipped_rows+parse_errors))*100:.1f}%\")\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 60)\n",
    "print(f\"Analysis completed successfully! 🎉\")\n",
    "print(f\"Generated on: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
