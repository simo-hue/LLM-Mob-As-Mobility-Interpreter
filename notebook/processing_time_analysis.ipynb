{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing Time Analysis by Prompt Strategy\n",
    "\n",
    "This notebook analyzes the processing time performance across different prompt strategies:\n",
    "- **base_version**: Basic version with minimal context\n",
    "- **with_geom**: Version with geospatial features  \n",
    "- **base_version_second_try**: Base version alternative implementation (found in penultimate/)\n",
    "\n",
    "The analysis focuses on the `processing_time` column from prediction CSV files.\n",
    "\n",
    "**Current Status**: Analysis focuses on base_version and with_geom strategies for comprehensive performance comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import os\n",
    "import gc\n",
    "import psutil\n",
    "from collections import defaultdict\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import pickle\n",
    "import hashlib\n",
    "from tqdm import tqdm\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# ============= ENHANCED CONFIGURATION =============\n",
    "# MODEL SELECTION - Change this to analyze specific model\n",
    "MODEL = \"qwen2.5_7b\"  # Options: qwen2.5_7b, qwen2.5_14b, llama3.1_8b, mixtral_8x7b, deepseek-coder_33b, mistral_7b\n",
    "\n",
    "# Strategy configuration - UPDATED: Only analyzing available strategies\n",
    "EXPECTED_STRATEGIES = {\n",
    "    'base_version': 'Base Version',\n",
    "    'with_geom': 'With Geometry'\n",
    "}\n",
    "\n",
    "# Additional strategies that may only exist in specific anchors\n",
    "OPTIONAL_STRATEGIES = {\n",
    "    'base_version_second_try': 'Base Version (Second Try)'  # Only in penultimate/\n",
    "}\n",
    "\n",
    "# Anchor configuration - now configurable  \n",
    "ANCHOR_DIRS = ['middle', 'penultimate']\n",
    "\n",
    "# Performance configuration\n",
    "MAX_WORKERS = 4  # Parallel file processing\n",
    "BATCH_SIZE = 1000  # Records per batch before DataFrame creation\n",
    "CHUNK_SIZE = 5000  # CSV reading chunk size\n",
    "PROGRESS_UPDATE_INTERVAL = 5  # Files between progress updates\n",
    "\n",
    "# Cache configuration\n",
    "USE_CACHE = True\n",
    "CACHE_DIR = Path('cache')\n",
    "\n",
    "# Memory management settings\n",
    "pd.set_option('mode.chained_assignment', None)\n",
    "plt.rcParams['figure.max_open_warning'] = 0\n",
    "\n",
    "# Set paths\n",
    "base_path = Path('/leonardo_work/IscrC_LLM-Mob/LLM-Mob-As-Mobility-Interpreter')\n",
    "results_path = base_path / 'results'\n",
    "cache_path = base_path / 'notebook' / CACHE_DIR\n",
    "\n",
    "# Ensure cache directory exists\n",
    "cache_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def get_memory_usage():\n",
    "    \"\"\"Get current memory usage in MB\"\"\"\n",
    "    process = psutil.Process(os.getpid())\n",
    "    return process.memory_info().rss / 1024 / 1024\n",
    "\n",
    "def get_cache_key(model: str) -> str:\n",
    "    \"\"\"Generate cache key for model data\"\"\"\n",
    "    # Include file modification times in cache key for invalidation\n",
    "    cache_inputs = [model, str(EXPECTED_STRATEGIES), str(OPTIONAL_STRATEGIES), str(ANCHOR_DIRS)]\n",
    "    return hashlib.md5(''.join(cache_inputs).encode()).hexdigest()\n",
    "\n",
    "print(f\"=== ENHANCED PROCESSING TIME ANALYSIS FOR MODEL: {MODEL} ===\")\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  - Parallel workers: {MAX_WORKERS}\")\n",
    "print(f\"  - Batch processing: {BATCH_SIZE} records per batch\")\n",
    "print(f\"  - Cache enabled: {USE_CACHE}\")\n",
    "print(f\"  - Core strategies: {len(EXPECTED_STRATEGIES)} ({', '.join(EXPECTED_STRATEGIES.keys())})\")\n",
    "print(f\"  - Optional strategies: {len(OPTIONAL_STRATEGIES)} ({', '.join(OPTIONAL_STRATEGIES.keys())})\")\n",
    "print(f\"  - Expected anchors: {len(ANCHOR_DIRS)} ({', '.join(ANCHOR_DIRS)})\")\n",
    "print(f\"Directory structure:\")\n",
    "for anchor in ANCHOR_DIRS:\n",
    "    print(f\"  - {anchor}/MODEL/{{base_version,with_geom}} + optional strategies\")\n",
    "print(f\"Estimated files:\")\n",
    "print(f\"  - middle: 2 core strategies (base_version, with_geom)\")\n",
    "print(f\"  - penultimate: 3 strategies (+ base_version_second_try)\")\n",
    "print(f\"  - Total: ~40+ files (varies by strategy)\")\n",
    "print(f\"Base path: {base_path}\")\n",
    "print(f\"Results path: {results_path}\")\n",
    "print(f\"Cache path: {cache_path}\")\n",
    "print(f\"Results path exists: {results_path.exists()}\")\n",
    "print(f\"✅ Focus on base_version and with_geom strategies for performance analysis\")\n",
    "print(f\"Initial memory usage: {get_memory_usage():.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============= ENHANCED DATA LOADING WITH OPTIMIZATIONS =============\n",
    "\n",
    "class ProcessingTimeAnalyzer:\n",
    "    \"\"\"Enhanced processing time analyzer with caching, validation and parallel processing\"\"\"\n",
    "    \n",
    "    def __init__(self, model: str, base_path: Path):\n",
    "        self.model = model\n",
    "        self.base_path = base_path\n",
    "        self.results_path = base_path / 'results'\n",
    "        self.cache_path = cache_path\n",
    "        self.file_stats = defaultdict(int)\n",
    "        self.record_stats = defaultdict(int)\n",
    "        \n",
    "    def validate_path_structure(self) -> Tuple[bool, List[str]]:\n",
    "        \"\"\"Validate that the expected directory structure exists\"\"\"\n",
    "        validation_errors = []\n",
    "        \n",
    "        if not self.results_path.exists():\n",
    "            validation_errors.append(f\"Results directory not found: {self.results_path}\")\n",
    "            return False, validation_errors\n",
    "            \n",
    "        all_strategies = {**EXPECTED_STRATEGIES, **OPTIONAL_STRATEGIES}\n",
    "        \n",
    "        for anchor_type in ANCHOR_DIRS:\n",
    "            model_path = self.results_path / anchor_type / self.model\n",
    "            \n",
    "            if not model_path.exists():\n",
    "                validation_errors.append(f\"Model directory not found: {model_path}\")\n",
    "                continue\n",
    "                \n",
    "            # Check for strategy directories\n",
    "            existing_strategies = {d.name for d in model_path.iterdir() if d.is_dir()}\n",
    "            \n",
    "            # Core strategies validation - require at least these in middle\n",
    "            if anchor_type == 'middle':\n",
    "                missing_core = set(EXPECTED_STRATEGIES.keys()) - existing_strategies\n",
    "                if missing_core:\n",
    "                    validation_errors.append(f\"Missing core strategies in {model_path}: {missing_core}\")\n",
    "            \n",
    "            # Check for CSV files in each existing strategy\n",
    "            for strategy in existing_strategies:\n",
    "                if strategy in all_strategies:  # Only check known strategies\n",
    "                    strategy_path = model_path / strategy\n",
    "                    csv_files = list(strategy_path.glob('*.csv'))\n",
    "                    csv_files = [f for f in csv_files if not f.name.endswith('_checkpoint.txt')]\n",
    "                    \n",
    "                    if not csv_files:\n",
    "                        validation_errors.append(f\"No CSV files found in {strategy_path}\")\n",
    "                        \n",
    "        return len(validation_errors) == 0, validation_errors\n",
    "    \n",
    "    def get_cache_filepath(self) -> Path:\n",
    "        \"\"\"Get cache file path for current model\"\"\"\n",
    "        cache_key = get_cache_key(self.model)\n",
    "        return self.cache_path / f\"processing_times_{self.model}_{cache_key}.pkl\"\n",
    "        \n",
    "    def save_to_cache(self, data: pd.DataFrame) -> None:\n",
    "        \"\"\"Save processed data to cache\"\"\"\n",
    "        if not USE_CACHE:\n",
    "            return\n",
    "            \n",
    "        cache_file = self.get_cache_filepath()\n",
    "        try:\n",
    "            with open(cache_file, 'wb') as f:\n",
    "                pickle.dump({\n",
    "                    'data': data,\n",
    "                    'model': self.model,\n",
    "                    'timestamp': pd.Timestamp.now(),\n",
    "                    'file_stats': dict(self.file_stats),\n",
    "                    'record_stats': dict(self.record_stats)\n",
    "                }, f)\n",
    "            print(f\"✅ Data cached to: {cache_file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Failed to save cache: {e}\")\n",
    "    \n",
    "    def load_from_cache(self) -> Optional[pd.DataFrame]:\n",
    "        \"\"\"Load processed data from cache if available\"\"\"\n",
    "        if not USE_CACHE:\n",
    "            return None\n",
    "            \n",
    "        cache_file = self.get_cache_filepath()\n",
    "        if not cache_file.exists():\n",
    "            return None\n",
    "            \n",
    "        try:\n",
    "            with open(cache_file, 'rb') as f:\n",
    "                cached = pickle.load(f)\n",
    "                \n",
    "            print(f\"✅ Loaded from cache: {cache_file}\")\n",
    "            print(f\"   Cache timestamp: {cached['timestamp']}\")\n",
    "            print(f\"   Cached records: {len(cached['data']):,}\")\n",
    "            \n",
    "            # Restore stats\n",
    "            self.file_stats = defaultdict(int, cached.get('file_stats', {}))\n",
    "            self.record_stats = defaultdict(int, cached.get('record_stats', {}))\n",
    "            \n",
    "            return cached['data']\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Failed to load cache: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def process_single_file(self, file_info: Dict) -> Tuple[List[Dict], int, str]:\n",
    "        \"\"\"Process a single CSV file with enhanced error handling\"\"\"\n",
    "        csv_file = Path(file_info['path'])\n",
    "        model_name = file_info['model']\n",
    "        strategy_label = file_info['strategy']\n",
    "        anchor_type = file_info['anchor']\n",
    "        \n",
    "        records = []\n",
    "        total_records = 0\n",
    "        error_msg = \"\"\n",
    "        \n",
    "        try:\n",
    "            # Validate file exists and is readable\n",
    "            if not csv_file.exists():\n",
    "                return [], 0, f\"File not found: {csv_file}\"\n",
    "                \n",
    "            if csv_file.stat().st_size == 0:\n",
    "                return [], 0, f\"Empty file: {csv_file}\"\n",
    "            \n",
    "            # Read file in chunks with specific error handling\n",
    "            # FIXED: Remove low_memory parameter when using python engine\n",
    "            try:\n",
    "                chunk_iter = pd.read_csv(\n",
    "                    csv_file, \n",
    "                    chunksize=CHUNK_SIZE,\n",
    "                    on_bad_lines='skip', \n",
    "                    engine='python'\n",
    "                )\n",
    "            except pd.errors.EmptyDataError:\n",
    "                return [], 0, f\"Empty data in file: {csv_file.name}\"\n",
    "            except pd.errors.ParserError as e:\n",
    "                return [], 0, f\"Parser error in {csv_file.name}: {str(e)[:100]}\"\n",
    "            except UnicodeDecodeError as e:\n",
    "                return [], 0, f\"Encoding error in {csv_file.name}: {str(e)[:100]}\"\n",
    "            \n",
    "            # Process chunks\n",
    "            for chunk_idx, chunk in enumerate(chunk_iter):\n",
    "                try:\n",
    "                    # Validate required columns\n",
    "                    if 'processing_time' not in chunk.columns:\n",
    "                        if chunk_idx == 0:  # Only warn once per file\n",
    "                            error_msg = f\"Missing 'processing_time' column in {csv_file.name}\"\n",
    "                        continue\n",
    "                    \n",
    "                    # Filter valid processing times\n",
    "                    valid_chunk = chunk.dropna(subset=['processing_time'])\n",
    "                    \n",
    "                    # Validate data types\n",
    "                    try:\n",
    "                        valid_chunk['processing_time'] = pd.to_numeric(valid_chunk['processing_time'], errors='coerce')\n",
    "                        valid_chunk = valid_chunk.dropna(subset=['processing_time'])\n",
    "                    except Exception as e:\n",
    "                        if chunk_idx == 0:\n",
    "                            error_msg = f\"Invalid processing_time format in {csv_file.name}: {str(e)[:100]}\"\n",
    "                        continue\n",
    "                    \n",
    "                    if len(valid_chunk) == 0:\n",
    "                        continue\n",
    "                    \n",
    "                    # Filter reasonable processing times (0.1s to 1000s)\n",
    "                    valid_chunk = valid_chunk[\n",
    "                        (valid_chunk['processing_time'] >= 0.1) & \n",
    "                        (valid_chunk['processing_time'] <= 1000)\n",
    "                    ]\n",
    "                    \n",
    "                    if len(valid_chunk) == 0:\n",
    "                        continue\n",
    "                    \n",
    "                    # Extract dataset name\n",
    "                    dataset_name = csv_file.stem.split('_pred_')[0] if '_pred_' in csv_file.stem else csv_file.stem\n",
    "                    \n",
    "                    # Batch process records for better performance\n",
    "                    batch_records = [\n",
    "                        {\n",
    "                            'strategy': strategy_label,\n",
    "                            'model': model_name,\n",
    "                            'anchor': anchor_type,\n",
    "                            'dataset': dataset_name,\n",
    "                            'file': csv_file.name,\n",
    "                            'processing_time': float(row['processing_time'])\n",
    "                        }\n",
    "                        for _, row in valid_chunk.iterrows()\n",
    "                    ]\n",
    "                    \n",
    "                    records.extend(batch_records)\n",
    "                    total_records += len(batch_records)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    error_msg = f\"Chunk processing error in {csv_file.name}: {str(e)[:100]}\"\n",
    "                    continue\n",
    "        \n",
    "        except PermissionError:\n",
    "            error_msg = f\"Permission denied: {csv_file}\"\n",
    "        except MemoryError:\n",
    "            error_msg = f\"Memory error processing: {csv_file.name}\"\n",
    "        except Exception as e:\n",
    "            error_msg = f\"Unexpected error in {csv_file.name}: {str(e)[:100]}\"\n",
    "        \n",
    "        return records, total_records, error_msg\n",
    "    \n",
    "    def collect_file_list(self) -> List[Dict]:\n",
    "        \"\"\"Collect list of all CSV files to process\"\"\"\n",
    "        file_list = []\n",
    "        all_strategies = {**EXPECTED_STRATEGIES, **OPTIONAL_STRATEGIES}\n",
    "        \n",
    "        for anchor_type in ANCHOR_DIRS:\n",
    "            model_path = self.results_path / anchor_type / self.model\n",
    "            \n",
    "            if not model_path.exists():\n",
    "                print(f\"⚠️ Skipping missing directory: {model_path}\")\n",
    "                continue\n",
    "            \n",
    "            strategy_dirs = [d for d in model_path.iterdir() if d.is_dir()]\n",
    "            \n",
    "            for strategy_dir in strategy_dirs:\n",
    "                strategy = strategy_dir.name\n",
    "                \n",
    "                # Only process known strategies\n",
    "                if strategy in all_strategies:\n",
    "                    strategy_label = all_strategies[strategy]\n",
    "                    \n",
    "                    csv_files = list(strategy_dir.glob('*.csv'))\n",
    "                    csv_files = [f for f in csv_files if not f.name.endswith('_checkpoint.txt')]\n",
    "                    \n",
    "                    print(f\"📁 Found {len(csv_files)} CSV files in {anchor_type}/{self.model}/{strategy}\")\n",
    "                    \n",
    "                    for csv_file in csv_files:\n",
    "                        file_list.append({\n",
    "                            'path': str(csv_file),\n",
    "                            'model': self.model,\n",
    "                            'strategy': strategy_label,\n",
    "                            'anchor': anchor_type,\n",
    "                            'filename': csv_file.name\n",
    "                        })\n",
    "                else:\n",
    "                    print(f\"⚠️ Ignoring unknown strategy: {strategy} in {anchor_type}\")\n",
    "        \n",
    "        return file_list\n",
    "    \n",
    "    def load_all_processing_times_optimized(self) -> pd.DataFrame:\n",
    "        \"\"\"Load all processing times with optimizations\"\"\"\n",
    "        \n",
    "        # Try loading from cache first\n",
    "        cached_data = self.load_from_cache()\n",
    "        if cached_data is not None:\n",
    "            print(\"🚀 Using cached data\")\n",
    "            return cached_data\n",
    "        \n",
    "        # Validate directory structure\n",
    "        is_valid, errors = self.validate_path_structure()\n",
    "        if not is_valid:\n",
    "            print(\"❌ Directory structure validation failed:\")\n",
    "            for error in errors:\n",
    "                print(f\"   - {error}\")\n",
    "            print(\"⚠️ Continuing with available data...\")\n",
    "        \n",
    "        # Collect all files to process\n",
    "        file_list = self.collect_file_list()\n",
    "        \n",
    "        if not file_list:\n",
    "            print(\"❌ No files found to process\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        print(f\"📁 Found {len(file_list)} files to process\")\n",
    "        print(f\"🔧 Processing with {MAX_WORKERS} parallel workers...\")\n",
    "        \n",
    "        # Process files in parallel with progress bar\n",
    "        all_dataframes = []\n",
    "        successful_files = 0\n",
    "        failed_files = 0\n",
    "        \n",
    "        with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "            # Submit all file processing tasks\n",
    "            future_to_file = {\n",
    "                executor.submit(self.process_single_file, file_info): file_info \n",
    "                for file_info in file_list\n",
    "            }\n",
    "            \n",
    "            # Process completed tasks with progress bar\n",
    "            with tqdm(total=len(file_list), desc=\"Processing files\", unit=\"file\") as pbar:\n",
    "                batch_records = []\n",
    "                \n",
    "                for future in as_completed(future_to_file):\n",
    "                    file_info = future_to_file[future]\n",
    "                    \n",
    "                    try:\n",
    "                        records, record_count, error_msg = future.result()\n",
    "                        \n",
    "                        if error_msg:\n",
    "                            print(f\"⚠️ {error_msg}\")\n",
    "                            failed_files += 1\n",
    "                        else:\n",
    "                            successful_files += 1\n",
    "                            \n",
    "                        if records:\n",
    "                            batch_records.extend(records)\n",
    "                            \n",
    "                            # Update statistics\n",
    "                            strategy = file_info['strategy']\n",
    "                            anchor = file_info['anchor']\n",
    "                            self.file_stats[f\"{strategy}_{anchor}\"] += 1\n",
    "                            self.record_stats[strategy] += record_count\n",
    "                            \n",
    "                            # Create DataFrame in batches to manage memory\n",
    "                            if len(batch_records) >= BATCH_SIZE:\n",
    "                                df_batch = pd.DataFrame(batch_records)\n",
    "                                all_dataframes.append(df_batch)\n",
    "                                batch_records.clear()\n",
    "                                gc.collect()  # Free memory\n",
    "                                \n",
    "                        pbar.set_postfix({\n",
    "                            'Success': successful_files, \n",
    "                            'Failed': failed_files,\n",
    "                            'Memory': f\"{get_memory_usage():.0f}MB\"\n",
    "                        })\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        print(f\"❌ Task execution error for {file_info['filename']}: {e}\")\n",
    "                        failed_files += 1\n",
    "                    \n",
    "                    pbar.update(1)\n",
    "                \n",
    "                # Process remaining records\n",
    "                if batch_records:\n",
    "                    df_batch = pd.DataFrame(batch_records)\n",
    "                    all_dataframes.append(df_batch)\n",
    "        \n",
    "        # Combine all DataFrames\n",
    "        if not all_dataframes:\n",
    "            print(\"❌ No valid data found\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        print(f\"🔄 Combining {len(all_dataframes)} DataFrame batches...\")\n",
    "        \n",
    "        try:\n",
    "            final_df = pd.concat(all_dataframes, ignore_index=True)\n",
    "            \n",
    "            # Final data validation and cleanup\n",
    "            final_df = final_df.dropna(subset=['processing_time'])\n",
    "            final_df = final_df[final_df['processing_time'] > 0]\n",
    "            \n",
    "            print(f\"✅ Data loading completed:\")\n",
    "            print(f\"   - Successful files: {successful_files}\")\n",
    "            print(f\"   - Failed files: {failed_files}\")\n",
    "            print(f\"   - Total records: {len(final_df):,}\")\n",
    "            print(f\"   - Memory usage: {get_memory_usage():.1f} MB\")\n",
    "            \n",
    "            # Save to cache for future use\n",
    "            self.save_to_cache(final_df)\n",
    "            \n",
    "            return final_df\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error combining DataFrames: {e}\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "# Initialize analyzer and load data\n",
    "print(f\"🚀 Initializing enhanced analyzer for model: {MODEL}\")\n",
    "analyzer = ProcessingTimeAnalyzer(MODEL, base_path)\n",
    "\n",
    "try:\n",
    "    df_times = analyzer.load_all_processing_times_optimized()\n",
    "    \n",
    "    if len(df_times) == 0:\n",
    "        print(f\"❌ No data loaded for model {MODEL}\")\n",
    "    else:\n",
    "        print(f\"\\n🎉 SUCCESS! Loaded {len(df_times):,} processing time records\")\n",
    "        print(f\"📊 Data overview:\")\n",
    "        print(f\"   - Strategies: {sorted(df_times['strategy'].unique())}\")\n",
    "        print(f\"   - Anchors: {sorted(df_times['anchor'].unique())}\")\n",
    "        print(f\"   - Datasets: {len(df_times['dataset'].unique())}\")\n",
    "        print(f\"   - Processing time range: {df_times['processing_time'].min():.3f}s - {df_times['processing_time'].max():.3f}s\")\n",
    "        \n",
    "        # Verify strategy-anchor combinations\n",
    "        combinations = df_times.groupby(['strategy', 'anchor']).size()\n",
    "        print(f\"\\n📈 Strategy-Anchor combinations:\")\n",
    "        for (strategy, anchor), count in combinations.items():\n",
    "            print(f\"   - {strategy} × {anchor}: {count:,} records\")\n",
    "        \n",
    "        # Memory cleanup\n",
    "        gc.collect()\n",
    "        print(f\"💾 Final memory usage: {get_memory_usage():.1f} MB\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ CRITICAL ERROR: {e}\")\n",
    "    df_times = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data overview with safety checks - optimized for single model\n",
    "if len(df_times) == 0:\n",
    "    print(f\"ERROR: No data available for model {MODEL}. Please check the data loading step.\")\n",
    "else:\n",
    "    print(f\"=== DATA OVERVIEW FOR MODEL {MODEL} ===\")\n",
    "    print(f\"Total records: {len(df_times):,}\")\n",
    "    print(f\"Processing time range: {df_times['processing_time'].min():.2f} - {df_times['processing_time'].max():.2f} seconds\")\n",
    "    print(f\"Mean processing time: {df_times['processing_time'].mean():.2f} seconds\")\n",
    "    print(f\"Median processing time: {df_times['processing_time'].median():.2f} seconds\")\n",
    "\n",
    "    print(\"\\n=== RECORDS PER STRATEGY ===\")\n",
    "    strategy_counts = df_times['strategy'].value_counts()\n",
    "    for strategy, count in strategy_counts.items():\n",
    "        print(f\"{strategy}: {count:,} records\")\n",
    "        \n",
    "    print(\"\\n=== RECORDS PER ANCHOR TYPE ===\")\n",
    "    anchor_counts = df_times['anchor'].value_counts()\n",
    "    for anchor, count in anchor_counts.items():\n",
    "        print(f\"{anchor}: {count:,} records\")\n",
    "\n",
    "    print(\"\\n=== BASIC STATISTICS BY STRATEGY ===\")\n",
    "    strategy_stats = df_times.groupby('strategy')['processing_time'].agg([\n",
    "        'count', 'mean', 'median', 'std', 'min', 'max'\n",
    "    ]).round(3)\n",
    "    print(strategy_stats)\n",
    "    \n",
    "    # Add anchor type comparison\n",
    "    if len(df_times['anchor'].unique()) > 1:\n",
    "        print(\"\\n=== BASIC STATISTICS BY ANCHOR TYPE ===\")\n",
    "        anchor_stats = df_times.groupby('anchor')['processing_time'].agg([\n",
    "            'count', 'mean', 'median', 'std', 'min', 'max'\n",
    "        ]).round(3)\n",
    "        print(anchor_stats)\n",
    "    \n",
    "    print(f\"\\nMemory usage: {get_memory_usage():.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing Time Distribution by Strategy - Optimized for single model\n",
    "if len(df_times) == 0:\n",
    "    print(\"No data available for visualization.\")\n",
    "else:\n",
    "    # Enhanced visualization for single model analysis\n",
    "    fig = plt.figure(figsize=(15, 12))\n",
    "    \n",
    "    try:\n",
    "        # Create a 2x3 grid for comprehensive analysis\n",
    "        gs = fig.add_gridspec(3, 2, hspace=0.3, wspace=0.3)\n",
    "        \n",
    "        # 1. Box plot by strategy\n",
    "        ax1 = fig.add_subplot(gs[0, 0])\n",
    "        sns.boxplot(data=df_times, x='strategy', y='processing_time', ax=ax1)\n",
    "        ax1.set_title(f'Processing Time Distribution by Strategy\\n({MODEL})')\n",
    "        ax1.set_xlabel('Strategy')\n",
    "        ax1.set_ylabel('Processing Time (seconds)')\n",
    "        ax1.tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # 2. Box plot by anchor type (if available)\n",
    "        ax2 = fig.add_subplot(gs[0, 1])\n",
    "        if len(df_times['anchor'].unique()) > 1:\n",
    "            sns.boxplot(data=df_times, x='anchor', y='processing_time', ax=ax2)\n",
    "            ax2.set_title('Processing Time by Anchor Type')\n",
    "            ax2.set_xlabel('Anchor Type')\n",
    "            ax2.set_ylabel('Processing Time (seconds)')\n",
    "        else:\n",
    "            # Show histogram if only one anchor type\n",
    "            df_times['processing_time'].hist(bins=50, ax=ax2)\n",
    "            ax2.set_title('Processing Time Distribution')\n",
    "            ax2.set_xlabel('Processing Time (seconds)')\n",
    "            ax2.set_ylabel('Frequency')\n",
    "\n",
    "        # 3. Mean processing time bar chart\n",
    "        ax3 = fig.add_subplot(gs[1, 0])\n",
    "        strategy_means = df_times.groupby('strategy')['processing_time'].mean().sort_values()\n",
    "        bars = ax3.bar(range(len(strategy_means)), strategy_means.values, \n",
    "                      color=['#ff9999', '#66b3ff', '#99ff99'][:len(strategy_means)])\n",
    "        ax3.set_title('Mean Processing Time by Strategy')\n",
    "        ax3.set_xlabel('Strategy')\n",
    "        ax3.set_ylabel('Mean Processing Time (seconds)')\n",
    "        ax3.set_xticks(range(len(strategy_means)))\n",
    "        ax3.set_xticklabels(strategy_means.index, rotation=45, ha='right')\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for i, bar in enumerate(bars):\n",
    "            height = bar.get_height()\n",
    "            ax3.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                     f'{height:.2f}s', ha='center', va='bottom', fontsize=9)\n",
    "        \n",
    "        # 4. Processing rate (predictions per second)\n",
    "        ax4 = fig.add_subplot(gs[1, 1])\n",
    "        processing_rates = 1 / strategy_means\n",
    "        bars4 = ax4.bar(range(len(processing_rates)), processing_rates.values, \n",
    "                       color=['#ffcc99', '#c2c2f0', '#ccffcc'][:len(processing_rates)])\n",
    "        ax4.set_title('Processing Rate by Strategy')\n",
    "        ax4.set_xlabel('Strategy')\n",
    "        ax4.set_ylabel('Predictions/Second')\n",
    "        ax4.set_xticks(range(len(processing_rates)))\n",
    "        ax4.set_xticklabels(processing_rates.index, rotation=45, ha='right')\n",
    "        \n",
    "        # Add value labels\n",
    "        for i, bar in enumerate(bars4):\n",
    "            height = bar.get_height()\n",
    "            ax4.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                     f'{height:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "        \n",
    "        # 5. Strategy comparison pie chart\n",
    "        ax5 = fig.add_subplot(gs[2, 0])\n",
    "        strategy_counts = df_times['strategy'].value_counts()\n",
    "        colors = plt.cm.Set3(np.linspace(0, 1, len(strategy_counts)))\n",
    "        ax5.pie(strategy_counts.values, labels=strategy_counts.index, autopct='%1.1f%%', \n",
    "               startangle=90, colors=colors)\n",
    "        ax5.set_title('Data Distribution by Strategy')\n",
    "        \n",
    "        # 6. Processing time violin plot\n",
    "        ax6 = fig.add_subplot(gs[2, 1])\n",
    "        sns.violinplot(data=df_times, x='strategy', y='processing_time', ax=ax6)\n",
    "        ax6.set_title('Processing Time Distribution (Violin Plot)')\n",
    "        ax6.set_xlabel('Strategy')\n",
    "        ax6.set_ylabel('Processing Time (seconds)')\n",
    "        ax6.tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        plt.suptitle(f'Processing Time Analysis - Model: {MODEL}', fontsize=16, y=0.98)\n",
    "        plt.show()\n",
    "        \n",
    "        # Force cleanup\n",
    "        plt.close()\n",
    "        gc.collect()\n",
    "        print(f\"Memory usage after visualization: {get_memory_usage():.1f} MB\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error creating visualization: {e}\")\n",
    "        plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed Statistics Table - Complete Dataset Analysis\n",
    "if len(df_times) == 0:\n",
    "    print(\"No data available for statistical analysis.\")\n",
    "else:\n",
    "    print(f\"=== COMPLETE PROCESSING TIME STATISTICS FOR {MODEL} ===\")\n",
    "    print(f\"Total records analyzed: {len(df_times):,}\")\n",
    "\n",
    "    # Detailed statistics by strategy\n",
    "    detailed_stats = df_times.groupby('strategy')['processing_time'].agg([\n",
    "        'count',\n",
    "        'mean',\n",
    "        'median', \n",
    "        'std',\n",
    "        'min',\n",
    "        ('q25', lambda x: x.quantile(0.25)),\n",
    "        ('q75', lambda x: x.quantile(0.75)),\n",
    "        'max',\n",
    "        ('range', lambda x: x.max() - x.min()),\n",
    "        ('cv', lambda x: x.std() / x.mean())  # Coefficient of variation\n",
    "    ]).round(4)\n",
    "\n",
    "    print(\"\\n--- Processing Time Statistics by Strategy ---\")\n",
    "    print(detailed_stats)\n",
    "    \n",
    "    # Add efficiency metrics\n",
    "    print(\"\\n--- Efficiency Metrics ---\")\n",
    "    efficiency_stats = df_times.groupby('strategy')['processing_time'].agg(['mean', 'count']).round(4)\n",
    "    efficiency_stats['predictions_per_second'] = (1 / efficiency_stats['mean']).round(4)\n",
    "    efficiency_stats['records_per_hour'] = (3600 / efficiency_stats['mean']).round(0)\n",
    "    print(efficiency_stats)\n",
    "\n",
    "    # Statistics by anchor type if available\n",
    "    if len(df_times['anchor'].unique()) > 1:\n",
    "        print(f\"\\n--- Processing Time Statistics by Anchor Type ---\")\n",
    "        anchor_stats = df_times.groupby('anchor')['processing_time'].agg([\n",
    "            'count', 'mean', 'median', 'std', 'min', 'max'\n",
    "        ]).round(4)\n",
    "        print(anchor_stats)\n",
    "        \n",
    "        # Cross-tabulation: Strategy × Anchor\n",
    "        print(f\"\\n--- Strategy × Anchor Cross-Analysis ---\")\n",
    "        cross_stats = df_times.groupby(['strategy', 'anchor'])['processing_time'].agg([\n",
    "            'count', 'mean', 'std'\n",
    "        ]).round(4)\n",
    "        print(cross_stats)\n",
    "\n",
    "    # Dataset-level analysis if available\n",
    "    if 'dataset' in df_times.columns and len(df_times['dataset'].unique()) > 1:\n",
    "        print(f\"\\n--- Dataset Performance Analysis ---\")\n",
    "        dataset_stats = df_times.groupby('dataset')['processing_time'].agg([\n",
    "            'count', 'mean', 'median', 'std'\n",
    "        ]).round(4).sort_values('mean', ascending=False)\n",
    "        \n",
    "        print(\"Top 10 slowest datasets:\")\n",
    "        print(dataset_stats.head(10))\n",
    "        \n",
    "        print(\"\\nTop 10 fastest datasets:\")\n",
    "        print(dataset_stats.tail(10))\n",
    "\n",
    "    # Statistical significance tests - only if we have multiple strategies\n",
    "    strategies = df_times['strategy'].unique()\n",
    "    \n",
    "    if len(strategies) > 1:\n",
    "        try:\n",
    "            from scipy import stats\n",
    "\n",
    "            groups = [df_times[df_times['strategy'] == s]['processing_time'].values for s in strategies]\n",
    "\n",
    "            # Perform one-way ANOVA\n",
    "            f_stat, p_value = stats.f_oneway(*groups)\n",
    "\n",
    "            print(f\"\\n--- Statistical Significance Test ---\")\n",
    "            print(f\"One-way ANOVA F-statistic: {f_stat:.4f}\")\n",
    "            print(f\"P-value: {p_value:.2e}\")\n",
    "            print(f\"Significant difference between strategies: {'Yes' if p_value < 0.05 else 'No'}\")\n",
    "\n",
    "            # Effect size (eta-squared)\n",
    "            total_sum_squares = sum([(group - df_times['processing_time'].mean())**2 for group in groups for _ in group])\n",
    "            between_sum_squares = sum([len(group) * (group.mean() - df_times['processing_time'].mean())**2 for group in groups])\n",
    "            eta_squared = between_sum_squares / total_sum_squares\n",
    "            print(f\"Effect size (η²): {eta_squared:.4f}\")\n",
    "            \n",
    "            if eta_squared < 0.01:\n",
    "                effect_size = \"Small\"\n",
    "            elif eta_squared < 0.06:\n",
    "                effect_size = \"Medium\"  \n",
    "            else:\n",
    "                effect_size = \"Large\"\n",
    "            print(f\"Effect size interpretation: {effect_size}\")\n",
    "\n",
    "            # Pairwise comparisons with Bonferroni correction\n",
    "            if len(strategies) > 2:\n",
    "                print(f\"\\n--- Pairwise Comparisons (Bonferroni corrected) ---\")\n",
    "                from itertools import combinations\n",
    "                \n",
    "                alpha = 0.05\n",
    "                n_comparisons = len(list(combinations(strategies, 2)))\n",
    "                bonferroni_alpha = alpha / n_comparisons\n",
    "                \n",
    "                print(f\"Bonferroni corrected alpha: {bonferroni_alpha:.4f}\")\n",
    "                \n",
    "                for i, strategy1 in enumerate(strategies):\n",
    "                    for j, strategy2 in enumerate(strategies):\n",
    "                        if i < j:\n",
    "                            group1 = df_times[df_times['strategy'] == strategy1]['processing_time']\n",
    "                            group2 = df_times[df_times['strategy'] == strategy2]['processing_time']\n",
    "                            \n",
    "                            t_stat, p_val = stats.ttest_ind(group1, group2)\n",
    "                            is_significant = p_val < bonferroni_alpha\n",
    "                            \n",
    "                            mean_diff = group1.mean() - group2.mean()\n",
    "                            cohen_d = mean_diff / np.sqrt((group1.var() + group2.var()) / 2)\n",
    "                            \n",
    "                            print(f\"{strategy1} vs {strategy2}:\")\n",
    "                            print(f\"  Mean difference: {mean_diff:.4f}s\")\n",
    "                            print(f\"  t-statistic: {t_stat:.4f}\")\n",
    "                            print(f\"  p-value: {p_val:.6f}\")\n",
    "                            print(f\"  Significant: {'Yes' if is_significant else 'No'}\")\n",
    "                            print(f\"  Cohen's d: {cohen_d:.4f}\")\n",
    "                            print()\n",
    "                            \n",
    "        except ImportError:\n",
    "            print(\"scipy not available - skipping statistical tests\")\n",
    "        except Exception as e:\n",
    "            print(f\"Could not perform statistical tests: {e}\")\n",
    "    else:\n",
    "        print(\"Only one strategy found - no statistical comparison possible.\")\n",
    "    \n",
    "    print(f\"\\nMemory usage: {get_memory_usage():.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strategy and Anchor Comparison - Enhanced for single model\n",
    "if len(df_times) == 0:\n",
    "    print(\"No data available for strategy and anchor analysis.\")\n",
    "else:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    try:\n",
    "        # 1. Strategy vs Anchor heatmap (if both dimensions available)\n",
    "        if len(df_times['anchor'].unique()) > 1 and len(df_times['strategy'].unique()) > 1:\n",
    "            strategy_anchor_stats = df_times.groupby(['strategy', 'anchor'])['processing_time'].mean().unstack()\n",
    "            sns.heatmap(strategy_anchor_stats, annot=True, fmt='.2f', cmap='YlOrRd', ax=axes[0,0])\n",
    "            axes[0,0].set_title(f'Mean Processing Time by Strategy and Anchor\\n({MODEL})')\n",
    "            axes[0,0].set_xlabel('Anchor Type')\n",
    "            axes[0,0].set_ylabel('Strategy')\n",
    "        else:\n",
    "            # Show just strategy means if only one anchor type\n",
    "            strategy_means = df_times.groupby('strategy')['processing_time'].mean()\n",
    "            bars = axes[0,0].bar(range(len(strategy_means)), strategy_means.values)\n",
    "            axes[0,0].set_title(f'Mean Processing Time by Strategy\\n({MODEL})')\n",
    "            axes[0,0].set_xlabel('Strategy')\n",
    "            axes[0,0].set_ylabel('Processing Time (seconds)')\n",
    "            axes[0,0].set_xticks(range(len(strategy_means)))\n",
    "            axes[0,0].set_xticklabels(strategy_means.index, rotation=45, ha='right')\n",
    "\n",
    "        # 2. Processing time comparison by strategy and anchor\n",
    "        if len(df_times['anchor'].unique()) > 1:\n",
    "            sns.boxplot(data=df_times, x='strategy', y='processing_time', hue='anchor', ax=axes[0,1])\n",
    "            axes[0,1].set_title('Processing Time by Strategy and Anchor')\n",
    "            axes[0,1].set_xlabel('Strategy')\n",
    "            axes[0,1].set_ylabel('Processing Time (seconds)')\n",
    "            axes[0,1].tick_params(axis='x', rotation=45)\n",
    "            axes[0,1].legend(title='Anchor')\n",
    "        else:\n",
    "            # Show violin plot if only one anchor\n",
    "            sns.violinplot(data=df_times, x='strategy', y='processing_time', ax=axes[0,1])\n",
    "            axes[0,1].set_title('Processing Time Distribution by Strategy')\n",
    "            axes[0,1].set_xlabel('Strategy')\n",
    "            axes[0,1].set_ylabel('Processing Time (seconds)')\n",
    "            axes[0,1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "        # 3. Efficiency comparison\n",
    "        ax3 = axes[1,0]\n",
    "        strategy_stats = df_times.groupby('strategy')['processing_time'].agg(['mean', 'std'])\n",
    "        x_pos = np.arange(len(strategy_stats))\n",
    "        bars = ax3.bar(x_pos, strategy_stats['mean'], yerr=strategy_stats['std'], capsize=5)\n",
    "        ax3.set_title('Processing Time with Standard Deviation')\n",
    "        ax3.set_xlabel('Strategy')\n",
    "        ax3.set_ylabel('Processing Time (seconds)')\n",
    "        ax3.set_xticks(x_pos)\n",
    "        ax3.set_xticklabels(strategy_stats.index, rotation=45, ha='right')\n",
    "        \n",
    "        # Add value labels\n",
    "        for i, bar in enumerate(bars):\n",
    "            height = bar.get_height()\n",
    "            ax3.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                     f'{height:.2f}±{strategy_stats.iloc[i][\"std\"]:.2f}', \n",
    "                     ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "        # 4. Dataset analysis (if available)\n",
    "        ax4 = axes[1,1]\n",
    "        if 'dataset' in df_times.columns and len(df_times['dataset'].unique()) > 1:\n",
    "            dataset_means = df_times.groupby('dataset')['processing_time'].mean().sort_values()\n",
    "            top_datasets = dataset_means.tail(10)  # Show top 10 slowest datasets\n",
    "            bars = ax4.bar(range(len(top_datasets)), top_datasets.values)\n",
    "            ax4.set_title('Top 10 Slowest Datasets')\n",
    "            ax4.set_xlabel('Dataset')\n",
    "            ax4.set_ylabel('Mean Processing Time (seconds)')\n",
    "            ax4.set_xticks(range(len(top_datasets)))\n",
    "            ax4.set_xticklabels(top_datasets.index, rotation=45, ha='right')\n",
    "        else:\n",
    "            # Show cumulative distribution\n",
    "            sorted_times = np.sort(df_times['processing_time'])\n",
    "            y_vals = np.arange(len(sorted_times)) / float(len(sorted_times))\n",
    "            ax4.plot(sorted_times, y_vals)\n",
    "            ax4.set_title('Cumulative Distribution of Processing Times')\n",
    "            ax4.set_xlabel('Processing Time (seconds)')\n",
    "            ax4.set_ylabel('Cumulative Probability')\n",
    "            ax4.grid(True, alpha=0.3)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Print detailed comparison if multiple anchor types\n",
    "        if len(df_times['anchor'].unique()) > 1:\n",
    "            print(f\"\\n=== STRATEGY-ANCHOR COMBINATIONS FOR {MODEL} ===\")\n",
    "            combo_stats = df_times.groupby(['strategy', 'anchor'])['processing_time'].agg(['count', 'mean', 'std']).round(3)\n",
    "            print(combo_stats)\n",
    "        \n",
    "        # Force cleanup\n",
    "        plt.close()\n",
    "        gc.collect()\n",
    "        print(f\"\\nMemory usage after analysis: {get_memory_usage():.1f} MB\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error creating strategy-anchor analysis: {e}\")\n",
    "        plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance Summary Report\n",
    "if len(df_times) == 0:\n",
    "    print(\"No data available for performance summary.\")\n",
    "else:\n",
    "    print(\"=== PROCESSING TIME PERFORMANCE SUMMARY ===\")\n",
    "    print(\"\\nStrategy Performance Ranking (by mean processing time):\")\n",
    "\n",
    "    strategy_ranking = df_times.groupby('strategy')['processing_time'].agg([\n",
    "        'mean', 'median', 'count'\n",
    "    ]).sort_values('mean')\n",
    "\n",
    "    for i, (strategy, stats) in enumerate(strategy_ranking.iterrows(), 1):\n",
    "        print(f\"{i}. {strategy}:\")\n",
    "        print(f\"   Mean: {stats['mean']:.2f}s\")\n",
    "        print(f\"   Median: {stats['median']:.2f}s\")\n",
    "        print(f\"   Records: {stats['count']:,}\")\n",
    "        print()\n",
    "\n",
    "    # Efficiency metrics\n",
    "    print(\"\\n=== EFFICIENCY ANALYSIS ===\")\n",
    "    base_mean = strategy_ranking.loc['Base Version', 'mean'] if 'Base Version' in strategy_ranking.index else None\n",
    "\n",
    "    if base_mean:\n",
    "        print(f\"Base Version mean processing time: {base_mean:.2f}s\")\n",
    "        print(\"\\nOverhead compared to Base Version:\")\n",
    "        \n",
    "        for strategy, stats in strategy_ranking.iterrows():\n",
    "            if strategy != 'Base Version':\n",
    "                overhead = ((stats['mean'] - base_mean) / base_mean) * 100\n",
    "                print(f\"{strategy}: +{overhead:.1f}% ({stats['mean'] - base_mean:.2f}s additional)\")\n",
    "\n",
    "    # Processing rate (predictions per second)\n",
    "    print(\"\\n=== PROCESSING RATE ===\")\n",
    "    for strategy, stats in strategy_ranking.iterrows():\n",
    "        rate = 1 / stats['mean']\n",
    "        print(f\"{strategy}: {rate:.3f} predictions/second\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export summary statistics to CSV - Optimized for single model\n",
    "if len(df_times) == 0:\n",
    "    print(\"No data available for export.\")\n",
    "else:\n",
    "    try:\n",
    "        output_path = base_path / 'notebook' / f'processing_time_summary_{MODEL}.csv'\n",
    "\n",
    "        # Prepare detailed summary for single model\n",
    "        summary_data = []\n",
    "\n",
    "        for strategy in df_times['strategy'].unique():\n",
    "            strategy_data = df_times[df_times['strategy'] == strategy]['processing_time']\n",
    "            \n",
    "            base_stats = {\n",
    "                'model': MODEL,\n",
    "                'strategy': strategy,\n",
    "                'count': len(strategy_data),\n",
    "                'mean': strategy_data.mean(),\n",
    "                'median': strategy_data.median(),\n",
    "                'std': strategy_data.std(),\n",
    "                'min': strategy_data.min(),\n",
    "                'max': strategy_data.max(),\n",
    "                'q25': strategy_data.quantile(0.25),\n",
    "                'q75': strategy_data.quantile(0.75),\n",
    "                'predictions_per_second': 1 / strategy_data.mean() if strategy_data.mean() > 0 else 0\n",
    "            }\n",
    "            \n",
    "            # Add anchor-specific statistics if multiple anchors\n",
    "            if len(df_times['anchor'].unique()) > 1:\n",
    "                for anchor in df_times['anchor'].unique():\n",
    "                    anchor_data = df_times[(df_times['strategy'] == strategy) & \n",
    "                                         (df_times['anchor'] == anchor)]['processing_time']\n",
    "                    \n",
    "                    if len(anchor_data) > 0:\n",
    "                        anchor_stats = base_stats.copy()\n",
    "                        anchor_stats.update({\n",
    "                            'anchor': anchor,\n",
    "                            'count': len(anchor_data),\n",
    "                            'mean': anchor_data.mean(),\n",
    "                            'median': anchor_data.median(),\n",
    "                            'std': anchor_data.std(),\n",
    "                            'predictions_per_second': 1 / anchor_data.mean() if anchor_data.mean() > 0 else 0\n",
    "                        })\n",
    "                        summary_data.append(anchor_stats)\n",
    "            else:\n",
    "                base_stats['anchor'] = df_times['anchor'].iloc[0]\n",
    "                summary_data.append(base_stats)\n",
    "\n",
    "        summary_df = pd.DataFrame(summary_data)\n",
    "        summary_df.to_csv(output_path, index=False)\n",
    "\n",
    "        print(f\"Summary statistics exported to: {output_path}\")\n",
    "        print(f\"\\nSummary table for {MODEL}:\")\n",
    "        display_cols = ['strategy', 'anchor', 'count', 'mean', 'median', 'std', 'predictions_per_second']\n",
    "        available_cols = [col for col in display_cols if col in summary_df.columns]\n",
    "        print(summary_df[available_cols].round(3))\n",
    "        \n",
    "        # Export additional dataset-level statistics if available\n",
    "        if 'dataset' in df_times.columns and len(df_times['dataset'].unique()) > 1:\n",
    "            dataset_output_path = base_path / 'notebook' / f'dataset_processing_times_{MODEL}.csv'\n",
    "            dataset_stats = df_times.groupby(['dataset', 'strategy'])['processing_time'].agg([\n",
    "                'count', 'mean', 'median', 'std', 'min', 'max'\n",
    "            ]).round(3).reset_index()\n",
    "            dataset_stats.to_csv(dataset_output_path, index=False)\n",
    "            print(f\"Dataset-level statistics exported to: {dataset_output_path}\")\n",
    "        \n",
    "        # Force cleanup\n",
    "        del summary_data, summary_df\n",
    "        gc.collect()\n",
    "        print(f\"\\nMemory usage: {get_memory_usage():.1f} MB\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error exporting summary: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Summary Report - Optimized for single model\n",
    "if len(df_times) == 0:\n",
    "    print(f\"No data available for final analysis of model {MODEL}.\")\n",
    "else:\n",
    "    print(f\"=== PROCESSING TIME PERFORMANCE SUMMARY FOR {MODEL} ===\")\n",
    "    print(f\"\\nStrategy Performance Ranking (by mean processing time):\")\n",
    "\n",
    "    try:\n",
    "        strategy_ranking = df_times.groupby('strategy')['processing_time'].agg([\n",
    "            'mean', 'median', 'count', 'std'\n",
    "        ]).sort_values('mean')\n",
    "\n",
    "        for i, (strategy, stats) in enumerate(strategy_ranking.iterrows(), 1):\n",
    "            efficiency_ratio = 1 / stats['mean'] if stats['mean'] > 0 else 0\n",
    "            print(f\"{i}. {strategy}:\")\n",
    "            print(f\"   Mean: {stats['mean']:.2f}s (±{stats['std']:.2f}s)\")\n",
    "            print(f\"   Median: {stats['median']:.2f}s\")\n",
    "            print(f\"   Records: {stats['count']:,}\")\n",
    "            print(f\"   Efficiency: {efficiency_ratio:.3f} predictions/second\")\n",
    "            print()\n",
    "\n",
    "        # Efficiency analysis\n",
    "        print(f\"\\n=== EFFICIENCY ANALYSIS FOR {MODEL} ===\")\n",
    "        base_mean = strategy_ranking.loc['Base Version', 'mean'] if 'Base Version' in strategy_ranking.index else None\n",
    "\n",
    "        if base_mean:\n",
    "            print(f\"Base Version mean processing time: {base_mean:.2f}s\")\n",
    "            print(f\"Overhead compared to Base Version:\")\n",
    "            \n",
    "            for strategy, stats in strategy_ranking.iterrows():\n",
    "                if strategy != 'Base Version':\n",
    "                    overhead = ((stats['mean'] - base_mean) / base_mean) * 100\n",
    "                    additional_time = stats['mean'] - base_mean\n",
    "                    print(f\"  {strategy}: +{overhead:.1f}% (+{additional_time:.2f}s)\")\n",
    "                    \n",
    "            # Calculate efficiency loss\n",
    "            base_rate = 1 / base_mean\n",
    "            print(f\"\\nThroughput comparison:\")\n",
    "            print(f\"  Base Version: {base_rate:.3f} predictions/second\")\n",
    "            for strategy, stats in strategy_ranking.iterrows():\n",
    "                if strategy != 'Base Version':\n",
    "                    strategy_rate = 1 / stats['mean']\n",
    "                    rate_loss = ((base_rate - strategy_rate) / base_rate) * 100\n",
    "                    print(f\"  {strategy}: {strategy_rate:.3f} predictions/second (-{rate_loss:.1f}%)\")\n",
    "        else:\n",
    "            print(\"Base Version not found - showing absolute performance:\")\n",
    "            for strategy, stats in strategy_ranking.iterrows():\n",
    "                rate = 1 / stats['mean']\n",
    "                print(f\"  {strategy}: {rate:.3f} predictions/second\")\n",
    "\n",
    "        # Anchor type analysis if available\n",
    "        if len(df_times['anchor'].unique()) > 1:\n",
    "            print(f\"\\n=== ANCHOR TYPE COMPARISON FOR {MODEL} ===\")\n",
    "            anchor_ranking = df_times.groupby('anchor')['processing_time'].agg([\n",
    "                'mean', 'median', 'count'\n",
    "            ]).sort_values('mean')\n",
    "            \n",
    "            for anchor, stats in anchor_ranking.iterrows():\n",
    "                rate = 1 / stats['mean']\n",
    "                print(f\"{anchor}: {stats['mean']:.2f}s avg ({rate:.3f} pred/sec, {stats['count']:,} records)\")\n",
    "\n",
    "        # Statistical summary\n",
    "        print(f\"\\n=== STATISTICAL SUMMARY FOR {MODEL} ===\")\n",
    "        overall_stats = df_times['processing_time'].describe()\n",
    "        print(f\"Overall processing time statistics:\")\n",
    "        print(f\"  Count: {overall_stats['count']:,.0f}\")\n",
    "        print(f\"  Mean: {overall_stats['mean']:.2f}s\")\n",
    "        print(f\"  Std: {overall_stats['std']:.2f}s\")\n",
    "        print(f\"  Min: {overall_stats['min']:.2f}s\")\n",
    "        print(f\"  25th percentile: {overall_stats['25%']:.2f}s\")\n",
    "        print(f\"  Median: {overall_stats['50%']:.2f}s\")\n",
    "        print(f\"  75th percentile: {overall_stats['75%']:.2f}s\")\n",
    "        print(f\"  Max: {overall_stats['max']:.2f}s\")\n",
    "\n",
    "        print(f\"\\n=== ANALYSIS COMPLETE FOR {MODEL} ===\")\n",
    "        print(f\"Total processing time records analyzed: {len(df_times):,}\")\n",
    "        print(f\"Strategies analyzed: {', '.join(sorted(df_times['strategy'].unique()))}\")\n",
    "        print(f\"Anchor types analyzed: {', '.join(sorted(df_times['anchor'].unique()))}\")\n",
    "        if 'dataset' in df_times.columns:\n",
    "            print(f\"Datasets covered: {len(df_times['dataset'].unique())}\")\n",
    "        print(f\"Final memory usage: {get_memory_usage():.1f} MB\")\n",
    "        \n",
    "        # Final cleanup\n",
    "        gc.collect()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in final analysis: {e}\")\n",
    "\n",
    "# Memory cleanup at end\n",
    "try:\n",
    "    plt.close('all')  # Close any remaining plots\n",
    "    gc.collect()\n",
    "    print(f\"\\nFinal cleanup complete. Memory usage: {get_memory_usage():.1f} MB\")\n",
    "    print(f\"\\nTo analyze a different model, change the MODEL variable in the first cell and rerun the notebook.\")\n",
    "    print(f\"Available models: qwen2.5_7b, qwen2.5_14b, llama3.1_8b, mixtral_8x7b, deepseek-coder_33b, mistral_7b\")\n",
    "except:\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
