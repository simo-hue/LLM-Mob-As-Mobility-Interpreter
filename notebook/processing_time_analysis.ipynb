{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing Time Analysis by Prompt Strategy\n",
    "\n",
    "This notebook analyzes the processing time performance across different prompt strategies:\n",
    "- **base_version**: Basic version with minimal context\n",
    "- **with_geom**: Version with geospatial features\n",
    "- **with_geom_time**: Full version with temporal + geospatial analysis\n",
    "\n",
    "The analysis focuses on the `processing_time` column from prediction CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import os\n",
    "import gc\n",
    "import psutil\n",
    "from collections import defaultdict\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# ============= CONFIGURATION =============\n",
    "# MODEL SELECTION - Change this to analyze specific model\n",
    "MODEL = \"qwen2.5_7b\"  # Options: qwen2.5_7b, qwen2.5_14b, llama3.1_8b, mixtral_8x7b, deepseek-coder_33b, mistral_7b\n",
    "\n",
    "# Memory management settings\n",
    "pd.set_option('mode.chained_assignment', None)\n",
    "plt.rcParams['figure.max_open_warning'] = 0\n",
    "\n",
    "# Configuration for complete analysis - NO LIMITS\n",
    "MAX_FILES_PER_STRATEGY = None  # Load ALL files (9 per strategy = 18 total)\n",
    "SAMPLE_SIZE = None  # Load ALL records - no sampling\n",
    "CHUNK_SIZE = 5000  # Larger chunks for better performance with complete dataset\n",
    "\n",
    "# Set paths\n",
    "base_path = Path('/leonardo_work/IscrC_LLM-Mob/LLM-Mob-As-Mobility-Interpreter')\n",
    "results_path = base_path / 'results'\n",
    "\n",
    "def get_memory_usage():\n",
    "    \"\"\"Get current memory usage in MB\"\"\"\n",
    "    process = psutil.Process(os.getpid())\n",
    "    return process.memory_info().rss / 1024 / 1024\n",
    "\n",
    "print(f\"=== COMPLETE PROCESSING TIME ANALYSIS FOR MODEL: {MODEL} ===\")\n",
    "print(f\"Configuration: Loading ALL files with ALL records (no sampling)\")\n",
    "print(f\"Expected: 9 files per strategy Ã— 2 strategies = 18 total files\")\n",
    "print(f\"Base path: {base_path}\")\n",
    "print(f\"Results path: {results_path}\")\n",
    "print(f\"Results path exists: {results_path.exists()}\")\n",
    "print(f\"Initial memory usage: {get_memory_usage():.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_processing_times_complete():\n",
    "    \"\"\"\n",
    "    Load ALL processing times from CSV files for a single model from middle/ and penultimate/ directories\n",
    "    NO SAMPLING - Complete analysis of all 18 files (9 per strategy)\n",
    "    Returns: DataFrame with columns [strategy, anchor, dataset, processing_time]\n",
    "    \"\"\"\n",
    "    all_data = []\n",
    "    files_processed = 0\n",
    "    \n",
    "    # Define strategy mapping\n",
    "    strategy_names = {\n",
    "        'base_version': 'Base Version',\n",
    "        'with_geom': 'With Geometry', \n",
    "        'with_geom_time': 'With Geometry + Time'\n",
    "    }\n",
    "    \n",
    "    file_counts = defaultdict(int)\n",
    "    record_counts = defaultdict(int)\n",
    "    \n",
    "    # Define anchor directories to search\n",
    "    anchor_dirs = ['middle', 'penultimate']\n",
    "    \n",
    "    print(f\"Starting complete data loading for model {MODEL}...\")\n",
    "    print(\"Loading from both middle/ and penultimate/ directories with NO sampling limits\")\n",
    "    \n",
    "    try:\n",
    "        for anchor_type in anchor_dirs:\n",
    "            anchor_path = results_path / anchor_type / MODEL\n",
    "            \n",
    "            if not anchor_path.exists():\n",
    "                print(f\"Warning: Directory {anchor_path} does not exist, skipping...\")\n",
    "                continue\n",
    "                \n",
    "            print(f\"\\n=== Processing {anchor_type.upper()} directory: {anchor_path} ===\")\n",
    "            \n",
    "            # Look for strategy subdirectories first (expected structure)\n",
    "            strategy_dirs = [d for d in anchor_path.iterdir() if d.is_dir()]\n",
    "            print(f\"Found strategy directories: {[d.name for d in strategy_dirs]}\")\n",
    "            \n",
    "            for strategy_dir in strategy_dirs:\n",
    "                strategy = strategy_dir.name\n",
    "                strategy_label = strategy_names.get(strategy, strategy)\n",
    "                \n",
    "                print(f\"\\n--- Processing strategy: {strategy_label} ---\")\n",
    "                print(f\"    Directory: {strategy_dir}\")\n",
    "                \n",
    "                # Load ALL CSV files in this strategy directory\n",
    "                csv_files = list(strategy_dir.glob('*.csv'))\n",
    "                csv_files = [f for f in csv_files if not f.name.endswith('_checkpoint.txt')]\n",
    "                \n",
    "                print(f\"    Found {len(csv_files)} CSV files\")\n",
    "                \n",
    "                # Process ALL files (no limits)\n",
    "                for csv_file in csv_files:\n",
    "                    try:\n",
    "                        records_loaded = process_csv_file_complete(csv_file, MODEL, strategy_label, anchor_type, all_data)\n",
    "                        files_processed += 1\n",
    "                        file_counts[f\"{strategy_label}_{anchor_type}\"] += 1\n",
    "                        record_counts[strategy_label] += records_loaded\n",
    "                        \n",
    "                        print(f\"    âœ“ {csv_file.name}: {records_loaded:,} records loaded\")\n",
    "                        \n",
    "                        if files_processed % 3 == 0:\n",
    "                            print(f\"      Progress: {files_processed} files processed. Memory: {get_memory_usage():.1f} MB\")\n",
    "                            gc.collect()\n",
    "                            \n",
    "                    except Exception as e:\n",
    "                        print(f\"    âœ— Error loading {csv_file.name}: {e}\")\n",
    "                        continue\n",
    "            \n",
    "            # Also check for direct CSV files in anchor directory (backup check)\n",
    "            direct_csv_files = [f for f in anchor_path.glob('*.csv') if not f.name.endswith('_checkpoint.txt')]\n",
    "            if direct_csv_files:\n",
    "                print(f\"\\nFound {len(direct_csv_files)} direct CSV files in {anchor_type}/\")\n",
    "                for csv_file in direct_csv_files:\n",
    "                    try:\n",
    "                        records_loaded = process_csv_file_complete(csv_file, MODEL, 'Unknown', anchor_type, all_data)\n",
    "                        files_processed += 1\n",
    "                        record_counts['Unknown'] += records_loaded\n",
    "                        print(f\"    âœ“ Direct file {csv_file.name}: {records_loaded:,} records loaded\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"    âœ— Error loading direct file {csv_file.name}: {e}\")\n",
    "                        continue\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Critical error during data loading: {e}\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    print(f\"\\n=== LOADING SUMMARY FOR {MODEL} ===\")\n",
    "    print(f\"Total files processed: {files_processed}\")\n",
    "    print(f\"File count by strategy-anchor combination:\")\n",
    "    for combo, count in file_counts.items():\n",
    "        print(f\"  {combo}: {count} files\")\n",
    "    print(f\"\\nRecord count by strategy:\")\n",
    "    total_records = 0\n",
    "    for strategy, count in record_counts.items():\n",
    "        print(f\"  {strategy}: {count:,} records\")\n",
    "        total_records += count\n",
    "    print(f\"  TOTAL: {total_records:,} records\")\n",
    "    \n",
    "    return pd.DataFrame(all_data)\n",
    "\n",
    "def process_csv_file_complete(csv_file, model_name, strategy_label, anchor_type, all_data):\n",
    "    \"\"\"Process a single CSV file and extract ALL processing time data - NO sampling\"\"\"\n",
    "    total_records = 0\n",
    "    \n",
    "    try:\n",
    "        # Read file in chunks to manage memory efficiently\n",
    "        chunk_iter = pd.read_csv(csv_file, chunksize=CHUNK_SIZE, \n",
    "                               on_bad_lines='skip', engine='python')\n",
    "        \n",
    "        for chunk in chunk_iter:\n",
    "            if 'processing_time' not in chunk.columns:\n",
    "                continue\n",
    "            \n",
    "            # Filter valid processing times - keep ALL valid records\n",
    "            valid_chunk = chunk.dropna(subset=['processing_time'])\n",
    "            \n",
    "            if len(valid_chunk) == 0:\n",
    "                continue\n",
    "            \n",
    "            # Extract dataset name\n",
    "            dataset_name = csv_file.stem.split('_pred_')[0] if '_pred_' in csv_file.stem else csv_file.stem\n",
    "            \n",
    "            # Add ALL records to collection (no sampling)\n",
    "            for _, row in valid_chunk.iterrows():\n",
    "                all_data.append({\n",
    "                    'strategy': strategy_label,\n",
    "                    'model': model_name,\n",
    "                    'anchor': anchor_type,\n",
    "                    'dataset': dataset_name,\n",
    "                    'file': csv_file.name,\n",
    "                    'processing_time': float(row['processing_time'])\n",
    "                })\n",
    "            \n",
    "            total_records += len(valid_chunk)\n",
    "        \n",
    "        return total_records\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Failed to process {csv_file.name}: {e}\")\n",
    "\n",
    "# Load complete data for selected model\n",
    "print(f\"Loading ALL processing times for model: {MODEL}\")\n",
    "print(\"Searching in middle/ and penultimate/ directories...\")\n",
    "print(\"NO LIMITS - Complete dataset analysis\")\n",
    "\n",
    "try:\n",
    "    df_times = load_all_processing_times_complete()\n",
    "    \n",
    "    if len(df_times) == 0:\n",
    "        print(f\"ERROR: No data loaded for model {MODEL}. Check if the model directory exists in middle/ or penultimate/\")\n",
    "        print(\"Available directories:\")\n",
    "        for anchor in ['middle', 'penultimate']:\n",
    "            anchor_path = results_path / anchor\n",
    "            if anchor_path.exists():\n",
    "                print(f\"  {anchor}/: {[d.name for d in anchor_path.iterdir() if d.is_dir()]}\")\n",
    "    else:\n",
    "        print(f\"\\nðŸŽ‰ COMPLETE DATA LOADING SUCCESS! ðŸŽ‰\")\n",
    "        print(f\"Loaded {len(df_times):,} processing time records for model {MODEL}\")\n",
    "        print(f\"Strategies found: {sorted(df_times['strategy'].unique())}\")\n",
    "        print(f\"Anchor types found: {sorted(df_times['anchor'].unique())}\")\n",
    "        if 'dataset' in df_times.columns:\n",
    "            print(f\"Datasets found: {len(df_times['dataset'].unique())}\")\n",
    "        print(f\"Memory usage after complete loading: {get_memory_usage():.1f} MB\")\n",
    "        \n",
    "        # Force garbage collection\n",
    "        gc.collect()\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"CRITICAL ERROR: Failed to load data for model {MODEL}: {e}\")\n",
    "    df_times = pd.DataFrame()  # Empty dataframe to prevent further errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data overview with safety checks - optimized for single model\n",
    "if len(df_times) == 0:\n",
    "    print(f\"ERROR: No data available for model {MODEL}. Please check the data loading step.\")\n",
    "else:\n",
    "    print(f\"=== DATA OVERVIEW FOR MODEL {MODEL} ===\")\n",
    "    print(f\"Total records: {len(df_times):,}\")\n",
    "    print(f\"Processing time range: {df_times['processing_time'].min():.2f} - {df_times['processing_time'].max():.2f} seconds\")\n",
    "    print(f\"Mean processing time: {df_times['processing_time'].mean():.2f} seconds\")\n",
    "    print(f\"Median processing time: {df_times['processing_time'].median():.2f} seconds\")\n",
    "\n",
    "    print(\"\\n=== RECORDS PER STRATEGY ===\")\n",
    "    strategy_counts = df_times['strategy'].value_counts()\n",
    "    for strategy, count in strategy_counts.items():\n",
    "        print(f\"{strategy}: {count:,} records\")\n",
    "        \n",
    "    print(\"\\n=== RECORDS PER ANCHOR TYPE ===\")\n",
    "    anchor_counts = df_times['anchor'].value_counts()\n",
    "    for anchor, count in anchor_counts.items():\n",
    "        print(f\"{anchor}: {count:,} records\")\n",
    "\n",
    "    print(\"\\n=== BASIC STATISTICS BY STRATEGY ===\")\n",
    "    strategy_stats = df_times.groupby('strategy')['processing_time'].agg([\n",
    "        'count', 'mean', 'median', 'std', 'min', 'max'\n",
    "    ]).round(3)\n",
    "    print(strategy_stats)\n",
    "    \n",
    "    # Add anchor type comparison\n",
    "    if len(df_times['anchor'].unique()) > 1:\n",
    "        print(\"\\n=== BASIC STATISTICS BY ANCHOR TYPE ===\")\n",
    "        anchor_stats = df_times.groupby('anchor')['processing_time'].agg([\n",
    "            'count', 'mean', 'median', 'std', 'min', 'max'\n",
    "        ]).round(3)\n",
    "        print(anchor_stats)\n",
    "    \n",
    "    print(f\"\\nMemory usage: {get_memory_usage():.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing Time Distribution by Strategy - Optimized for single model\n",
    "if len(df_times) == 0:\n",
    "    print(\"No data available for visualization.\")\n",
    "else:\n",
    "    # Enhanced visualization for single model analysis\n",
    "    fig = plt.figure(figsize=(15, 12))\n",
    "    \n",
    "    try:\n",
    "        # Create a 2x3 grid for comprehensive analysis\n",
    "        gs = fig.add_gridspec(3, 2, hspace=0.3, wspace=0.3)\n",
    "        \n",
    "        # 1. Box plot by strategy\n",
    "        ax1 = fig.add_subplot(gs[0, 0])\n",
    "        sns.boxplot(data=df_times, x='strategy', y='processing_time', ax=ax1)\n",
    "        ax1.set_title(f'Processing Time Distribution by Strategy\\n({MODEL})')\n",
    "        ax1.set_xlabel('Strategy')\n",
    "        ax1.set_ylabel('Processing Time (seconds)')\n",
    "        ax1.tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # 2. Box plot by anchor type (if available)\n",
    "        ax2 = fig.add_subplot(gs[0, 1])\n",
    "        if len(df_times['anchor'].unique()) > 1:\n",
    "            sns.boxplot(data=df_times, x='anchor', y='processing_time', ax=ax2)\n",
    "            ax2.set_title('Processing Time by Anchor Type')\n",
    "            ax2.set_xlabel('Anchor Type')\n",
    "            ax2.set_ylabel('Processing Time (seconds)')\n",
    "        else:\n",
    "            # Show histogram if only one anchor type\n",
    "            df_times['processing_time'].hist(bins=50, ax=ax2)\n",
    "            ax2.set_title('Processing Time Distribution')\n",
    "            ax2.set_xlabel('Processing Time (seconds)')\n",
    "            ax2.set_ylabel('Frequency')\n",
    "\n",
    "        # 3. Mean processing time bar chart\n",
    "        ax3 = fig.add_subplot(gs[1, 0])\n",
    "        strategy_means = df_times.groupby('strategy')['processing_time'].mean().sort_values()\n",
    "        bars = ax3.bar(range(len(strategy_means)), strategy_means.values, \n",
    "                      color=['#ff9999', '#66b3ff', '#99ff99'][:len(strategy_means)])\n",
    "        ax3.set_title('Mean Processing Time by Strategy')\n",
    "        ax3.set_xlabel('Strategy')\n",
    "        ax3.set_ylabel('Mean Processing Time (seconds)')\n",
    "        ax3.set_xticks(range(len(strategy_means)))\n",
    "        ax3.set_xticklabels(strategy_means.index, rotation=45, ha='right')\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for i, bar in enumerate(bars):\n",
    "            height = bar.get_height()\n",
    "            ax3.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                     f'{height:.2f}s', ha='center', va='bottom', fontsize=9)\n",
    "        \n",
    "        # 4. Processing rate (predictions per second)\n",
    "        ax4 = fig.add_subplot(gs[1, 1])\n",
    "        processing_rates = 1 / strategy_means\n",
    "        bars4 = ax4.bar(range(len(processing_rates)), processing_rates.values, \n",
    "                       color=['#ffcc99', '#c2c2f0', '#ccffcc'][:len(processing_rates)])\n",
    "        ax4.set_title('Processing Rate by Strategy')\n",
    "        ax4.set_xlabel('Strategy')\n",
    "        ax4.set_ylabel('Predictions/Second')\n",
    "        ax4.set_xticks(range(len(processing_rates)))\n",
    "        ax4.set_xticklabels(processing_rates.index, rotation=45, ha='right')\n",
    "        \n",
    "        # Add value labels\n",
    "        for i, bar in enumerate(bars4):\n",
    "            height = bar.get_height()\n",
    "            ax4.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                     f'{height:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "        \n",
    "        # 5. Strategy comparison pie chart\n",
    "        ax5 = fig.add_subplot(gs[2, 0])\n",
    "        strategy_counts = df_times['strategy'].value_counts()\n",
    "        colors = plt.cm.Set3(np.linspace(0, 1, len(strategy_counts)))\n",
    "        ax5.pie(strategy_counts.values, labels=strategy_counts.index, autopct='%1.1f%%', \n",
    "               startangle=90, colors=colors)\n",
    "        ax5.set_title('Data Distribution by Strategy')\n",
    "        \n",
    "        # 6. Processing time violin plot\n",
    "        ax6 = fig.add_subplot(gs[2, 1])\n",
    "        sns.violinplot(data=df_times, x='strategy', y='processing_time', ax=ax6)\n",
    "        ax6.set_title('Processing Time Distribution (Violin Plot)')\n",
    "        ax6.set_xlabel('Strategy')\n",
    "        ax6.set_ylabel('Processing Time (seconds)')\n",
    "        ax6.tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        plt.suptitle(f'Processing Time Analysis - Model: {MODEL}', fontsize=16, y=0.98)\n",
    "        plt.show()\n",
    "        \n",
    "        # Force cleanup\n",
    "        plt.close()\n",
    "        gc.collect()\n",
    "        print(f\"Memory usage after visualization: {get_memory_usage():.1f} MB\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error creating visualization: {e}\")\n",
    "        plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed Statistics Table - Complete Dataset Analysis\n",
    "if len(df_times) == 0:\n",
    "    print(\"No data available for statistical analysis.\")\n",
    "else:\n",
    "    print(f\"=== COMPLETE PROCESSING TIME STATISTICS FOR {MODEL} ===\")\n",
    "    print(f\"Total records analyzed: {len(df_times):,}\")\n",
    "\n",
    "    # Detailed statistics by strategy\n",
    "    detailed_stats = df_times.groupby('strategy')['processing_time'].agg([\n",
    "        'count',\n",
    "        'mean',\n",
    "        'median', \n",
    "        'std',\n",
    "        'min',\n",
    "        ('q25', lambda x: x.quantile(0.25)),\n",
    "        ('q75', lambda x: x.quantile(0.75)),\n",
    "        'max',\n",
    "        ('range', lambda x: x.max() - x.min()),\n",
    "        ('cv', lambda x: x.std() / x.mean())  # Coefficient of variation\n",
    "    ]).round(4)\n",
    "\n",
    "    print(\"\\n--- Processing Time Statistics by Strategy ---\")\n",
    "    print(detailed_stats)\n",
    "    \n",
    "    # Add efficiency metrics\n",
    "    print(\"\\n--- Efficiency Metrics ---\")\n",
    "    efficiency_stats = df_times.groupby('strategy')['processing_time'].agg(['mean', 'count']).round(4)\n",
    "    efficiency_stats['predictions_per_second'] = (1 / efficiency_stats['mean']).round(4)\n",
    "    efficiency_stats['records_per_hour'] = (3600 / efficiency_stats['mean']).round(0)\n",
    "    print(efficiency_stats)\n",
    "\n",
    "    # Statistics by anchor type if available\n",
    "    if len(df_times['anchor'].unique()) > 1:\n",
    "        print(f\"\\n--- Processing Time Statistics by Anchor Type ---\")\n",
    "        anchor_stats = df_times.groupby('anchor')['processing_time'].agg([\n",
    "            'count', 'mean', 'median', 'std', 'min', 'max'\n",
    "        ]).round(4)\n",
    "        print(anchor_stats)\n",
    "        \n",
    "        # Cross-tabulation: Strategy Ã— Anchor\n",
    "        print(f\"\\n--- Strategy Ã— Anchor Cross-Analysis ---\")\n",
    "        cross_stats = df_times.groupby(['strategy', 'anchor'])['processing_time'].agg([\n",
    "            'count', 'mean', 'std'\n",
    "        ]).round(4)\n",
    "        print(cross_stats)\n",
    "\n",
    "    # Dataset-level analysis if available\n",
    "    if 'dataset' in df_times.columns and len(df_times['dataset'].unique()) > 1:\n",
    "        print(f\"\\n--- Dataset Performance Analysis ---\")\n",
    "        dataset_stats = df_times.groupby('dataset')['processing_time'].agg([\n",
    "            'count', 'mean', 'median', 'std'\n",
    "        ]).round(4).sort_values('mean', ascending=False)\n",
    "        \n",
    "        print(\"Top 10 slowest datasets:\")\n",
    "        print(dataset_stats.head(10))\n",
    "        \n",
    "        print(\"\\nTop 10 fastest datasets:\")\n",
    "        print(dataset_stats.tail(10))\n",
    "\n",
    "    # Statistical significance tests - only if we have multiple strategies\n",
    "    strategies = df_times['strategy'].unique()\n",
    "    \n",
    "    if len(strategies) > 1:\n",
    "        try:\n",
    "            from scipy import stats\n",
    "\n",
    "            groups = [df_times[df_times['strategy'] == s]['processing_time'].values for s in strategies]\n",
    "\n",
    "            # Perform one-way ANOVA\n",
    "            f_stat, p_value = stats.f_oneway(*groups)\n",
    "\n",
    "            print(f\"\\n--- Statistical Significance Test ---\")\n",
    "            print(f\"One-way ANOVA F-statistic: {f_stat:.4f}\")\n",
    "            print(f\"P-value: {p_value:.2e}\")\n",
    "            print(f\"Significant difference between strategies: {'Yes' if p_value < 0.05 else 'No'}\")\n",
    "\n",
    "            # Effect size (eta-squared)\n",
    "            total_sum_squares = sum([(group - df_times['processing_time'].mean())**2 for group in groups for _ in group])\n",
    "            between_sum_squares = sum([len(group) * (group.mean() - df_times['processing_time'].mean())**2 for group in groups])\n",
    "            eta_squared = between_sum_squares / total_sum_squares\n",
    "            print(f\"Effect size (Î·Â²): {eta_squared:.4f}\")\n",
    "            \n",
    "            if eta_squared < 0.01:\n",
    "                effect_size = \"Small\"\n",
    "            elif eta_squared < 0.06:\n",
    "                effect_size = \"Medium\"  \n",
    "            else:\n",
    "                effect_size = \"Large\"\n",
    "            print(f\"Effect size interpretation: {effect_size}\")\n",
    "\n",
    "            # Pairwise comparisons with Bonferroni correction\n",
    "            if len(strategies) > 2:\n",
    "                print(f\"\\n--- Pairwise Comparisons (Bonferroni corrected) ---\")\n",
    "                from itertools import combinations\n",
    "                \n",
    "                alpha = 0.05\n",
    "                n_comparisons = len(list(combinations(strategies, 2)))\n",
    "                bonferroni_alpha = alpha / n_comparisons\n",
    "                \n",
    "                print(f\"Bonferroni corrected alpha: {bonferroni_alpha:.4f}\")\n",
    "                \n",
    "                for i, strategy1 in enumerate(strategies):\n",
    "                    for j, strategy2 in enumerate(strategies):\n",
    "                        if i < j:\n",
    "                            group1 = df_times[df_times['strategy'] == strategy1]['processing_time']\n",
    "                            group2 = df_times[df_times['strategy'] == strategy2]['processing_time']\n",
    "                            \n",
    "                            t_stat, p_val = stats.ttest_ind(group1, group2)\n",
    "                            is_significant = p_val < bonferroni_alpha\n",
    "                            \n",
    "                            mean_diff = group1.mean() - group2.mean()\n",
    "                            cohen_d = mean_diff / np.sqrt((group1.var() + group2.var()) / 2)\n",
    "                            \n",
    "                            print(f\"{strategy1} vs {strategy2}:\")\n",
    "                            print(f\"  Mean difference: {mean_diff:.4f}s\")\n",
    "                            print(f\"  t-statistic: {t_stat:.4f}\")\n",
    "                            print(f\"  p-value: {p_val:.6f}\")\n",
    "                            print(f\"  Significant: {'Yes' if is_significant else 'No'}\")\n",
    "                            print(f\"  Cohen's d: {cohen_d:.4f}\")\n",
    "                            print()\n",
    "                            \n",
    "        except ImportError:\n",
    "            print(\"scipy not available - skipping statistical tests\")\n",
    "        except Exception as e:\n",
    "            print(f\"Could not perform statistical tests: {e}\")\n",
    "    else:\n",
    "        print(\"Only one strategy found - no statistical comparison possible.\")\n",
    "    \n",
    "    print(f\"\\nMemory usage: {get_memory_usage():.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strategy and Anchor Comparison - Enhanced for single model\n",
    "if len(df_times) == 0:\n",
    "    print(\"No data available for strategy and anchor analysis.\")\n",
    "else:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    try:\n",
    "        # 1. Strategy vs Anchor heatmap (if both dimensions available)\n",
    "        if len(df_times['anchor'].unique()) > 1 and len(df_times['strategy'].unique()) > 1:\n",
    "            strategy_anchor_stats = df_times.groupby(['strategy', 'anchor'])['processing_time'].mean().unstack()\n",
    "            sns.heatmap(strategy_anchor_stats, annot=True, fmt='.2f', cmap='YlOrRd', ax=axes[0,0])\n",
    "            axes[0,0].set_title(f'Mean Processing Time by Strategy and Anchor\\n({MODEL})')\n",
    "            axes[0,0].set_xlabel('Anchor Type')\n",
    "            axes[0,0].set_ylabel('Strategy')\n",
    "        else:\n",
    "            # Show just strategy means if only one anchor type\n",
    "            strategy_means = df_times.groupby('strategy')['processing_time'].mean()\n",
    "            bars = axes[0,0].bar(range(len(strategy_means)), strategy_means.values)\n",
    "            axes[0,0].set_title(f'Mean Processing Time by Strategy\\n({MODEL})')\n",
    "            axes[0,0].set_xlabel('Strategy')\n",
    "            axes[0,0].set_ylabel('Processing Time (seconds)')\n",
    "            axes[0,0].set_xticks(range(len(strategy_means)))\n",
    "            axes[0,0].set_xticklabels(strategy_means.index, rotation=45, ha='right')\n",
    "\n",
    "        # 2. Processing time comparison by strategy and anchor\n",
    "        if len(df_times['anchor'].unique()) > 1:\n",
    "            sns.boxplot(data=df_times, x='strategy', y='processing_time', hue='anchor', ax=axes[0,1])\n",
    "            axes[0,1].set_title('Processing Time by Strategy and Anchor')\n",
    "            axes[0,1].set_xlabel('Strategy')\n",
    "            axes[0,1].set_ylabel('Processing Time (seconds)')\n",
    "            axes[0,1].tick_params(axis='x', rotation=45)\n",
    "            axes[0,1].legend(title='Anchor')\n",
    "        else:\n",
    "            # Show violin plot if only one anchor\n",
    "            sns.violinplot(data=df_times, x='strategy', y='processing_time', ax=axes[0,1])\n",
    "            axes[0,1].set_title('Processing Time Distribution by Strategy')\n",
    "            axes[0,1].set_xlabel('Strategy')\n",
    "            axes[0,1].set_ylabel('Processing Time (seconds)')\n",
    "            axes[0,1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "        # 3. Efficiency comparison\n",
    "        ax3 = axes[1,0]\n",
    "        strategy_stats = df_times.groupby('strategy')['processing_time'].agg(['mean', 'std'])\n",
    "        x_pos = np.arange(len(strategy_stats))\n",
    "        bars = ax3.bar(x_pos, strategy_stats['mean'], yerr=strategy_stats['std'], capsize=5)\n",
    "        ax3.set_title('Processing Time with Standard Deviation')\n",
    "        ax3.set_xlabel('Strategy')\n",
    "        ax3.set_ylabel('Processing Time (seconds)')\n",
    "        ax3.set_xticks(x_pos)\n",
    "        ax3.set_xticklabels(strategy_stats.index, rotation=45, ha='right')\n",
    "        \n",
    "        # Add value labels\n",
    "        for i, bar in enumerate(bars):\n",
    "            height = bar.get_height()\n",
    "            ax3.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                     f'{height:.2f}Â±{strategy_stats.iloc[i][\"std\"]:.2f}', \n",
    "                     ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "        # 4. Dataset analysis (if available)\n",
    "        ax4 = axes[1,1]\n",
    "        if 'dataset' in df_times.columns and len(df_times['dataset'].unique()) > 1:\n",
    "            dataset_means = df_times.groupby('dataset')['processing_time'].mean().sort_values()\n",
    "            top_datasets = dataset_means.tail(10)  # Show top 10 slowest datasets\n",
    "            bars = ax4.bar(range(len(top_datasets)), top_datasets.values)\n",
    "            ax4.set_title('Top 10 Slowest Datasets')\n",
    "            ax4.set_xlabel('Dataset')\n",
    "            ax4.set_ylabel('Mean Processing Time (seconds)')\n",
    "            ax4.set_xticks(range(len(top_datasets)))\n",
    "            ax4.set_xticklabels(top_datasets.index, rotation=45, ha='right')\n",
    "        else:\n",
    "            # Show cumulative distribution\n",
    "            sorted_times = np.sort(df_times['processing_time'])\n",
    "            y_vals = np.arange(len(sorted_times)) / float(len(sorted_times))\n",
    "            ax4.plot(sorted_times, y_vals)\n",
    "            ax4.set_title('Cumulative Distribution of Processing Times')\n",
    "            ax4.set_xlabel('Processing Time (seconds)')\n",
    "            ax4.set_ylabel('Cumulative Probability')\n",
    "            ax4.grid(True, alpha=0.3)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Print detailed comparison if multiple anchor types\n",
    "        if len(df_times['anchor'].unique()) > 1:\n",
    "            print(f\"\\n=== STRATEGY-ANCHOR COMBINATIONS FOR {MODEL} ===\")\n",
    "            combo_stats = df_times.groupby(['strategy', 'anchor'])['processing_time'].agg(['count', 'mean', 'std']).round(3)\n",
    "            print(combo_stats)\n",
    "        \n",
    "        # Force cleanup\n",
    "        plt.close()\n",
    "        gc.collect()\n",
    "        print(f\"\\nMemory usage after analysis: {get_memory_usage():.1f} MB\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error creating strategy-anchor analysis: {e}\")\n",
    "        plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance Summary Report\n",
    "print(\"=== PROCESSING TIME PERFORMANCE SUMMARY ===\")\n",
    "print(\"\\nStrategy Performance Ranking (by mean processing time):\")\n",
    "\n",
    "strategy_ranking = df_times.groupby('strategy')['processing_time'].agg([\n",
    "    'mean', 'median', 'count'\n",
    "]).sort_values('mean')\n",
    "\n",
    "for i, (strategy, stats) in enumerate(strategy_ranking.iterrows(), 1):\n",
    "    print(f\"{i}. {strategy}:\")\n",
    "    print(f\"   Mean: {stats['mean']:.2f}s\")\n",
    "    print(f\"   Median: {stats['median']:.2f}s\")\n",
    "    print(f\"   Records: {stats['count']:,}\")\n",
    "    print()\n",
    "\n",
    "# Efficiency metrics\n",
    "print(\"\\n=== EFFICIENCY ANALYSIS ===\")\n",
    "base_mean = strategy_ranking.loc['Base Version', 'mean'] if 'Base Version' in strategy_ranking.index else None\n",
    "\n",
    "if base_mean:\n",
    "    print(f\"Base Version mean processing time: {base_mean:.2f}s\")\n",
    "    print(\"\\nOverhead compared to Base Version:\")\n",
    "    \n",
    "    for strategy, stats in strategy_ranking.iterrows():\n",
    "        if strategy != 'Base Version':\n",
    "            overhead = ((stats['mean'] - base_mean) / base_mean) * 100\n",
    "            print(f\"{strategy}: +{overhead:.1f}% ({stats['mean'] - base_mean:.2f}s additional)\")\n",
    "\n",
    "# Processing rate (predictions per second)\n",
    "print(\"\\n=== PROCESSING RATE ===\")\n",
    "for strategy, stats in strategy_ranking.iterrows():\n",
    "    rate = 1 / stats['mean']\n",
    "    print(f\"{strategy}: {rate:.3f} predictions/second\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export summary statistics to CSV - Optimized for single model\n",
    "if len(df_times) == 0:\n",
    "    print(\"No data available for export.\")\n",
    "else:\n",
    "    try:\n",
    "        output_path = base_path / 'notebook' / f'processing_time_summary_{MODEL}.csv'\n",
    "\n",
    "        # Prepare detailed summary for single model\n",
    "        summary_data = []\n",
    "\n",
    "        for strategy in df_times['strategy'].unique():\n",
    "            strategy_data = df_times[df_times['strategy'] == strategy]['processing_time']\n",
    "            \n",
    "            base_stats = {\n",
    "                'model': MODEL,\n",
    "                'strategy': strategy,\n",
    "                'count': len(strategy_data),\n",
    "                'mean': strategy_data.mean(),\n",
    "                'median': strategy_data.median(),\n",
    "                'std': strategy_data.std(),\n",
    "                'min': strategy_data.min(),\n",
    "                'max': strategy_data.max(),\n",
    "                'q25': strategy_data.quantile(0.25),\n",
    "                'q75': strategy_data.quantile(0.75),\n",
    "                'predictions_per_second': 1 / strategy_data.mean() if strategy_data.mean() > 0 else 0\n",
    "            }\n",
    "            \n",
    "            # Add anchor-specific statistics if multiple anchors\n",
    "            if len(df_times['anchor'].unique()) > 1:\n",
    "                for anchor in df_times['anchor'].unique():\n",
    "                    anchor_data = df_times[(df_times['strategy'] == strategy) & \n",
    "                                         (df_times['anchor'] == anchor)]['processing_time']\n",
    "                    \n",
    "                    if len(anchor_data) > 0:\n",
    "                        anchor_stats = base_stats.copy()\n",
    "                        anchor_stats.update({\n",
    "                            'anchor': anchor,\n",
    "                            'count': len(anchor_data),\n",
    "                            'mean': anchor_data.mean(),\n",
    "                            'median': anchor_data.median(),\n",
    "                            'std': anchor_data.std(),\n",
    "                            'predictions_per_second': 1 / anchor_data.mean() if anchor_data.mean() > 0 else 0\n",
    "                        })\n",
    "                        summary_data.append(anchor_stats)\n",
    "            else:\n",
    "                base_stats['anchor'] = df_times['anchor'].iloc[0]\n",
    "                summary_data.append(base_stats)\n",
    "\n",
    "        summary_df = pd.DataFrame(summary_data)\n",
    "        summary_df.to_csv(output_path, index=False)\n",
    "\n",
    "        print(f\"Summary statistics exported to: {output_path}\")\n",
    "        print(f\"\\nSummary table for {MODEL}:\")\n",
    "        display_cols = ['strategy', 'anchor', 'count', 'mean', 'median', 'std', 'predictions_per_second']\n",
    "        available_cols = [col for col in display_cols if col in summary_df.columns]\n",
    "        print(summary_df[available_cols].round(3))\n",
    "        \n",
    "        # Export additional dataset-level statistics if available\n",
    "        if 'dataset' in df_times.columns and len(df_times['dataset'].unique()) > 1:\n",
    "            dataset_output_path = base_path / 'notebook' / f'dataset_processing_times_{MODEL}.csv'\n",
    "            dataset_stats = df_times.groupby(['dataset', 'strategy'])['processing_time'].agg([\n",
    "                'count', 'mean', 'median', 'std', 'min', 'max'\n",
    "            ]).round(3).reset_index()\n",
    "            dataset_stats.to_csv(dataset_output_path, index=False)\n",
    "            print(f\"Dataset-level statistics exported to: {dataset_output_path}\")\n",
    "        \n",
    "        # Force cleanup\n",
    "        del summary_data, summary_df\n",
    "        gc.collect()\n",
    "        print(f\"\\nMemory usage: {get_memory_usage():.1f} MB\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error exporting summary: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Summary Report - Optimized for single model\n",
    "if len(df_times) == 0:\n",
    "    print(f\"No data available for final analysis of model {MODEL}.\")\n",
    "else:\n",
    "    print(f\"=== PROCESSING TIME PERFORMANCE SUMMARY FOR {MODEL} ===\")\n",
    "    print(f\"\\nStrategy Performance Ranking (by mean processing time):\")\n",
    "\n",
    "    try:\n",
    "        strategy_ranking = df_times.groupby('strategy')['processing_time'].agg([\n",
    "            'mean', 'median', 'count', 'std'\n",
    "        ]).sort_values('mean')\n",
    "\n",
    "        for i, (strategy, stats) in enumerate(strategy_ranking.iterrows(), 1):\n",
    "            efficiency_ratio = 1 / stats['mean'] if stats['mean'] > 0 else 0\n",
    "            print(f\"{i}. {strategy}:\")\n",
    "            print(f\"   Mean: {stats['mean']:.2f}s (Â±{stats['std']:.2f}s)\")\n",
    "            print(f\"   Median: {stats['median']:.2f}s\")\n",
    "            print(f\"   Records: {stats['count']:,}\")\n",
    "            print(f\"   Efficiency: {efficiency_ratio:.3f} predictions/second\")\n",
    "            print()\n",
    "\n",
    "        # Efficiency analysis\n",
    "        print(f\"\\n=== EFFICIENCY ANALYSIS FOR {MODEL} ===\")\n",
    "        base_mean = strategy_ranking.loc['Base Version', 'mean'] if 'Base Version' in strategy_ranking.index else None\n",
    "\n",
    "        if base_mean:\n",
    "            print(f\"Base Version mean processing time: {base_mean:.2f}s\")\n",
    "            print(f\"Overhead compared to Base Version:\")\n",
    "            \n",
    "            for strategy, stats in strategy_ranking.iterrows():\n",
    "                if strategy != 'Base Version':\n",
    "                    overhead = ((stats['mean'] - base_mean) / base_mean) * 100\n",
    "                    additional_time = stats['mean'] - base_mean\n",
    "                    print(f\"  {strategy}: +{overhead:.1f}% (+{additional_time:.2f}s)\")\n",
    "                    \n",
    "            # Calculate efficiency loss\n",
    "            base_rate = 1 / base_mean\n",
    "            print(f\"\\nThroughput comparison:\")\n",
    "            print(f\"  Base Version: {base_rate:.3f} predictions/second\")\n",
    "            for strategy, stats in strategy_ranking.iterrows():\n",
    "                if strategy != 'Base Version':\n",
    "                    strategy_rate = 1 / stats['mean']\n",
    "                    rate_loss = ((base_rate - strategy_rate) / base_rate) * 100\n",
    "                    print(f\"  {strategy}: {strategy_rate:.3f} predictions/second (-{rate_loss:.1f}%)\")\n",
    "        else:\n",
    "            print(\"Base Version not found - showing absolute performance:\")\n",
    "            for strategy, stats in strategy_ranking.iterrows():\n",
    "                rate = 1 / stats['mean']\n",
    "                print(f\"  {strategy}: {rate:.3f} predictions/second\")\n",
    "\n",
    "        # Anchor type analysis if available\n",
    "        if len(df_times['anchor'].unique()) > 1:\n",
    "            print(f\"\\n=== ANCHOR TYPE COMPARISON FOR {MODEL} ===\")\n",
    "            anchor_ranking = df_times.groupby('anchor')['processing_time'].agg([\n",
    "                'mean', 'median', 'count'\n",
    "            ]).sort_values('mean')\n",
    "            \n",
    "            for anchor, stats in anchor_ranking.iterrows():\n",
    "                rate = 1 / stats['mean']\n",
    "                print(f\"{anchor}: {stats['mean']:.2f}s avg ({rate:.3f} pred/sec, {stats['count']:,} records)\")\n",
    "\n",
    "        # Statistical summary\n",
    "        print(f\"\\n=== STATISTICAL SUMMARY FOR {MODEL} ===\")\n",
    "        overall_stats = df_times['processing_time'].describe()\n",
    "        print(f\"Overall processing time statistics:\")\n",
    "        print(f\"  Count: {overall_stats['count']:,.0f}\")\n",
    "        print(f\"  Mean: {overall_stats['mean']:.2f}s\")\n",
    "        print(f\"  Std: {overall_stats['std']:.2f}s\")\n",
    "        print(f\"  Min: {overall_stats['min']:.2f}s\")\n",
    "        print(f\"  25th percentile: {overall_stats['25%']:.2f}s\")\n",
    "        print(f\"  Median: {overall_stats['50%']:.2f}s\")\n",
    "        print(f\"  75th percentile: {overall_stats['75%']:.2f}s\")\n",
    "        print(f\"  Max: {overall_stats['max']:.2f}s\")\n",
    "\n",
    "        print(f\"\\n=== ANALYSIS COMPLETE FOR {MODEL} ===\")\n",
    "        print(f\"Total processing time records analyzed: {len(df_times):,}\")\n",
    "        print(f\"Strategies analyzed: {', '.join(sorted(df_times['strategy'].unique()))}\")\n",
    "        print(f\"Anchor types analyzed: {', '.join(sorted(df_times['anchor'].unique()))}\")\n",
    "        if 'dataset' in df_times.columns:\n",
    "            print(f\"Datasets covered: {len(df_times['dataset'].unique())}\")\n",
    "        print(f\"Final memory usage: {get_memory_usage():.1f} MB\")\n",
    "        \n",
    "        # Final cleanup\n",
    "        gc.collect()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in final analysis: {e}\")\n",
    "\n",
    "# Memory cleanup at end\n",
    "try:\n",
    "    plt.close('all')  # Close any remaining plots\n",
    "    gc.collect()\n",
    "    print(f\"\\nFinal cleanup complete. Memory usage: {get_memory_usage():.1f} MB\")\n",
    "    print(f\"\\nTo analyze a different model, change the MODEL variable in the first cell and rerun the notebook.\")\n",
    "    print(f\"Available models: qwen2.5_7b, qwen2.5_14b, llama3.1_8b, mixtral_8x7b, deepseek-coder_33b, mistral_7b\")\n",
    "except:\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
