{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3428bdf",
   "metadata": {},
   "source": [
    "# VeronaCard Next‑POI Prediction – Metrics & Exploratory Analysis\n",
    "\n",
    "*Generated automatically on 2025-06-25 13:01 UTC*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b32bbc",
   "metadata": {},
   "source": [
    "\n",
    "**Objectives**\n",
    "\n",
    "1. Load all prediction CSV files produced by `veronacard_mob.py` (`*_pred_*.csv`).\n",
    "2. Compute evaluation metrics  \n",
    "   - Top‑1 Accuracy  \n",
    "   - Top‑5 Hit Rate  \n",
    "   - Mean Reciprocal Rank (MRR)  \n",
    "   - Catalogue Coverage  \n",
    "3. Visualise the metrics globally and per calendar year (2016 → 2020).\n",
    "4. Provide an extendable framework for further error analysis (confusion matrices, cluster quality, temporal patterns).\n",
    "\n",
    "> **Nota bene** – questo notebook non assume directory hard‑coded: basta mettere i CSV nella stessa cartella del notebook o impostare `DATA_DIR` opportunamente.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07dfa4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "import ast\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams['figure.dpi'] = 120\n",
    "plt.rcParams['axes.grid'] = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a744cbc",
   "metadata": {},
   "source": [
    "## 1. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac05b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import glob, ast, os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Percorso ai file CSV\n",
    "csv_files = [Path(p) for p in glob.glob(os.path.join('results', '*_pred_*.csv'))]\n",
    "csv_files = sorted(csv_files)\n",
    "assert csv_files, \"No CSV files matching *_pred_*.csv found!\"\n",
    "\n",
    "dfs = []\n",
    "for fp in csv_files:\n",
    "    df = pd.read_csv(fp)\n",
    "\n",
    "    # Estrai l'anno dal nome file, es. dati_2016_pred_...\n",
    "    year_token = next((part for part in fp.stem.split('_')\n",
    "                       if part.isdigit() and len(part) == 4), None)\n",
    "    df['year'] = int(year_token) if year_token else np.nan\n",
    "\n",
    "    df['prediction_list'] = df['prediction'].apply(\n",
    "        lambda x: ast.literal_eval(x) if isinstance(x, str) else []\n",
    "    )\n",
    "    df = df[df['prediction_list'].apply(len) == 5]\n",
    "    dfs.append(df)\n",
    "\n",
    "df_all = pd.concat(dfs, ignore_index=True)\n",
    "print(f\"Loaded {len(df_all):,} rows from {len(csv_files)} files\")\n",
    "df_all.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8882065",
   "metadata": {},
   "source": [
    "## 2 · Metric definitions\n",
    "\n",
    "> Sia il vero next-POI \\(y_i\\) e sia  \n",
    "> la lista **ordinata** di raccomandazioni  \n",
    "> \\(\\hat{y}_i^{(1)},\\,\\hat{y}_i^{(2)},\\dots,\\hat{y}_i^{(k)}\\).\n",
    "\n",
    "---\n",
    "\n",
    "### • Top-1 Accuracy\n",
    "\n",
    "\\[\n",
    "\\text{Acc}_{@1}=\\frac{1}{N}\\sum_{i=1}^{N}\\mathbf{1}\\!\\bigl\\{\\,y_i=\\hat{y}_i^{(1)}\\bigr\\}\n",
    "\\]\n",
    "\n",
    "---\n",
    "\n",
    "### • Top-\\(k\\) Hit Rate\n",
    "\n",
    "\\[\n",
    "\\text{HR}_{@k}=\\frac{1}{N}\\sum_{i=1}^{N}\n",
    "\\mathbf{1}\\!\\bigl\\{\\,y_i\\in\\{\\hat{y}_i^{(1)},\\dots,\\hat{y}_i^{(k)}\\}\\bigr\\}\n",
    "\\]\n",
    "\n",
    "---\n",
    "\n",
    "### • Mean Reciprocal Rank (MRR)\n",
    "\n",
    "\\[\n",
    "\\text{MRR}=\\frac{1}{N}\\sum_{i=1}^{N}\\frac{1}{\\operatorname{rank}_i},\\qquad\n",
    "\\operatorname{rank}_i=\\min\\{\\,r\\mid y_i=\\hat{y}_i^{(r)}\\}\n",
    "\\]\n",
    "\n",
    "---\n",
    "\n",
    "### • Catalogue Coverage\n",
    "\n",
    "\\[\n",
    "\\text{Coverage}=\\frac{\\bigl|\\bigcup_{i}\\{\\hat{y}_i^{(1)},\\dots,\\hat{y}_i^{(k)}\\}\\bigr|}\n",
    "                     {|\\mathcal{P}|}\n",
    "\\]\n",
    "\n",
    "dove \\(\\mathcal{P}\\) è l’insieme completo dei Point of Interest presenti nel *ground-truth*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04234505",
   "metadata": {},
   "source": [
    "### 2.1 Helper functions & per‑row computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc35347",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def poi_id(x):\n",
    "    \"\"\"\n",
    "    Converte 'x' in un identificatore hashable comparabile con ground-truth.\n",
    "    • Se x è un dict          → prova a usare 'poi', 'poi_id', 'name', 'id'\n",
    "    • Se x è list/tuple       → ritorna tupla ricorsiva\n",
    "    • Altrimenti (str/int)    → cast a str\n",
    "    \"\"\"\n",
    "    if isinstance(x, dict):\n",
    "        for key in ('poi', 'poi_id', 'name', 'id'):\n",
    "            if key in x:\n",
    "                return str(x[key])\n",
    "        # fallback: serializza in JSON ordinato\n",
    "        return json.dumps(x, sort_keys=True)\n",
    "    elif isinstance(x, (list, tuple)):\n",
    "        return tuple(map(poi_id, x))\n",
    "    else:\n",
    "        return str(x)\n",
    "\n",
    "# Normalizza prediction_list e ground_truth\n",
    "df_all['prediction_norm']    = df_all['prediction_list'].apply(lambda lst: [poi_id(e) for e in lst])\n",
    "df_all['ground_truth_norm']  = df_all['ground_truth'].apply(poi_id)\n",
    "\n",
    "# -- metriche element-wise --------------------------------------------\n",
    "df_all['hit@1'] = df_all['prediction_norm'].str[0] == df_all['ground_truth_norm']\n",
    "\n",
    "def top_k_hit(row, k=5):\n",
    "    return row['ground_truth_norm'] in row['prediction_norm'][:k]\n",
    "\n",
    "def reciprocal_rank(row, k=5):\n",
    "    try:\n",
    "        rank = row['prediction_norm'][:k].index(row['ground_truth_norm']) + 1\n",
    "        return 1.0 / rank\n",
    "    except ValueError:\n",
    "        return 0.0\n",
    "\n",
    "df_all['hit@5'] = df_all.apply(top_k_hit, axis=1)\n",
    "df_all['rr']    = df_all.apply(reciprocal_rank, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4b31a1",
   "metadata": {},
   "source": [
    "### 2.2 Global metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87936f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_global = {\n",
    "    \"Top-1 Accuracy\": df_all[\"hit@1\"].mean(),\n",
    "    \"Top-5 Hit Rate\": df_all[\"hit@5\"].mean(),\n",
    "    \"MRR\": df_all[\"rr\"].mean(),\n",
    "}\n",
    "\n",
    "# Catalogue Coverage\n",
    "coverage_set = {poi for preds in df_all[\"prediction_norm\"] for poi in preds}\n",
    "metrics_global[\"Catalogue Coverage\"] = (\n",
    "    len(coverage_set) / df_all[\"ground_truth_norm\"].nunique()\n",
    ")\n",
    "\n",
    "# Visualizza in percentuale\n",
    "metrics_df = (\n",
    "    pd.DataFrame(metrics_global, index=[\"Value\"])\n",
    "      .T\n",
    "      .style.format(\"{:.2%}\")\n",
    ")\n",
    "display(metrics_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1f7e3c",
   "metadata": {},
   "source": [
    "### 2.3 Metrics by year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b12f0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "by_year = (\n",
    "    df_all\n",
    "    .groupby('year')\n",
    "    .agg(\n",
    "        top1=('hit@1', 'mean'),\n",
    "        hit5=('hit@5', 'mean'),\n",
    "        mrr=('rr', 'mean'),\n",
    "        n=('card_id', 'size')\n",
    "    )\n",
    "    .reset_index()\n",
    "    .sort_values('year')\n",
    ")\n",
    "display(by_year)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ac7adc",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3f4d2744",
   "metadata": {},
   "source": [
    "## 3. Visualisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cbabe05",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(figsize=(6,4))\n",
    "ax.bar(by_year['year'].astype(str), by_year['top1']*100)\n",
    "ax.set_ylabel('Top‑1 Accuracy (%)')\n",
    "ax.set_title('Top‑1 Accuracy (%) by Year')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58242dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(figsize=(6,4))\n",
    "ax.bar(by_year['year'].astype(str), by_year['hit5']*100)\n",
    "ax.set_ylabel('Top‑5 Hit Rate (%)')\n",
    "ax.set_title('Top‑5 Hit Rate (%) by Year')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12cf7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(figsize=(6,4))\n",
    "ax.bar(by_year['year'].astype(str), by_year['mrr']*100)\n",
    "ax.set_ylabel('MRR (%)')\n",
    "ax.set_title('MRR (%) by Year')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38d992e",
   "metadata": {},
   "source": [
    "### 3.1 Coverage breakdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dab3c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_pred_poi = (\n",
    "    pd.Series([poi for preds in df_all[\"prediction_norm\"] for poi in preds])\n",
    "      .nunique()\n",
    ")\n",
    "unique_gt_poi = df_all[\"ground_truth_norm\"].nunique()\n",
    "\n",
    "print(f\"POI distinct in predictions: {unique_pred_poi}\")\n",
    "print(f\"POI distinct in ground-truth: {unique_gt_poi}\")\n",
    "print(f\"Catalogue Coverage: {unique_pred_poi/unique_gt_poi:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2aee9a",
   "metadata": {},
   "source": [
    "## 4. Error analysis – overview\n",
    "\n",
    "In questa sezione valutiamo **dove** e **perché** il modello sbaglia, seguendo tre filoni:\n",
    "\n",
    "1. **Worst-performing POI pairs**  \n",
    "   Scopriamo quali coppie `ground-truth → first_pred` generano il maggior numero di errori.\n",
    "\n",
    "2. **Confusion matrix per subset**  \n",
    "   Heat-map delle frequenze (o tasso d’errore) su un sotto-insieme di POI di interesse\n",
    "   (ad es. i 20 più visitati o solo l’anno 2020).\n",
    "\n",
    "3. **Explainability (SHAP / LIME)**  \n",
    "   Analisi dei fattori testuali nella colonna `history` che portano a una predizione\n",
    "   sbagliata. Qui mostriamo un prototipo basato su LIME; lo script è modulare\n",
    "   e può essere sostituito da SHAP se usi modelli compatibili.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f8e9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------\n",
    "# 4.1 Worst-performing POI pairs (Top-N) – FIX\n",
    "# ---------------------------------------------------------------------\n",
    "# filtro righe con errore Top-1\n",
    "ERR = df_all.assign(first_pred=df_all[\"prediction_norm\"].str[0])\n",
    "ERR = ERR[~ERR[\"hit@1\"]]           # equivalente a query(\"`hit@1` == False\")\n",
    "\n",
    "# conteggia coppie ground-truth → first_pred\n",
    "pairs = (\n",
    "    ERR.groupby([\"ground_truth_norm\", \"first_pred\"])\n",
    "        .size()\n",
    "        .reset_index(name=\"errors\")\n",
    "        .sort_values(\"errors\", ascending=False)\n",
    "        .head(15)          # Top-15 coppie\n",
    ")\n",
    "\n",
    "display(pairs)\n",
    "\n",
    "# barplot (opzionale)\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "ax.barh(\n",
    "    pairs.apply(lambda r: f\"{r['ground_truth_norm']} → {r['first_pred']}\", axis=1),\n",
    "    pairs[\"errors\"]\n",
    ")\n",
    "ax.set_xlabel(\"Errori (count)\")\n",
    "ax.set_title(\"Worst-performing POI pairs – Top 15\")\n",
    "ax.invert_yaxis()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a501fa2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------\n",
    "# 4.2 Confusion matrix – seleziona subset\n",
    "# ---------------------------------------------------------------------\n",
    "TOP_K = 20          # limitiamoci ai 20 POI più frequenti (ground-truth)\n",
    "YEAR  = None        # imposta un anno (es. 2020) oppure None per all\n",
    "\n",
    "subset = df_all.copy()\n",
    "if YEAR is not None:\n",
    "    subset = subset.query(\"year == @YEAR\")\n",
    "\n",
    "# filtra i POI più frequenti\n",
    "top_poi = (\n",
    "    subset[\"ground_truth_norm\"]\n",
    "           .value_counts()\n",
    "           .head(TOP_K)\n",
    "           .index\n",
    ")\n",
    "mask = subset[\"ground_truth_norm\"].isin(top_poi) & subset[\"prediction_norm\"].str[0].isin(top_poi)\n",
    "cm_df = pd.crosstab(\n",
    "    subset.loc[mask, \"ground_truth_norm\"],\n",
    "    subset.loc[mask, \"prediction_norm\"].str[0],\n",
    "    rownames=[\"True\"],\n",
    "    colnames=[\"Pred\"],\n",
    "    dropna=False\n",
    ")\n",
    "\n",
    "# Normalizza su righe per avere tassi d’errore\n",
    "cm_norm = cm_df.div(cm_df.sum(axis=1), axis=0)\n",
    "\n",
    "# Heat-map (matplotlib only)\n",
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "im = ax.imshow(cm_norm.values, aspect=\"auto\")\n",
    "ax.set_xticks(range(len(cm_norm.columns)))\n",
    "ax.set_xticklabels(cm_norm.columns, rotation=90)\n",
    "ax.set_yticks(range(len(cm_norm.index)))\n",
    "ax.set_yticklabels(cm_norm.index)\n",
    "ax.set_xlabel(\"Predicted POI\")\n",
    "ax.set_ylabel(\"True POI\")\n",
    "ax.set_title(f\"Confusion matrix – top {TOP_K} POI\" + (f\", year {YEAR}\" if YEAR else \"\"))\n",
    "fig.colorbar(im, ax=ax, fraction=0.02, pad=0.04, label=\"Error rate\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0439af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------\n",
    "# 4.3 Explainability – LIME Text (robust version)\n",
    "# ---------------------------------------------------------------------\n",
    "import warnings, json, numpy as np, pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# ╭───────────────────────────────────────────────────────────────────╮\n",
    "# | 1) Import & graceful-degradation                                 |\n",
    "# ╰───────────────────────────────────────────────────────────────────╯\n",
    "try:\n",
    "    from lime.lime_text import LimeTextExplainer\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "except ModuleNotFoundError:\n",
    "    warnings.warn(\n",
    "        \"🛑  Il pacchetto 'lime' o 'scikit-learn' non è installato.\\n\"\n",
    "        \"    Esegui `!pip install lime scikit-learn` e riesegui la cella.\"\n",
    "    )\n",
    "    raise  # interrompe l’esecuzione solo di questa cella\n",
    "\n",
    "# ╭───────────────────────────────────────────────────────────────────╮\n",
    "# | 2) Prepara training set (history → first_pred)                    |\n",
    "# ╰───────────────────────────────────────────────────────────────────╯\n",
    "X_text = df_all[\"history\"].astype(str).values\n",
    "y_label = np.array([str(lbl) for lbl in df_all[\"prediction_norm\"].str[0].values])  # cast a str\n",
    "\n",
    "SAMPLE = 5_000\n",
    "rng = np.random.default_rng(42)\n",
    "idx = rng.choice(len(X_text), size=min(SAMPLE, len(X_text)), replace=False)\n",
    "\n",
    "# TF-IDF senza stop-word italiane (sklearn ≥1.2 non le include)\n",
    "vec = TfidfVectorizer(max_features=10_000, ngram_range=(1, 2), stop_words=None)\n",
    "X_tf = vec.fit_transform(X_text[idx])\n",
    "\n",
    "clf = LogisticRegression(max_iter=1_000, n_jobs=1).fit(X_tf, y_label[idx])\n",
    "print(f\"Training accuracy (proxy) on sampled set: {clf.score(X_tf, y_label[idx]):.2%}\")\n",
    "\n",
    "# ╭───────────────────────────────────────────────────────────────────╮\n",
    "# | 3) Instanzia LIME ed estrai un caso d’errore                      |\n",
    "# ╰───────────────────────────────────────────────────────────────────╯\n",
    "explainer = LimeTextExplainer(class_names=np.unique(y_label).tolist())\n",
    "\n",
    "err_rows = df_all[~df_all[\"hit@1\"]]\n",
    "if err_rows.empty:\n",
    "    print(\"🎉  Nessun errore Top-1 da spiegare!\")\n",
    "else:\n",
    "    err_idx = err_rows.index[0]           # primo errore disponibile\n",
    "    sample_text = df_all.at[err_idx, \"history\"]\n",
    "    pred_poi     = df_all.at[err_idx, \"prediction_norm\"][0]\n",
    "    true_poi     = df_all.at[err_idx, \"ground_truth_norm\"]\n",
    "\n",
    "    explanation = explainer.explain_instance(\n",
    "        sample_text,\n",
    "        lambda txt: clf.predict_proba(vec.transform(txt)),\n",
    "        num_features=10,\n",
    "        labels=[clf.classes_.tolist().index(pred_poi)],\n",
    "    )\n",
    "\n",
    "    # ╭───────────────────────────────────────────────────────────────╮\n",
    "    # | 4) Rendering con fallback testuale                            |\n",
    "    # ╰───────────────────────────────────────────────────────────────╯\n",
    "    print(\"History  :\", sample_text[:300].replace(\"\\n\", \" \") + (\" …\" if len(sample_text) > 300 else \"\"))\n",
    "    print(\"True POI :\", true_poi)\n",
    "    print(\"Pred POI :\", pred_poi)\n",
    "    print(\"─\"*60)\n",
    "\n",
    "    try:\n",
    "        from IPython.display import display, HTML\n",
    "        display(HTML(explanation.as_html()))\n",
    "    except Exception as e:  # IPython mancante o frontend non grafico\n",
    "        warnings.warn(f\"Rendering HTML fallito: {e}\\nStampa fallback testuale.\")\n",
    "        for feat, weight in explanation.as_list():\n",
    "            print(f\"{feat:>25s} | {weight:+.3f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
