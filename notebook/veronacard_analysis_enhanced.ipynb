{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3428bdf",
   "metadata": {},
   "source": [
    "# VeronaCard Next‑POI Prediction – Metrics & Exploratory Analysis\n",
    "\n",
    "*Generated automatically on 2025-06-25 13:01 UTC*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b32bbc",
   "metadata": {},
   "source": [
    "# 🎯 Obiettivi dell'Analisi\n",
    "\n",
    "Questo notebook fornisce una **valutazione completa** delle performance del modello di predizione next-POI implementato in `veronacard_mob.py`, attraverso un framework di analisi strutturato e riproducibile.\n",
    "\n",
    "## 📋 Pipeline di Analisi\n",
    "\n",
    "### 🔄 **1. Data Loading & Preprocessing**\n",
    "- **Caricamento automatico** di tutti i file CSV di predizione (`*_pred_*.csv`)\n",
    "- **Parsing robusto** con gestione errori per file malformati\n",
    "- **Estrazione automatica** dell'anno dai nomi file (es. `dati_2016_pred_model.csv` → 2016)\n",
    "- **Validazione** delle predizioni (filtro per liste con esattamente 5 elementi)\n",
    "\n",
    "### 📊 **2. Evaluation Metrics**\n",
    "Calcolo di **quattro metriche standard** per sistemi di raccomandazione:\n",
    "- **🎯 Top-1 Accuracy**: Precisione della predizione principale\n",
    "- **📈 Top-5 Hit Rate**: Presenza del POI corretto nella top-5\n",
    "- **⚡ Mean Reciprocal Rank (MRR)**: Qualità del ranking delle predizioni\n",
    "- **🗂️ Catalogue Coverage**: Diversità delle raccomandazioni\n",
    "\n",
    "### 📈 **3. Multi-level Visualization**\n",
    "- **📊 Global metrics**: Performance aggregate su tutto il dataset\n",
    "- **📅 Temporal analysis**: Trend evolutivo per anno (2016→2020)\n",
    "- **🎨 Interactive charts**: Grafici comparativi e heat-maps\n",
    "\n",
    "### 🔍 **4. Advanced Error Analysis Framework**\n",
    "Sistema modulare ed estendibile per analisi approfondita:\n",
    "- **🚨 Worst-case analysis**: Identificazione delle coppie POI più problematiche\n",
    "- **🔥 Confusion matrices**: Visualizzazione degli errori sui POI più frequenti\n",
    "- **🧠 Explainability**: Analisi dei pattern testuali che causano errori (LIME/SHAP-ready)\n",
    "- **📊 Temporal patterns**: Analisi stagionale e trend temporali degli errori\n",
    "\n",
    "## 🏗️ Caratteristiche Architetturali\n",
    "\n",
    "### **✅ Robustezza**\n",
    "- **Error handling** completo per file CSV corrotti\n",
    "- **Graceful degradation** per dipendenze mancanti (LIME, etc.)\n",
    "- **Validation** automatica dei dati di input\n",
    "\n",
    "### **🔧 Flessibilità**\n",
    "- **Path-agnostic**: Funziona con CSV nella cartella corrente o tramite `DATA_DIR`\n",
    "- **Modular design**: Sezioni indipendenti eseguibili singolarmente\n",
    "- **Extensible framework**: Facile aggiungere nuove metriche o analisi\n",
    "\n",
    "### **📊 Riproducibilità**\n",
    "- **Timestamp automatico** nel titolo\n",
    "- **Seed fisso** per operazioni random\n",
    "- **Versioning implicito** tramite nomi file con data\n",
    "\n",
    "---\n",
    "\n",
    "## 🚀 Quick Start\n",
    "\n",
    "```python\n",
    "# Esecuzione standard - CSV nella cartella 'results/'\n",
    "# Nessuna configurazione richiesta\n",
    "\n",
    "# Esecuzione personalizzata\n",
    "DATA_DIR = \"/path/to/your/csv/files\"  # opzionale\n",
    "```\n",
    "\n",
    "## 📁 Struttura File Attesa\n",
    "\n",
    "```\n",
    "project_root/\n",
    "├── notebook.ipynb           # questo notebook\n",
    "├── results/                 # directory predefinita\n",
    "│   ├── dati_2016_pred_model1.csv\n",
    "│   ├── dati_2017_pred_model1.csv\n",
    "│   └── ...\n",
    "└── veronacard_mob.py       # script di generazione predizioni\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "> **💡 Pro Tip**: Il notebook è progettato per **fail-safe operation** — anche se alcuni file sono corrotti o alcune librerie mancano, l'analisi procede con le parti disponibili e fornisce sempre risultati utili."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07dfa4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "import ast\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams['figure.dpi'] = 120\n",
    "plt.rcParams['axes.grid'] = True\n",
    "\n",
    "#DATA_DIR = \"../results_mixtral_8x7b_with_geom/\" \n",
    "DATA_DIR = \"../results_parziali_sequenziali_ministral/\" \n",
    "#DATA_DIR = \"../result_run_completa_with_geom_srv_univr/\" #llama 3.1:8b\n",
    "#DATA_DIR = \"../results\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a744cbc",
   "metadata": {},
   "source": [
    "## 1. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7ac05b72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Parser error in ../results_mixtral_8x7b_with_geom/dati_2015_pred_20250823_170255.csv, trying with error handling...\n",
      "Loaded 176,573 rows from 7 files\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>card_id</th>\n",
       "      <th>cluster</th>\n",
       "      <th>history</th>\n",
       "      <th>current_poi</th>\n",
       "      <th>prediction</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>reason</th>\n",
       "      <th>hit</th>\n",
       "      <th>processing_time</th>\n",
       "      <th>status</th>\n",
       "      <th>year</th>\n",
       "      <th>prediction_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>040002523F3885</td>\n",
       "      <td>3</td>\n",
       "      <td>['San Fermo', 'Santa Anastasia', 'Teatro Roman...</td>\n",
       "      <td>Arena</td>\n",
       "      <td>['Verona Tour', 'Museo Lapidario', 'Sighseeing...</td>\n",
       "      <td>Castelvecchio</td>\n",
       "      <td>These POIs are the closest to the current loca...</td>\n",
       "      <td>True</td>\n",
       "      <td>38.512314</td>\n",
       "      <td>success</td>\n",
       "      <td>2014</td>\n",
       "      <td>[Verona Tour, Museo Lapidario, Sighseeing, Cas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>04000E523F3885</td>\n",
       "      <td>3</td>\n",
       "      <td>['San Fermo', 'Casa Giulietta', 'Santa Anastas...</td>\n",
       "      <td>Palazzo della Ragione</td>\n",
       "      <td>['Duomo', 'Teatro Romano', 'Arena', 'AMO', 'Mu...</td>\n",
       "      <td>Castelvecchio</td>\n",
       "      <td>These suggestions are based on distance and to...</td>\n",
       "      <td>False</td>\n",
       "      <td>39.588162</td>\n",
       "      <td>success</td>\n",
       "      <td>2014</td>\n",
       "      <td>[Duomo, Teatro Romano, Arena, AMO, Museo Conte]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>040005523F3885</td>\n",
       "      <td>3</td>\n",
       "      <td>['Casa Giulietta', 'Arena', 'Castelvecchio', '...</td>\n",
       "      <td>Duomo</td>\n",
       "      <td>['AMO', 'Teatro Romano', 'Palazzo della Ragion...</td>\n",
       "      <td>Santa Anastasia</td>\n",
       "      <td>These suggestions are based on proximity to th...</td>\n",
       "      <td>False</td>\n",
       "      <td>41.261395</td>\n",
       "      <td>success</td>\n",
       "      <td>2014</td>\n",
       "      <td>[AMO, Teatro Romano, Palazzo della Ragione, To...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>040071523F3881</td>\n",
       "      <td>3</td>\n",
       "      <td>['San Fermo', 'Casa Giulietta', 'Arena', 'Pala...</td>\n",
       "      <td>Teatro Romano</td>\n",
       "      <td>['Santa Anastasia', 'Duomo', 'AMO', 'Museo Con...</td>\n",
       "      <td>Giardino Giusti</td>\n",
       "      <td>These suggestions are based on the proximity o...</td>\n",
       "      <td>True</td>\n",
       "      <td>1.917337</td>\n",
       "      <td>success</td>\n",
       "      <td>2014</td>\n",
       "      <td>[Santa Anastasia, Duomo, AMO, Museo Conte, Gia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>04005A523F3881</td>\n",
       "      <td>5</td>\n",
       "      <td>['San Fermo', 'Casa Giulietta', 'Torre Lambert...</td>\n",
       "      <td>Arena</td>\n",
       "      <td>['Verona Tour', 'Castelvecchio', 'Sighseeing',...</td>\n",
       "      <td>Castelvecchio</td>\n",
       "      <td>These POIs are the closest to the current loca...</td>\n",
       "      <td>True</td>\n",
       "      <td>42.214125</td>\n",
       "      <td>success</td>\n",
       "      <td>2014</td>\n",
       "      <td>[Verona Tour, Castelvecchio, Sighseeing, Centr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          card_id  cluster                                            history  \\\n",
       "0  040002523F3885        3  ['San Fermo', 'Santa Anastasia', 'Teatro Roman...   \n",
       "1  04000E523F3885        3  ['San Fermo', 'Casa Giulietta', 'Santa Anastas...   \n",
       "2  040005523F3885        3  ['Casa Giulietta', 'Arena', 'Castelvecchio', '...   \n",
       "3  040071523F3881        3  ['San Fermo', 'Casa Giulietta', 'Arena', 'Pala...   \n",
       "4  04005A523F3881        5  ['San Fermo', 'Casa Giulietta', 'Torre Lambert...   \n",
       "\n",
       "             current_poi                                         prediction  \\\n",
       "0                  Arena  ['Verona Tour', 'Museo Lapidario', 'Sighseeing...   \n",
       "1  Palazzo della Ragione  ['Duomo', 'Teatro Romano', 'Arena', 'AMO', 'Mu...   \n",
       "2                  Duomo  ['AMO', 'Teatro Romano', 'Palazzo della Ragion...   \n",
       "3          Teatro Romano  ['Santa Anastasia', 'Duomo', 'AMO', 'Museo Con...   \n",
       "4                  Arena  ['Verona Tour', 'Castelvecchio', 'Sighseeing',...   \n",
       "\n",
       "      ground_truth                                             reason    hit  \\\n",
       "0    Castelvecchio  These POIs are the closest to the current loca...   True   \n",
       "1    Castelvecchio  These suggestions are based on distance and to...  False   \n",
       "2  Santa Anastasia  These suggestions are based on proximity to th...  False   \n",
       "3  Giardino Giusti  These suggestions are based on the proximity o...   True   \n",
       "4    Castelvecchio  These POIs are the closest to the current loca...   True   \n",
       "\n",
       "   processing_time   status  year  \\\n",
       "0        38.512314  success  2014   \n",
       "1        39.588162  success  2014   \n",
       "2        41.261395  success  2014   \n",
       "3         1.917337  success  2014   \n",
       "4        42.214125  success  2014   \n",
       "\n",
       "                                     prediction_list  \n",
       "0  [Verona Tour, Museo Lapidario, Sighseeing, Cas...  \n",
       "1    [Duomo, Teatro Romano, Arena, AMO, Museo Conte]  \n",
       "2  [AMO, Teatro Romano, Palazzo della Ragione, To...  \n",
       "3  [Santa Anastasia, Duomo, AMO, Museo Conte, Gia...  \n",
       "4  [Verona Tour, Castelvecchio, Sighseeing, Centr...  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import glob, ast, os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Percorso ai file CSV\n",
    "csv_files = [Path(p) for p in glob.glob(os.path.join(DATA_DIR, '*_pred_*.csv'))]\n",
    "csv_files = sorted(csv_files)\n",
    "assert csv_files, \"No CSV files matching *_pred_*.csv found!\"\n",
    "\n",
    "dfs = []\n",
    "for fp in csv_files:\n",
    "    try:\n",
    "        df = pd.read_csv(fp)\n",
    "    except pd.errors.ParserError:\n",
    "        print(f\"Warning: Parser error in {fp}, trying with error handling...\")\n",
    "        df = pd.read_csv(fp, on_bad_lines='skip', engine='python')\n",
    "\n",
    "    # Estrai l'anno dal nome file, es. dati_2016_pred_...\n",
    "    year_token = next((part for part in fp.stem.split('_')\n",
    "                       if part.isdigit() and len(part) == 4), None)\n",
    "    df['year'] = int(year_token) if year_token else np.nan\n",
    "\n",
    "    df['prediction_list'] = df['prediction'].apply(\n",
    "        lambda x: ast.literal_eval(x) if isinstance(x, str) else []\n",
    "    )\n",
    "    df = df[df['prediction_list'].apply(len) == 5]\n",
    "    dfs.append(df)\n",
    "\n",
    "df_all = pd.concat(dfs, ignore_index=True)\n",
    "print(f\"Loaded {len(df_all):,} rows from {len(csv_files)} files\")\n",
    "df_all.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8882065",
   "metadata": {},
   "source": [
    "# 2. Definizione delle Metriche di Valutazione\n",
    "\n",
    "Per valutare le performance del modello di predizione next-POI, utilizziamo quattro metriche standard nel campo dei sistemi di raccomandazione e della predizione sequenziale.\n",
    "\n",
    "## Notazione\n",
    "\n",
    "Sia $y_i$ il vero next-POI (ground truth) per la sequenza $i$-esima, e sia $\\hat{y}_i^{(1)}, \\hat{y}_i^{(2)}, \\ldots, \\hat{y}_i^{(k)}$ la lista **ordinata** delle $k$ raccomandazioni prodotte dal modello per quella sequenza, dove $\\hat{y}_i^{(1)}$ è la predizione con confidence più alta.\n",
    "\n",
    "---\n",
    "\n",
    "## 📊 Top-1 Accuracy\n",
    "\n",
    "Misura la **precisione della predizione principale** del modello.\n",
    "\n",
    "$$\\text{Acc}_{@1} = \\frac{1}{N}\\sum_{i=1}^{N}\\mathbf{1}\\!\\left\\{y_i = \\hat{y}_i^{(1)}\\right\\}$$\n",
    "\n",
    "**Interpretazione**: Percentuale di casi in cui la prima predizione del modello coincide esattamente con il POI realmente visitato.\n",
    "\n",
    "**Range**: [0, 1], dove 1 = predizione perfetta\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 Top-k Hit Rate\n",
    "\n",
    "Misura se il POI corretto è presente **tra le prime $k$ predizioni**.\n",
    "\n",
    "$$\\text{HR}_{@k} = \\frac{1}{N}\\sum_{i=1}^{N}\\mathbf{1}\\!\\left\\{y_i \\in \\{\\hat{y}_i^{(1)}, \\hat{y}_i^{(2)}, \\ldots, \\hat{y}_i^{(k)}\\}\\right\\}$$\n",
    "\n",
    "**Interpretazione**: Percentuale di casi in cui il POI corretto è presente nella lista delle prime $k$ raccomandazioni. Per $k=5$: \"Il modello include il POI corretto nella sua top-5?\"\n",
    "\n",
    "**Range**: [0, 1], sempre $\\text{HR}_{@k} \\geq \\text{Acc}_{@1}$\n",
    "\n",
    "---\n",
    "\n",
    "## ⚡ Mean Reciprocal Rank (MRR)\n",
    "\n",
    "Considera sia la **presenza** del POI corretto nella lista che la sua **posizione**.\n",
    "\n",
    "$$\\text{MRR} = \\frac{1}{N}\\sum_{i=1}^{N}\\frac{1}{\\text{rank}_i}$$\n",
    "\n",
    "dove $\\text{rank}_i = \\min\\{r \\mid y_i = \\hat{y}_i^{(r)}\\}$ è la posizione del POI corretto nella lista ordinata.\n",
    "\n",
    "**Interpretazione**: \n",
    "- Se il POI corretto è al 1° posto → contributo = 1.0\n",
    "- Se il POI corretto è al 2° posto → contributo = 0.5  \n",
    "- Se il POI corretto è al 5° posto → contributo = 0.2\n",
    "- Se il POI corretto non è nella top-k → contributo = 0.0\n",
    "\n",
    "**Range**: [0, 1], dove valori più alti indicano che i POI corretti appaiono nelle prime posizioni\n",
    "\n",
    "---\n",
    "\n",
    "## 🗂️ Catalogue Coverage  \n",
    "\n",
    "Misura la **diversità** delle raccomandazioni prodotte dal modello.\n",
    "\n",
    "$$\\text{Coverage} = \\frac{\\left|\\bigcup_{i=1}^{N}\\{\\hat{y}_i^{(1)}, \\hat{y}_i^{(2)}, \\ldots, \\hat{y}_i^{(k)}\\}\\right|}{|\\mathcal{P}|}$$\n",
    "\n",
    "dove $\\mathcal{P}$ è l'insieme completo dei POI presenti nel dataset (ground truth).\n",
    "\n",
    "**Interpretazione**: Frazione dei POI disponibili che il modello è in grado di raccomandare. \n",
    "- Coverage = 1.0: il modello raccomanda tutti i POI del catalogo\n",
    "- Coverage = 0.1: il modello raccomanda solo il 10% dei POI disponibili\n",
    "\n",
    "**Importanza**: Previene il bias verso POI molto popolari e garantisce diversità nelle raccomandazioni.\n",
    "\n",
    "---\n",
    "\n",
    "## 💡 Note Metodologiche\n",
    "\n",
    "- **$N$**: Numero totale di predizioni nel dataset di test\n",
    "- **$k = 5$**: Utilizziamo consistently una finestra di 5 raccomandazioni\n",
    "- **Ordinamento**: Le predizioni sono ordinate per confidence/probabilità decrescente\n",
    "- **Handling missing**: Se $y_i \\notin \\{\\hat{y}_i^{(1)}, \\ldots, \\hat{y}_i^{(k)}\\}$, contributo = 0 per tutte le metriche\n",
    "\n",
    "Queste metriche forniscono una **valutazione completa** del modello: precision (Acc@1), recall (HR@k), ranking quality (MRR), e diversity (Coverage)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04234505",
   "metadata": {},
   "source": [
    "### 2.1 Helper functions & per‑row computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0bc35347",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def poi_id(x):\n",
    "    \"\"\"\n",
    "    Converte 'x' in un identificatore hashable comparabile con ground-truth.\n",
    "    • Se x è un dict          → prova a usare 'poi', 'poi_id', 'name', 'id'\n",
    "    • Se x è list/tuple       → ritorna tupla ricorsiva\n",
    "    • Altrimenti (str/int)    → cast a str\n",
    "    \"\"\"\n",
    "    if isinstance(x, dict):\n",
    "        for key in ('poi', 'poi_id', 'name', 'id'):\n",
    "            if key in x:\n",
    "                return str(x[key])\n",
    "        # fallback: serializza in JSON ordinato\n",
    "        return json.dumps(x, sort_keys=True)\n",
    "    elif isinstance(x, (list, tuple)):\n",
    "        return tuple(map(poi_id, x))\n",
    "    else:\n",
    "        return str(x)\n",
    "\n",
    "# Normalizza prediction_list e ground_truth\n",
    "df_all['prediction_norm']    = df_all['prediction_list'].apply(lambda lst: [poi_id(e) for e in lst])\n",
    "df_all['ground_truth_norm']  = df_all['ground_truth'].apply(poi_id)\n",
    "\n",
    "# -- metriche element-wise --------------------------------------------\n",
    "df_all['hit@1'] = df_all['prediction_norm'].str[0] == df_all['ground_truth_norm']\n",
    "\n",
    "def top_k_hit(row, k=5):\n",
    "    return row['ground_truth_norm'] in row['prediction_norm'][:k]\n",
    "\n",
    "def reciprocal_rank(row, k=5):\n",
    "    try:\n",
    "        rank = row['prediction_norm'][:k].index(row['ground_truth_norm']) + 1\n",
    "        return 1.0 / rank\n",
    "    except ValueError:\n",
    "        return 0.0\n",
    "\n",
    "df_all['hit@5'] = df_all.apply(top_k_hit, axis=1)\n",
    "df_all['rr']    = df_all.apply(reciprocal_rank, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4b31a1",
   "metadata": {},
   "source": [
    "### 2.2 Global metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "87936f47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_cceae\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_cceae_level0_col0\" class=\"col_heading level0 col0\" >Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_cceae_level0_row0\" class=\"row_heading level0 row0\" >Top-1 Accuracy</th>\n",
       "      <td id=\"T_cceae_row0_col0\" class=\"data row0 col0\" >17.22%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_cceae_level0_row1\" class=\"row_heading level0 row1\" >Top-5 Hit Rate</th>\n",
       "      <td id=\"T_cceae_row1_col0\" class=\"data row1 col0\" >49.57%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_cceae_level0_row2\" class=\"row_heading level0 row2\" >MRR</th>\n",
       "      <td id=\"T_cceae_row2_col0\" class=\"data row2 col0\" >28.79%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_cceae_level0_row3\" class=\"row_heading level0 row3\" >Catalogue Coverage</th>\n",
       "      <td id=\"T_cceae_row3_col0\" class=\"data row3 col0\" >1854.55%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7faad8bcc150>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "metrics_global = {\n",
    "    \"Top-1 Accuracy\": df_all[\"hit@1\"].mean(),\n",
    "    \"Top-5 Hit Rate\": df_all[\"hit@5\"].mean(),\n",
    "    \"MRR\": df_all[\"rr\"].mean(),\n",
    "}\n",
    "\n",
    "# Catalogue Coverage\n",
    "coverage_set = {poi for preds in df_all[\"prediction_norm\"] for poi in preds}\n",
    "metrics_global[\"Catalogue Coverage\"] = (\n",
    "    len(coverage_set) / df_all[\"ground_truth_norm\"].nunique()\n",
    ")\n",
    "\n",
    "# Visualizza in percentuale\n",
    "metrics_df = (\n",
    "    pd.DataFrame(metrics_global, index=[\"Value\"])\n",
    "      .T\n",
    "      .style.format(\"{:.2%}\")\n",
    ")\n",
    "display(metrics_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1f7e3c",
   "metadata": {},
   "source": [
    "### 2.3 Metrics by year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3b12f0f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>top1</th>\n",
       "      <th>hit5</th>\n",
       "      <th>mrr</th>\n",
       "      <th>n</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2014</td>\n",
       "      <td>0.097995</td>\n",
       "      <td>0.470489</td>\n",
       "      <td>0.224683</td>\n",
       "      <td>65891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015</td>\n",
       "      <td>0.268632</td>\n",
       "      <td>0.518972</td>\n",
       "      <td>0.365281</td>\n",
       "      <td>47176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016</td>\n",
       "      <td>0.102614</td>\n",
       "      <td>0.487599</td>\n",
       "      <td>0.237478</td>\n",
       "      <td>32900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017</td>\n",
       "      <td>0.271880</td>\n",
       "      <td>0.531706</td>\n",
       "      <td>0.371477</td>\n",
       "      <td>9872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018</td>\n",
       "      <td>0.267283</td>\n",
       "      <td>0.519520</td>\n",
       "      <td>0.363647</td>\n",
       "      <td>9836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2019</td>\n",
       "      <td>0.233394</td>\n",
       "      <td>0.515288</td>\n",
       "      <td>0.340719</td>\n",
       "      <td>10433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2020</td>\n",
       "      <td>0.309677</td>\n",
       "      <td>0.554839</td>\n",
       "      <td>0.399928</td>\n",
       "      <td>465</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year      top1      hit5       mrr      n\n",
       "0  2014  0.097995  0.470489  0.224683  65891\n",
       "1  2015  0.268632  0.518972  0.365281  47176\n",
       "2  2016  0.102614  0.487599  0.237478  32900\n",
       "3  2017  0.271880  0.531706  0.371477   9872\n",
       "4  2018  0.267283  0.519520  0.363647   9836\n",
       "5  2019  0.233394  0.515288  0.340719  10433\n",
       "6  2020  0.309677  0.554839  0.399928    465"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "by_year = (\n",
    "    df_all\n",
    "    .groupby('year')\n",
    "    .agg(\n",
    "        top1=('hit@1', 'mean'),\n",
    "        hit5=('hit@5', 'mean'),\n",
    "        mrr=('rr', 'mean'),\n",
    "        n=('card_id', 'size')\n",
    "    )\n",
    "    .reset_index()\n",
    "    .sort_values('year')\n",
    ")\n",
    "display(by_year)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ac7adc",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3f4d2744",
   "metadata": {},
   "source": "## 3. Enhanced Visualizations"
  },
  {
   "cell_type": "code",
   "id": "4b3j8n1a8ol",
   "source": "# Multi-metric comparison chart\nfig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n\n# Top-1 Accuracy\nbars1 = ax1.bar(by_year['year'].astype(str), by_year['top1']*100, \n                color='steelblue', alpha=0.7, edgecolor='black')\nmedian1 = by_year['top1'].median() * 100\nax1.axhline(y=median1, color='red', linestyle='--', alpha=0.8)\nfor bar, value in zip(bars1, by_year['top1']*100):\n    ax1.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.5,\n             f'{value:.1f}%', ha='center', va='bottom', fontweight='bold')\nax1.set_title('Top-1 Accuracy', fontweight='bold', fontsize=14)\nax1.set_ylabel('Accuracy (%)', fontweight='bold')\nax1.grid(True, alpha=0.3)\n\n# Top-5 Hit Rate  \nbars2 = ax2.bar(by_year['year'].astype(str), by_year['hit5']*100,\n                color='forestgreen', alpha=0.7, edgecolor='black')\nmedian2 = by_year['hit5'].median() * 100\nax2.axhline(y=median2, color='red', linestyle='--', alpha=0.8)\nfor bar, value in zip(bars2, by_year['hit5']*100):\n    ax2.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.5,\n             f'{value:.1f}%', ha='center', va='bottom', fontweight='bold')\nax2.set_title('Top-5 Hit Rate', fontweight='bold', fontsize=14)\nax2.set_ylabel('Hit Rate (%)', fontweight='bold')\nax2.grid(True, alpha=0.3)\n\n# MRR\nbars3 = ax3.bar(by_year['year'].astype(str), by_year['mrr']*100,\n                color='darkorange', alpha=0.7, edgecolor='black')\nmedian3 = by_year['mrr'].median() * 100\nax3.axhline(y=median3, color='red', linestyle='--', alpha=0.8)\nfor bar, value in zip(bars3, by_year['mrr']*100):\n    ax3.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.5,\n             f'{value:.1f}%', ha='center', va='bottom', fontweight='bold')\nax3.set_title('Mean Reciprocal Rank', fontweight='bold', fontsize=14)\nax3.set_ylabel('MRR (%)', fontweight='bold')\nax3.set_xlabel('Year', fontweight='bold')\nax3.grid(True, alpha=0.3)\n\n# Sample sizes\nbars4 = ax4.bar(by_year['year'].astype(str), by_year['n'],\n                color='purple', alpha=0.7, edgecolor='black')\nmedian4 = by_year['n'].median()\nax4.axhline(y=median4, color='red', linestyle='--', alpha=0.8)\nfor bar, value in zip(bars4, by_year['n']):\n    ax4.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 500,\n             f'{value:,}', ha='center', va='bottom', fontweight='bold')\nax4.set_title('Sample Size by Year', fontweight='bold', fontsize=14)\nax4.set_ylabel('Number of Predictions', fontweight='bold')\nax4.set_xlabel('Year', fontweight='bold')\nax4.grid(True, alpha=0.3)\n\nplt.suptitle('LLM Model Performance Analysis - All Metrics', fontsize=16, fontweight='bold', y=1.02)\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "6cqoxdfys26",
   "source": "### 3.1 Performance Trends Analysis\n\n# Line plot showing performance trends over years\nfig, ax = plt.subplots(figsize=(12, 8))\n\n# Plot all metrics as lines\nx_years = by_year['year'].astype(int)\nax.plot(x_years, by_year['top1']*100, marker='o', linewidth=3, markersize=8, \n        label='Top-1 Accuracy', color='steelblue')\nax.plot(x_years, by_year['hit5']*100, marker='s', linewidth=3, markersize=8,\n        label='Top-5 Hit Rate', color='forestgreen') \nax.plot(x_years, by_year['mrr']*100, marker='^', linewidth=3, markersize=8,\n        label='Mean Reciprocal Rank', color='darkorange')\n\n# Add value annotations\nfor i, year in enumerate(x_years):\n    ax.annotate(f\"{by_year.iloc[i]['top1']*100:.1f}%\", \n                (year, by_year.iloc[i]['top1']*100 + 1), \n                ha='center', fontsize=10, fontweight='bold')\n    ax.annotate(f\"{by_year.iloc[i]['hit5']*100:.1f}%\", \n                (year, by_year.iloc[i]['hit5']*100 + 1), \n                ha='center', fontsize=10, fontweight='bold')\n    ax.annotate(f\"{by_year.iloc[i]['mrr']*100:.1f}%\", \n                (year, by_year.iloc[i]['mrr']*100 + 1), \n                ha='center', fontsize=10, fontweight='bold')\n\n# Styling\nax.set_xlabel('Year', fontsize=14, fontweight='bold')\nax.set_ylabel('Performance (%)', fontsize=14, fontweight='bold')\nax.set_title('Model Performance Trends Over Time', fontsize=16, fontweight='bold', pad=20)\nax.legend(fontsize=12, loc='best')\nax.grid(True, alpha=0.3)\nax.set_xticks(x_years)\nax.set_ylim(0, max(by_year['hit5']*100) * 1.1)\n\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "45jb7n9ou9o",
   "source": "### 3.2 POI Popularity Distribution Analysis\n\n# Analyze POI visit frequency and prediction accuracy\npoi_stats = df_all.groupby('ground_truth_norm').agg({\n    'hit@1': ['count', 'mean'],\n    'hit@5': 'mean',\n    'rr': 'mean'\n}).round(4)\n\n# Flatten column names\npoi_stats.columns = ['visit_count', 'accuracy', 'hit5_rate', 'mrr']\npoi_stats = poi_stats.sort_values('visit_count', ascending=False).head(15)\n\n# Enhanced POI analysis visualization\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 8))\n\n# Left plot: Visit counts with accuracy color coding\ncolors = plt.cm.RdYlGn(poi_stats['accuracy'])\nbars1 = ax1.bar(range(len(poi_stats)), poi_stats['visit_count'], \n                color=colors, edgecolor='black', linewidth=1)\n\n# Add value labels\nfor i, (bar, count, acc) in enumerate(zip(bars1, poi_stats['visit_count'], poi_stats['accuracy'])):\n    ax1.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 500,\n             f'{count:,}', ha='center', va='bottom', fontweight='bold', fontsize=10)\n    ax1.text(bar.get_x() + bar.get_width()/2., bar.get_height()/2,\n             f'{acc:.1%}', ha='center', va='center', fontweight='bold', \n             fontsize=9, color='white' if acc < 0.5 else 'black')\n\nax1.set_xticks(range(len(poi_stats)))\nax1.set_xticklabels(poi_stats.index, rotation=45, ha='right')\nax1.set_ylabel('Visit Count', fontsize=12, fontweight='bold')\nax1.set_title('POI Visit Frequency (Color = Accuracy)', fontsize=14, fontweight='bold')\nax1.grid(True, axis='y', alpha=0.3)\n\n# Right plot: Accuracy vs Visit Count scatter\nscatter = ax2.scatter(poi_stats['visit_count'], poi_stats['accuracy']*100, \n                     s=200, c=poi_stats['hit5_rate']*100, cmap='viridis', \n                     alpha=0.7, edgecolor='black')\n\n# Add POI labels\nfor poi, row in poi_stats.iterrows():\n    ax2.annotate(poi[:15] + ('...' if len(poi) > 15 else ''), \n                (row['visit_count'], row['accuracy']*100),\n                xytext=(5, 5), textcoords='offset points', fontsize=9)\n\nax2.set_xlabel('Visit Count', fontsize=12, fontweight='bold')\nax2.set_ylabel('Top-1 Accuracy (%)', fontsize=12, fontweight='bold')\nax2.set_title('Accuracy vs Popularity (Color = Hit@5 Rate)', fontsize=14, fontweight='bold')\nax2.grid(True, alpha=0.3)\n\n# Add colorbar\ncbar = plt.colorbar(scatter, ax=ax2)\ncbar.set_label('Top-5 Hit Rate (%)', fontsize=11, fontweight='bold')\n\nplt.tight_layout()\nplt.show()\n\n# Summary statistics\nprint(f\"\\n📊 POI Analysis Summary:\")\nprint(f\"   • Most popular POI: {poi_stats.index[0]} ({poi_stats.iloc[0]['visit_count']:,} visits)\")\nprint(f\"   • Highest accuracy POI: {poi_stats.loc[poi_stats['accuracy'].idxmax()].name} ({poi_stats['accuracy'].max():.1%})\")\nprint(f\"   • Correlation visit_count vs accuracy: {poi_stats['visit_count'].corr(poi_stats['accuracy']):.3f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "1uv7qt4246i",
   "source": "### 3.3 Model Performance Summary Dashboard\n\n# Create comprehensive performance summary\nfig = plt.figure(figsize=(20, 12))\ngs = fig.add_gridspec(3, 4, height_ratios=[1, 1, 1], width_ratios=[1, 1, 1, 1])\n\n# Overall metrics (top row)\nax_overall = fig.add_subplot(gs[0, :2])\nmetrics_values = [metrics_global[\"Top-1 Accuracy\"]*100, \n                 metrics_global[\"Top-5 Hit Rate\"]*100,\n                 metrics_global[\"MRR\"]*100,\n                 metrics_global[\"Catalogue Coverage\"]*100]\nmetrics_names = [\"Top-1\\nAccuracy\", \"Top-5\\nHit Rate\", \"Mean Reciprocal\\nRank\", \"Catalogue\\nCoverage\"]\ncolors = ['steelblue', 'forestgreen', 'darkorange', 'purple']\n\nbars = ax_overall.bar(metrics_names, metrics_values, color=colors, alpha=0.7, edgecolor='black')\nfor bar, value in zip(bars, metrics_values):\n    ax_overall.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 1,\n                   f'{value:.1f}%', ha='center', va='bottom', fontweight='bold', fontsize=14)\nax_overall.set_title('Overall Model Performance Metrics', fontsize=16, fontweight='bold')\nax_overall.set_ylabel('Performance (%)', fontsize=12, fontweight='bold')\nax_overall.grid(True, alpha=0.3)\n\n# Year trend (top right)\nax_trend = fig.add_subplot(gs[0, 2:])\nx_pos = range(len(by_year))\nax_trend.plot(x_pos, by_year['top1']*100, 'o-', linewidth=2, markersize=6, label='Accuracy')\nax_trend.plot(x_pos, by_year['hit5']*100, 's-', linewidth=2, markersize=6, label='Hit@5')\nax_trend.set_xticks(x_pos)\nax_trend.set_xticklabels(by_year['year'].astype(int))\nax_trend.set_title('Performance Trend by Year', fontsize=16, fontweight='bold')\nax_trend.set_ylabel('Performance (%)', fontsize=12, fontweight='bold')\nax_trend.legend()\nax_trend.grid(True, alpha=0.3)\n\n# Sample distribution (middle left)\nax_samples = fig.add_subplot(gs[1, :2])\nbars_samples = ax_samples.bar(by_year['year'].astype(str), by_year['n'], \n                             color='mediumseagreen', alpha=0.7, edgecolor='black')\nfor bar, value in zip(bars_samples, by_year['n']):\n    ax_samples.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 200,\n                   f'{value:,}', ha='center', va='bottom', fontweight='bold', fontsize=10)\nax_samples.set_title('Sample Distribution by Year', fontsize=16, fontweight='bold')\nax_samples.set_ylabel('Number of Predictions', fontsize=12, fontweight='bold')\nax_samples.grid(True, alpha=0.3)\n\n# Top errors (middle right)\nax_errors = fig.add_subplot(gs[1, 2:])\ntop_errors = pairs.head(8)\nbars_err = ax_errors.barh(range(len(top_errors)), top_errors['errors'],\n                         color='lightcoral', alpha=0.7, edgecolor='black')\nax_errors.set_yticks(range(len(top_errors)))\nax_errors.set_yticklabels([f\"{row['ground_truth_norm'][:8]}→{row['first_pred'][:8]}\" \n                          for _, row in top_errors.iterrows()], fontsize=9)\nax_errors.set_title('Top Error Patterns', fontsize=16, fontweight='bold')\nax_errors.set_xlabel('Error Count', fontsize=12, fontweight='bold')\nax_errors.grid(True, alpha=0.3)\n\n# Performance by POI popularity (bottom)\nax_poi_perf = fig.add_subplot(gs[2, :])\ntop_poi_for_viz = poi_stats.head(10)\nx_poi = range(len(top_poi_for_viz))\n\n# Dual y-axis plot\nax_poi_perf2 = ax_poi_perf.twinx()\n\nbars_visits = ax_poi_perf.bar([x - 0.2 for x in x_poi], top_poi_for_viz['visit_count'], \n                             width=0.4, color='lightblue', alpha=0.7, label='Visit Count')\nbars_acc = ax_poi_perf2.bar([x + 0.2 for x in x_poi], top_poi_for_viz['accuracy']*100,\n                           width=0.4, color='orange', alpha=0.7, label='Accuracy (%)')\n\nax_poi_perf.set_xticks(x_poi)\nax_poi_perf.set_xticklabels([poi[:10] + ('...' if len(poi) > 10 else '') \n                           for poi in top_poi_for_viz.index], rotation=45, ha='right')\nax_poi_perf.set_title('Performance vs Popularity - Top 10 POI', fontsize=16, fontweight='bold')\nax_poi_perf.set_ylabel('Visit Count', fontsize=12, fontweight='bold', color='blue')\nax_poi_perf2.set_ylabel('Accuracy (%)', fontsize=12, fontweight='bold', color='orange')\nax_poi_perf.grid(True, alpha=0.3)\n\n# Add legends\nax_poi_perf.legend(loc='upper left')\nax_poi_perf2.legend(loc='upper right')\n\nplt.suptitle('🎯 LLM-Mob Model Performance Dashboard', fontsize=20, fontweight='bold', y=0.98)\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cbabe05",
   "metadata": {},
   "outputs": [],
   "source": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Enhanced bar chart for Top-1 Accuracy\nfig, ax = plt.subplots(figsize=(12, 8))\n\n# Create bars\nbars = ax.bar(by_year['year'].astype(str), by_year['top1']*100, \n              color='steelblue', alpha=0.7, edgecolor='black', linewidth=1.2)\n\n# Add median line\nmedian_value = by_year['top1'].median() * 100\nax.axhline(y=median_value, color='red', linestyle='--', linewidth=2, \n           label=f'Median: {median_value:.1f}%', alpha=0.8)\n\n# Add value labels on each bar\nfor bar, value in zip(bars, by_year['top1']*100):\n    height = bar.get_height()\n    ax.text(bar.get_x() + bar.get_width()/2., height + 0.5,\n            f'{value:.1f}%', ha='center', va='bottom', fontweight='bold', fontsize=11)\n\n# Styling improvements\nax.set_ylabel('Top-1 Accuracy (%)', fontsize=14, fontweight='bold')\nax.set_xlabel('Year', fontsize=14, fontweight='bold')\nax.set_title('Top-1 Accuracy (%) by Year', fontsize=16, fontweight='bold', pad=20)\nax.grid(True, axis='y', alpha=0.3, linestyle='-')\nax.set_ylim(0, max(by_year['top1']*100) * 1.15)\nax.legend(fontsize=12)\n\n# Add sample size annotations\nfor i, (bar, n) in enumerate(zip(bars, by_year['n'])):\n    ax.text(bar.get_x() + bar.get_width()/2., -max(by_year['top1']*100) * 0.05,\n            f'n={n:,}', ha='center', va='top', fontsize=10, alpha=0.7)\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58242dcd",
   "metadata": {},
   "outputs": [],
   "source": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Enhanced bar chart for Top-5 Hit Rate\nfig, ax = plt.subplots(figsize=(12, 8))\n\n# Create bars\nbars = ax.bar(by_year['year'].astype(str), by_year['hit5']*100, \n              color='forestgreen', alpha=0.7, edgecolor='black', linewidth=1.2)\n\n# Add median line\nmedian_value = by_year['hit5'].median() * 100\nax.axhline(y=median_value, color='red', linestyle='--', linewidth=2, \n           label=f'Median: {median_value:.1f}%', alpha=0.8)\n\n# Add value labels on each bar\nfor bar, value in zip(bars, by_year['hit5']*100):\n    height = bar.get_height()\n    ax.text(bar.get_x() + bar.get_width()/2., height + 0.5,\n            f'{value:.1f}%', ha='center', va='bottom', fontweight='bold', fontsize=11)\n\n# Styling improvements\nax.set_ylabel('Top-5 Hit Rate (%)', fontsize=14, fontweight='bold')\nax.set_xlabel('Year', fontsize=14, fontweight='bold')\nax.set_title('Top-5 Hit Rate (%) by Year', fontsize=16, fontweight='bold', pad=20)\nax.grid(True, axis='y', alpha=0.3, linestyle='-')\nax.set_ylim(0, max(by_year['hit5']*100) * 1.1)\nax.legend(fontsize=12)\n\n# Add sample size annotations\nfor i, (bar, n) in enumerate(zip(bars, by_year['n'])):\n    ax.text(bar.get_x() + bar.get_width()/2., -max(by_year['hit5']*100) * 0.03,\n            f'n={n:,}', ha='center', va='top', fontsize=10, alpha=0.7)\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12cf7a7",
   "metadata": {},
   "outputs": [],
   "source": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Enhanced bar chart for MRR\nfig, ax = plt.subplots(figsize=(12, 8))\n\n# Create bars\nbars = ax.bar(by_year['year'].astype(str), by_year['mrr']*100, \n              color='darkorange', alpha=0.7, edgecolor='black', linewidth=1.2)\n\n# Add median line\nmedian_value = by_year['mrr'].median() * 100\nax.axhline(y=median_value, color='red', linestyle='--', linewidth=2, \n           label=f'Median: {median_value:.1f}%', alpha=0.8)\n\n# Add value labels on each bar\nfor bar, value in zip(bars, by_year['mrr']*100):\n    height = bar.get_height()\n    ax.text(bar.get_x() + bar.get_width()/2., height + 0.5,\n            f'{value:.1f}%', ha='center', va='bottom', fontweight='bold', fontsize=11)\n\n# Styling improvements\nax.set_ylabel('Mean Reciprocal Rank (%)', fontsize=14, fontweight='bold')\nax.set_xlabel('Year', fontsize=14, fontweight='bold')\nax.set_title('Mean Reciprocal Rank (%) by Year', fontsize=16, fontweight='bold', pad=20)\nax.grid(True, axis='y', alpha=0.3, linestyle='-')\nax.set_ylim(0, max(by_year['mrr']*100) * 1.15)\nax.legend(fontsize=12)\n\n# Add sample size annotations\nfor i, (bar, n) in enumerate(zip(bars, by_year['n'])):\n    ax.text(bar.get_x() + bar.get_width()/2., -max(by_year['mrr']*100) * 0.05,\n            f'n={n:,}', ha='center', va='top', fontsize=10, alpha=0.7)\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "id": "c38d992e",
   "metadata": {},
   "source": [
    "### 3.1 Coverage breakdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5dab3c15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POI distinct in predictions: 408\n",
      "POI distinct in ground-truth: 22\n",
      "Catalogue Coverage: 1854.55%\n"
     ]
    }
   ],
   "source": [
    "unique_pred_poi = (\n",
    "    pd.Series([poi for preds in df_all[\"prediction_norm\"] for poi in preds])\n",
    "      .nunique()\n",
    ")\n",
    "unique_gt_poi = df_all[\"ground_truth_norm\"].nunique()\n",
    "\n",
    "print(f\"POI distinct in predictions: {unique_pred_poi}\")\n",
    "print(f\"POI distinct in ground-truth: {unique_gt_poi}\")\n",
    "print(f\"Catalogue Coverage: {unique_pred_poi/unique_gt_poi:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2aee9a",
   "metadata": {},
   "source": [
    "## 4. Error analysis – overview\n",
    "\n",
    "In questa sezione valutiamo **dove** e **perché** il modello sbaglia, seguendo tre filoni:\n",
    "\n",
    "1. **Worst-performing POI pairs**  \n",
    "   Scopriamo quali coppie `ground-truth → first_pred` generano il maggior numero di errori.\n",
    "\n",
    "2. **Confusion matrix per subset**  \n",
    "   Heat-map delle frequenze (o tasso d’errore) su un sotto-insieme di POI di interesse\n",
    "   (ad es. i 20 più visitati o solo l’anno 2020).\n",
    "\n",
    "3. **Explainability (SHAP / LIME)**  \n",
    "   Analisi dei fattori testuali nella colonna `history` che portano a una predizione\n",
    "   sbagliata. Qui mostriamo un prototipo basato su LIME; lo script è modulare\n",
    "   e può essere sostituito da SHAP se usi modelli compatibili.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f8e9b6",
   "metadata": {},
   "outputs": [],
   "source": "# Enhanced error analysis with improved visualization\nERR = df_all.assign(first_pred=df_all[\"prediction_norm\"].str[0])\nERR = ERR[~ERR[\"hit@1\"]]           \n\n# Count POI pairs with errors\npairs = (\n    ERR.groupby([\"ground_truth_norm\", \"first_pred\"])\n        .size()\n        .reset_index(name=\"errors\")\n        .sort_values(\"errors\", ascending=False)\n        .head(15)          \n)\n\ndisplay(pairs)\n\n# Enhanced horizontal bar plot\nfig, ax = plt.subplots(figsize=(14, 10))\n\n# Create color gradient based on error count\ncolors = plt.cm.Reds(np.linspace(0.3, 0.9, len(pairs)))\n\nbars = ax.barh(\n    pairs.apply(lambda r: f\"{r['ground_truth_norm']} → {r['first_pred']}\", axis=1),\n    pairs[\"errors\"],\n    color=colors,\n    edgecolor='black',\n    linewidth=0.8\n)\n\n# Add value labels on bars\nfor bar, value in zip(bars, pairs[\"errors\"]):\n    width = bar.get_width()\n    ax.text(width + width * 0.01, bar.get_y() + bar.get_height()/2,\n            f'{value:,}', ha='left', va='center', fontweight='bold', fontsize=11)\n\n# Styling improvements\nax.set_xlabel(\"Number of Errors\", fontsize=14, fontweight='bold')\nax.set_ylabel(\"POI Pair (True → Predicted)\", fontsize=14, fontweight='bold')\nax.set_title(\"Worst-performing POI Pairs – Top 15\", fontsize=16, fontweight='bold', pad=20)\nax.grid(True, axis='x', alpha=0.3, linestyle='-')\nax.invert_yaxis()\n\n# Add percentage annotations\ntotal_errors = len(ERR)\nfor bar, value in zip(bars, pairs[\"errors\"]):\n    percentage = value / total_errors * 100\n    ax.text(bar.get_width() * 0.5, bar.get_y() + bar.get_height()/2,\n            f'{percentage:.1f}%', ha='center', va='center', \n            fontweight='bold', fontsize=10, color='white')\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a501fa2e",
   "metadata": {},
   "outputs": [],
   "source": "# Enhanced confusion matrix visualization\nTOP_K = 20          \nYEAR  = None        \n\nsubset = df_all.copy()\nif YEAR is not None:\n    subset = subset.query(\"year == @YEAR\")\n\n# Filter most frequent POIs\ntop_poi = (\n    subset[\"ground_truth_norm\"]\n           .value_counts()\n           .head(TOP_K)\n           .index\n)\nmask = subset[\"ground_truth_norm\"].isin(top_poi) & subset[\"prediction_norm\"].str[0].isin(top_poi)\ncm_df = pd.crosstab(\n    subset.loc[mask, \"ground_truth_norm\"],\n    subset.loc[mask, \"prediction_norm\"].str[0],\n    rownames=[\"True\"],\n    colnames=[\"Pred\"],\n    dropna=False\n)\n\n# Normalize by rows for error rates\ncm_norm = cm_df.div(cm_df.sum(axis=1), axis=0)\n\n# Enhanced heat-map\nfig, ax = plt.subplots(figsize=(14, 12))\nim = ax.imshow(cm_norm.values, aspect=\"auto\", cmap='RdYlBu_r', vmin=0, vmax=1)\n\n# Customize ticks and labels\nax.set_xticks(range(len(cm_norm.columns)))\nax.set_xticklabels(cm_norm.columns, rotation=45, ha='right', fontsize=11)\nax.set_yticks(range(len(cm_norm.index)))\nax.set_yticklabels(cm_norm.index, fontsize=11)\n\n# Add text annotations\nfor i in range(len(cm_norm.index)):\n    for j in range(len(cm_norm.columns)):\n        value = cm_norm.values[i, j]\n        if value > 0.01:  # Only show values > 1%\n            color = 'white' if value > 0.5 else 'black'\n            ax.text(j, i, f'{value:.2f}', ha='center', va='center',\n                   fontweight='bold', fontsize=9, color=color)\n\n# Enhanced styling\nax.set_xlabel(\"Predicted POI\", fontsize=14, fontweight='bold')\nax.set_ylabel(\"True POI\", fontsize=14, fontweight='bold')\nax.set_title(f\"Confusion Matrix – Top {TOP_K} POI\" + \n            (f\", Year {YEAR}\" if YEAR else \"\"), \n            fontsize=16, fontweight='bold', pad=20)\n\n# Enhanced colorbar\ncbar = fig.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\ncbar.set_label(\"Prediction Probability\", fontsize=12, fontweight='bold')\ncbar.ax.tick_params(labelsize=11)\n\nplt.tight_layout()\nplt.show()\n\n# Add summary statistics\nprint(f\"\\n📊 Confusion Matrix Statistics:\")\nprint(f\"   • Diagonal accuracy: {np.diag(cm_norm).mean():.2%}\")\nprint(f\"   • Most confused pair: {cm_norm.stack().drop(cm_norm.index).idxmax()}\")\nprint(f\"   • Average prediction entropy: {-np.sum(cm_norm * np.log(cm_norm + 1e-10), axis=1).mean():.3f}\")"
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "aa0439af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ERROR ANALYSIS - Simplified Version (No LIME required)\n",
      "============================================================\n",
      "📊 Analyzing 146,175 prediction errors vs 30,398 correct predictions\n",
      "\n",
      "1️⃣ HISTORY LENGTH ANALYSIS\n",
      "----------------------------------------\n",
      "Average history length in ERRORS: 52 chars\n",
      "Average history length in CORRECT: 51 chars\n",
      "\n",
      "2️⃣ KEYWORD ANALYSIS IN ERROR CASES\n",
      "----------------------------------------\n",
      "Most frequent words in ERROR cases:\n",
      "            arena:  869\n",
      "        giulietta:  685\n",
      "             casa:  626\n",
      "            torre:  439\n",
      "         lamberti:  439\n",
      "            santa:  256\n",
      "        anastasia:  256\n",
      "    castelvecchio:  237\n",
      "          palazzo:  194\n",
      "            della:  194\n",
      "          ragione:  194\n",
      "              san:  190\n",
      "            duomo:  138\n",
      "            fermo:  125\n",
      "           teatro:  114\n",
      "\n",
      "Most frequent words in CORRECT cases:\n",
      "            arena:  768\n",
      "        giulietta:  595\n",
      "             casa:  460\n",
      "            torre:  415\n",
      "         lamberti:  415\n",
      "            santa:  384\n",
      "        anastasia:  384\n",
      "            duomo:  320\n",
      "              san:  306\n",
      "    castelvecchio:  276\n",
      "          palazzo:  201\n",
      "            della:  201\n",
      "          ragione:  201\n",
      "            fermo:  186\n",
      "           teatro:  169\n",
      "\n",
      "3️⃣ MOST PROBLEMATIC POI (Ground Truth)\n",
      "----------------------------------------\n",
      "POI with highest error counts:\n",
      "              Castelvecchio: 30742 errors (87.9% rate)\n",
      "            Tomba Giulietta: 19578 errors (100.0% rate)\n",
      "              Teatro Romano: 14633 errors (95.2% rate)\n",
      "            Santa Anastasia: 11899 errors (95.2% rate)\n",
      "                   San Zeno: 11450 errors (99.2% rate)\n",
      "                      Duomo: 11141 errors (63.3% rate)\n",
      "                      Arena: 9790 errors (71.9% rate)\n",
      "             Torre Lamberti: 9108 errors (63.4% rate)\n",
      "             Casa Giulietta: 8720 errors (55.7% rate)\n",
      "                  San Fermo: 5702 errors (90.4% rate)\n",
      "\n",
      "4️⃣ MOST FREQUENT WRONG PREDICTIONS\n",
      "----------------------------------------\n",
      "Most frequently predicted (wrong) POI:\n",
      "                        AMO: 29660 times\n",
      "             Casa Giulietta: 14199 times\n",
      "                 Sighseeing: 13922 times\n",
      "                      Duomo: 13613 times\n",
      "                Verona Tour: 11948 times\n",
      "             Torre Lamberti: 11429 times\n",
      "            Museo Lapidario: 11220 times\n",
      "          Centro Fotografia: 10670 times\n",
      "                      Arena: 5013 times\n",
      "               Museo Storia: 4158 times\n",
      "\n",
      "5️⃣ DETAILED ERROR EXAMPLE\n",
      "----------------------------------------\n",
      "Card ID: 040002523F3885\n",
      "Year: 2014\n",
      "True POI: Castelvecchio\n",
      "Predicted POI: Verona Tour\n",
      "Top-5 Predictions: ['Verona Tour', 'Museo Lapidario', 'Sighseeing', 'Castelvecchio', 'Centro Fotografia']\n",
      "History (first 200 chars): ['San Fermo', 'Santa Anastasia', 'Teatro Romano', 'Duomo', 'Torre Lamberti', 'Casa Giulietta', 'San Zeno', 'Palazzo della Ragione']...\n",
      "\n",
      "6️⃣ CONFUSION MATRIX - TOP 10 POI\n",
      "----------------------------------------\n",
      "Confusion matrix (% of predictions for each true POI):\n",
      "Rows = True POI, Columns = Predicted POI\n",
      "prediction_norm    Arena  Casa Giulietta  Castelvecchio  Duomo  San Fermo  \\\n",
      "ground_truth_norm                                                           \n",
      "Arena              0.476           0.140          0.028  0.120      0.046   \n",
      "Casa Giulietta     0.035           0.726          0.018  0.123      0.029   \n",
      "Castelvecchio      0.053           0.169          0.248  0.213      0.063   \n",
      "Duomo              0.026           0.132          0.020  0.627      0.013   \n",
      "San Fermo          0.113           0.228          0.078  0.139      0.189   \n",
      "San Zeno           0.200           0.334          0.045  0.142      0.061   \n",
      "Santa Anastasia    0.052           0.259          0.032  0.173      0.022   \n",
      "Teatro Romano      0.050           0.245          0.039  0.276      0.055   \n",
      "Tomba Giulietta    0.124           0.116          0.104  0.224      0.032   \n",
      "Torre Lamberti     0.029           0.135          0.031  0.126      0.028   \n",
      "\n",
      "prediction_norm    San Zeno  Santa Anastasia  Teatro Romano  Torre Lamberti  \n",
      "ground_truth_norm                                                            \n",
      "Arena                 0.000            0.027          0.032           0.130  \n",
      "Casa Giulietta        0.000            0.022          0.034           0.013  \n",
      "Castelvecchio         0.000            0.059          0.053           0.141  \n",
      "Duomo                 0.000            0.065          0.020           0.096  \n",
      "San Fermo             0.002            0.037          0.053           0.163  \n",
      "San Zeno              0.022            0.054          0.060           0.082  \n",
      "Santa Anastasia       0.000            0.100          0.040           0.323  \n",
      "Teatro Romano         0.002            0.031          0.118           0.183  \n",
      "Tomba Giulietta       0.005            0.081          0.055           0.260  \n",
      "Torre Lamberti        0.001            0.021          0.071           0.559  \n",
      "\n",
      "7️⃣ SUMMARY STATISTICS\n",
      "----------------------------------------\n",
      "Total predictions analyzed: 176,573\n",
      "Total errors (Top-1): 146,175 (82.78%)\n",
      "Unique POI in dataset: 22\n",
      "Unique POI in predictions: 408\n",
      "\n",
      "Error rate by year:\n",
      "  2014: 90.20% (65891 predictions)\n",
      "  2015: 73.14% (47176 predictions)\n",
      "  2016: 89.74% (32900 predictions)\n",
      "  2017: 72.81% (9872 predictions)\n",
      "  2018: 73.27% (9836 predictions)\n",
      "  2019: 76.66% (10433 predictions)\n",
      "  2020: 69.03% (465 predictions)\n",
      "\n",
      "============================================================\n",
      "ANALYSIS COMPLETE\n",
      "This analysis doesn't require LIME but provides valuable insights into prediction errors.\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------------------\n",
    "# 4.3 Explainability \n",
    "# ---------------------------------------------------------------------\n",
    "import warnings, json, numpy as np, pandas as pd\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ERROR ANALYSIS - Simplified Version (No LIME required)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Analisi dei pattern testuali negli errori\n",
    "# ---------------------------------------------------------------------\n",
    "err_rows = df_all[~df_all[\"hit@1\"]].copy()\n",
    "correct_rows = df_all[df_all[\"hit@1\"]].copy()\n",
    "\n",
    "if err_rows.empty:\n",
    "    print(\"🎉 Nessun errore Top-1 da analizzare!\")\n",
    "else:\n",
    "    print(f\"📊 Analyzing {len(err_rows):,} prediction errors vs {len(correct_rows):,} correct predictions\")\n",
    "    \n",
    "    # Aggiungi colonne per l'analisi\n",
    "    err_rows['first_pred'] = err_rows[\"prediction_norm\"].str[0]\n",
    "    correct_rows['first_pred'] = correct_rows[\"prediction_norm\"].str[0]\n",
    "    \n",
    "    # ---------------------------------------------------------------------\n",
    "    # 1. Analisi lunghezza history\n",
    "    # ---------------------------------------------------------------------\n",
    "    print(\"\\n1️⃣ HISTORY LENGTH ANALYSIS\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    err_rows['history_length'] = err_rows['history'].astype(str).str.len()\n",
    "    correct_rows['history_length'] = correct_rows['history'].astype(str).str.len()\n",
    "    \n",
    "    print(f\"Average history length in ERRORS: {err_rows['history_length'].mean():.0f} chars\")\n",
    "    print(f\"Average history length in CORRECT: {correct_rows['history_length'].mean():.0f} chars\")\n",
    "    \n",
    "    # ---------------------------------------------------------------------\n",
    "    # 2. Analisi parole chiave più frequenti negli errori\n",
    "    # ---------------------------------------------------------------------\n",
    "    print(\"\\n2️⃣ KEYWORD ANALYSIS IN ERROR CASES\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    def extract_keywords(text, min_length=3):\n",
    "        \"\"\"Estrae parole significative dal testo\"\"\"\n",
    "        if pd.isna(text):\n",
    "            return []\n",
    "        # Rimuovi caratteri speciali e converti in lowercase\n",
    "        words = re.findall(r'\\b[a-zA-ZÀ-ÿ]+\\b', str(text).lower())\n",
    "        # Filtra parole troppo corte e comuni\n",
    "        stop_words = {'and', 'the', 'poi', 'via', 'del', 'dei', 'per', 'con', 'una', 'uno', 'che', 'nel', 'sul'}\n",
    "        return [w for w in words if len(w) >= min_length and w not in stop_words]\n",
    "    \n",
    "    # Estrai keywords dalle history degli errori\n",
    "    error_keywords = []\n",
    "    for history in err_rows['history'].head(1000):  # Limita per performance\n",
    "        error_keywords.extend(extract_keywords(history))\n",
    "    \n",
    "    # Estrai keywords dalle history corrette\n",
    "    correct_keywords = []\n",
    "    for history in correct_rows['history'].head(1000):  # Limita per performance\n",
    "        correct_keywords.extend(extract_keywords(history))\n",
    "    \n",
    "    error_word_freq = Counter(error_keywords).most_common(15)\n",
    "    correct_word_freq = Counter(correct_keywords).most_common(15)\n",
    "    \n",
    "    print(\"Most frequent words in ERROR cases:\")\n",
    "    for word, freq in error_word_freq:\n",
    "        print(f\"  {word:>15s}: {freq:>4d}\")\n",
    "    \n",
    "    print(\"\\nMost frequent words in CORRECT cases:\")\n",
    "    for word, freq in correct_word_freq:\n",
    "        print(f\"  {word:>15s}: {freq:>4d}\")\n",
    "    \n",
    "    # ---------------------------------------------------------------------\n",
    "    # 3. Analisi dei POI più problematici\n",
    "    # ---------------------------------------------------------------------\n",
    "    print(\"\\n3️⃣ MOST PROBLEMATIC POI (Ground Truth)\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # POI con più errori\n",
    "    error_by_true_poi = (\n",
    "        err_rows['ground_truth_norm']\n",
    "        .value_counts()\n",
    "        .head(10)\n",
    "        .reset_index()\n",
    "    )\n",
    "    error_by_true_poi.columns = ['POI', 'Error_Count']\n",
    "    \n",
    "    # Calcola anche il tasso di errore\n",
    "    total_by_poi = df_all['ground_truth_norm'].value_counts()\n",
    "    error_by_true_poi['Total_Count'] = error_by_true_poi['POI'].map(total_by_poi)\n",
    "    error_by_true_poi['Error_Rate'] = error_by_true_poi['Error_Count'] / error_by_true_poi['Total_Count']\n",
    "    \n",
    "    print(\"POI with highest error counts:\")\n",
    "    for _, row in error_by_true_poi.iterrows():\n",
    "        print(f\"  {row['POI']:>25s}: {row['Error_Count']:>3d} errors ({row['Error_Rate']:>5.1%} rate)\")\n",
    "    \n",
    "    # ---------------------------------------------------------------------\n",
    "    # 4. Analisi delle predizioni più frequenti negli errori\n",
    "    # ---------------------------------------------------------------------\n",
    "    print(\"\\n4️⃣ MOST FREQUENT WRONG PREDICTIONS\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    wrong_pred_freq = err_rows['first_pred'].value_counts().head(10)\n",
    "    print(\"Most frequently predicted (wrong) POI:\")\n",
    "    for poi, freq in wrong_pred_freq.items():\n",
    "        print(f\"  {poi:>25s}: {freq:>3d} times\")\n",
    "    \n",
    "    # ---------------------------------------------------------------------\n",
    "    # 5. Esempio dettagliato di errore\n",
    "    # ---------------------------------------------------------------------\n",
    "    print(\"\\n5️⃣ DETAILED ERROR EXAMPLE\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Prendi il primo errore disponibile\n",
    "    sample_idx = err_rows.index[0]\n",
    "    sample = df_all.loc[sample_idx]\n",
    "    \n",
    "    print(f\"Card ID: {sample.get('card_id', 'N/A')}\")\n",
    "    print(f\"Year: {sample.get('year', 'N/A')}\")\n",
    "    print(f\"True POI: {sample['ground_truth_norm']}\")\n",
    "    print(f\"Predicted POI: {sample['prediction_norm'][0]}\")\n",
    "    print(f\"Top-5 Predictions: {sample['prediction_norm']}\")\n",
    "    print(f\"History (first 200 chars): {str(sample['history'])[:200]}...\")\n",
    "    \n",
    "    # ---------------------------------------------------------------------\n",
    "    # 6. Matrice di confusione semplificata (top POI)\n",
    "    # ---------------------------------------------------------------------\n",
    "    print(\"\\n6️⃣ CONFUSION MATRIX - TOP 10 POI\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Prendi i 10 POI più frequenti\n",
    "    top_poi = df_all['ground_truth_norm'].value_counts().head(10).index.tolist()\n",
    "    \n",
    "    # Filtra solo le predizioni che coinvolgono questi POI\n",
    "    confusion_data = df_all[\n",
    "        (df_all['ground_truth_norm'].isin(top_poi)) & \n",
    "        (df_all['prediction_norm'].str[0].isin(top_poi))\n",
    "    ].copy()\n",
    "    \n",
    "    confusion_matrix = pd.crosstab(\n",
    "        confusion_data['ground_truth_norm'],\n",
    "        confusion_data['prediction_norm'].str[0],\n",
    "        normalize='index'  # Normalizza per righe (% di errore per POI)\n",
    "    )\n",
    "    \n",
    "    print(\"Confusion matrix (% of predictions for each true POI):\")\n",
    "    print(\"Rows = True POI, Columns = Predicted POI\")\n",
    "    print(confusion_matrix.round(3))\n",
    "    \n",
    "    # ---------------------------------------------------------------------\n",
    "    # 7. Summary statistiche\n",
    "    # ---------------------------------------------------------------------\n",
    "    print(f\"\\n7️⃣ SUMMARY STATISTICS\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"Total predictions analyzed: {len(df_all):,}\")\n",
    "    print(f\"Total errors (Top-1): {len(err_rows):,} ({len(err_rows)/len(df_all):.2%})\")\n",
    "    print(f\"Unique POI in dataset: {df_all['ground_truth_norm'].nunique():,}\")\n",
    "    print(f\"Unique POI in predictions: {len({poi for preds in df_all['prediction_norm'] for poi in preds}):,}\")\n",
    "    \n",
    "    if len(csv_files) > 1:\n",
    "        error_by_year = df_all.groupby('year')['hit@1'].agg(['count', 'mean']).reset_index()\n",
    "        error_by_year['error_rate'] = 1 - error_by_year['mean']\n",
    "        print(f\"\\nError rate by year:\")\n",
    "        for _, row in error_by_year.iterrows():\n",
    "            if not pd.isna(row['year']):\n",
    "                print(f\"  {int(row['year'])}: {row['error_rate']:.2%} ({int(row['count'])} predictions)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ANALYSIS COMPLETE\")\n",
    "print(\"This analysis doesn't require LIME but provides valuable insights into prediction errors.\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}