{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🔬 Multi-Model Comparison Analysis for Tourism Mobility Prediction\n",
    "\n",
    "*Automated analysis and visualization for comparing multiple LLM models with dual prompt strategies on VeronaCard dataset*\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 Obiettivi del Notebook\n",
    "\n",
    "Questo notebook fornisce un **framework completo** per confrontare le performance di diversi modelli LLM con **due strategie di prompt** sulla predizione next-POI turistici:\n",
    "\n",
    "- **📊 Strategia Base**: Prompt senza informazioni geometriche  \n",
    "- **🌍 Strategia Geometrica**: Prompt con informazioni spaziali integrate\n",
    "\n",
    "Genera automaticamente:\n",
    "\n",
    "- **📈 Grafici comparativi affiancati** per visualizzare entrambe le strategie\n",
    "- **📊 Analisi dei miglioramenti** dovuti all'informazione geometrica\n",
    "- **🎨 Visualizzazioni side-by-side** pronte per inclusione nella tesi\n",
    "- **📋 Report automatici** con tabelle LaTeX-ready\n",
    "\n",
    "## 📁 Struttura Repository Attuale\n",
    "\n",
    "**✅ CONFIGURAZIONE AUTOMATICA**: Il notebook si adatta automaticamente alla struttura esistente:\n",
    "\n",
    "```\n",
    "LLM-Mob-As-Mobility-Interpreter/\n",
    "├── notebook/\n",
    "│   └── multi_model_comparison_analysis.ipynb  # questo notebook\n",
    "├── results/                                   # Modelli standard\n",
    "│   ├── llama3.1_8b/\n",
    "│   │   ├── base_version/        # CSV strategia base\n",
    "│   │   └── with_geom_srv_univr/ # CSV strategia geometrica\n",
    "│   ├── mixtral_8x7b/\n",
    "│   │   └── base_version/        # CSV strategia base\n",
    "│   ├── qwen2.5_7b/\n",
    "│   │   ├── base_version/        # CSV strategia base\n",
    "│   │   └── with_geom/           # CSV strategia geometrica\n",
    "│   ├── qwen2.5_14b/\n",
    "│   │   ├── base_version/        # CSV strategia base\n",
    "│   │   └── with_geom/           # CSV strategia geometrica\n",
    "│   └── DEV/                     # ⚠️ IGNORATA (file di debug)\n",
    "├── results_deepseek-coder_33b_base_version/   # DeepSeek base (directory separata)\n",
    "├── results_deepseek-coder_33b_with_geom/      # DeepSeek geom (directory separata)\n",
    "├── results_mixtr_with_geom/                   # Mixtral geom (directory separata)\n",
    "└── img/                                       # Output grafici\n",
    "    └── multi_model_comparison/\n",
    "        ├── base/           # Grafici solo strategia base\n",
    "        ├── geom/           # Grafici solo strategia geometrica\n",
    "        └── side_by_side/   # Grafici comparativi affiancati\n",
    "```\n",
    "\n",
    "## 🚀 Workflow di Utilizzo\n",
    "\n",
    "1. **✅ Auto-detection**: Il notebook rileva automaticamente le directory disponibili\n",
    "2. **🔍 Verifica dati**: Controlla che esistano file CSV in entrambe le strategie\n",
    "3. **📊 Carica ed analizza**: Processa entrambe le strategie per ogni modello\n",
    "4. **📈 Genera confronti**: Crea grafici affiancati e analisi miglioramenti\n",
    "5. **💾 Salva output**: Esporta grafici, tabelle e codice LaTeX\n",
    "\n",
    "## 🆕 Nuove Funzionalità Dual-Strategy\n",
    "\n",
    "- **Grafici 2x3 affiancati**: Strategia base sopra, geometrica sotto\n",
    "- **Barre sovrapposte**: Confronto diretto con frecce di miglioramento\n",
    "- **Analisi delta**: Grafici dei miglioramenti percentuali\n",
    "- **Radar charts**: Confronto multidimensionale per ogni modello\n",
    "- **Tabelle integrate**: Base e Geom nella stessa tabella con calcolo Δ%\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📦 Setup e Configurazione\n",
    "\n",
    "### Configurazione dei modelli da analizzare\n",
    "Modifica questa configurazione in base ai modelli che hai testato:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import ast\n",
    "import json\n",
    "import warnings\n",
    "from collections import Counter, defaultdict\n",
    "import os\n",
    "\n",
    "# Configurazione stile grafici\n",
    "plt.rcParams['figure.dpi'] = 300  # Alta qualità per la tesi\n",
    "plt.rcParams['axes.grid'] = True\n",
    "plt.rcParams['grid.alpha'] = 0.3\n",
    "plt.rcParams['font.size'] = 12\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['axes.titlesize'] = 16\n",
    "plt.rcParams['legend.fontsize'] = 11\n",
    "\n",
    "# Sopprime warning non critici\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "\n",
    "print(\"📦 Setup completato!\")\n",
    "print(f\"📊 Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configurazione modelli e colori"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CONFIGURAZIONE MODELLI - Struttura corretta basata sulla repository reale\n",
    "# ============================================================================\n",
    "\n",
    "# Configurazione basata sulla struttura reale osservata\n",
    "MODELS = {\n",
    "    'deepseek-coder_33b': {\n",
    "        'name': 'DeepSeek-Coder 33B',\n",
    "        'base_dir': '../results/deepseek-coder_33b/base_version/',  # Directory separata\n",
    "        'geom_dir': '../results/deepseek-coder_33b/with_geom/',     # Directory separata\n",
    "        'color': '#d62728',  # Rosso\n",
    "        'params': '33B',\n",
    "        'type': 'Coding'\n",
    "    },\n",
    "    'llama3.1_8b': {\n",
    "        'name': 'LLaMA 3.1 8B',\n",
    "        'base_dir': '../results/llama3.1_8b/base_version/',\n",
    "        'geom_dir': '../results/llama3.1_8b/with_geom/',\n",
    "        'color': '#1f77b4',  # Blu\n",
    "        'params': '8B',\n",
    "        'type': 'Baseline'\n",
    "    },\n",
    "    'mixtral_8x7b': {\n",
    "        'name': 'Mixtral 8x7B',\n",
    "        'base_dir': '../results/mixtral_8x7b/base_version/',\n",
    "        'geom_dir': '../results/mixtral_8x7b/with_geom/',  # Directory separata per geom\n",
    "        'color': '#ff7f0e',  # Arancione\n",
    "        'params': '8x7B (47B active)',\n",
    "        'type': 'MoE'\n",
    "    },\n",
    "    'qwen2.5_7b': {\n",
    "        'name': 'Qwen 2.5 7B',\n",
    "        'base_dir': '../results/qwen2.5_7b/base_version/',\n",
    "        'geom_dir': '../results/qwen2.5_7b/with_geom/',\n",
    "        'color': '#2ca02c',  # Verde\n",
    "        'params': '7B',\n",
    "        'type': 'Multilingual'\n",
    "    },\n",
    "    'qwen2.5_14b': {\n",
    "        'name': 'Qwen 2.5 14B',\n",
    "        'base_dir': '../results/qwen2.5_14b/base_version/',\n",
    "        'geom_dir': '../results/qwen2.5_14b/with_geom/',\n",
    "        'color': '#9467bd',  # Viola\n",
    "        'params': '14B',\n",
    "        'type': 'Multilingual-Large'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Directory per salvare i grafici - organizzate per strategie\n",
    "OUTPUT_DIRS = {\n",
    "    'comparison': '../img/multi_model_comparison/',\n",
    "    'base_comparison': '../img/multi_model_comparison/base/',\n",
    "    'geom_comparison': '../img/multi_model_comparison/geom/',\n",
    "    'side_by_side': '../img/multi_model_comparison/side_by_side/',\n",
    "    'individual': '../img/'  # + model_key per grafici specifici\n",
    "}\n",
    "\n",
    "# Crea directory se non esistono\n",
    "for dir_path in OUTPUT_DIRS.values():\n",
    "    Path(dir_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"🎨 Configurazione modelli corretta per struttura repository reale:\")\n",
    "print(\"   📊 Struttura mista: alcune directory in results/, altre separate\")\n",
    "print(\"   🚫 Directory results/DEV/ IGNORATA (file di debug)\")\n",
    "print()\n",
    "\n",
    "# Verifica esistenza directory e correzione automatica per DeepSeek\n",
    "print(\"🔍 Verifica e correzione directory configurate:\")\n",
    "\n",
    "# Controllo speciale per DeepSeek che ha directory separate dalla root\n",
    "deepseek_base_alt = '../results/deepseek-coder_33b_base_version/'\n",
    "deepseek_geom_alt = '../results/deepseek-coder_33b_with_geom/'\n",
    "\n",
    "if Path(deepseek_base_alt).exists():\n",
    "    MODELS['deepseek-coder_33b']['base_dir'] = deepseek_base_alt\n",
    "    print(\"✅ DeepSeek-Coder 33B: Utilizzando directory base_version separata\")\n",
    "\n",
    "if Path(deepseek_geom_alt).exists():\n",
    "    MODELS['deepseek-coder_33b']['geom_dir'] = deepseek_geom_alt\n",
    "    print(\"✅ DeepSeek-Coder 33B: Utilizzando directory with_geom separata\")\n",
    "\n",
    "# Controllo speciale per Mixtral che ha geom directory separata\n",
    "mixtral_geom_alt = '../results_mixtr_with_geom/'\n",
    "if Path(mixtral_geom_alt).exists():\n",
    "    MODELS['mixtral_8x7b']['geom_dir'] = mixtral_geom_alt\n",
    "    print(\"✅ Mixtral 8x7B: Utilizzando directory geom separata\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Verifica finale di tutte le directory\n",
    "for key, model in MODELS.items():\n",
    "    print(f\"  {model['name']:>20s}:\")\n",
    "    \n",
    "    base_exists = Path(model['base_dir']).exists()\n",
    "    geom_exists = Path(model['geom_dir']).exists()\n",
    "    \n",
    "    print(f\"    Base: {model['base_dir']} {'✅' if base_exists else '❌'}\")\n",
    "    print(f\"    Geom: {model['geom_dir']} {'✅' if geom_exists else '❌'}\")\n",
    "    \n",
    "    if base_exists:\n",
    "        base_files = list(Path(model['base_dir']).glob('*_pred_*.csv'))\n",
    "        print(f\"          └─ {len(base_files)} file CSV base\")\n",
    "    \n",
    "    if geom_exists:\n",
    "        geom_files = list(Path(model['geom_dir']).glob('*_pred_*.csv'))\n",
    "        print(f\"          └─ {len(geom_files)} file CSV geom\")\n",
    "\n",
    "print(f\"\\n📁 Output directories create:\")\n",
    "for name, path in OUTPUT_DIRS.items():\n",
    "    print(f\"  {name}: {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📈 1. Caricamento e Preprocessing Dati\n",
    "\n",
    "### Funzioni di utilità per il caricamento dati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def poi_id(x):\n",
    "    \"\"\"\n",
    "    Normalizza identificatori POI per confronti consistenti.\n",
    "    Gestisce dict, liste, e stringhe/numeri.\n",
    "    \"\"\"\n",
    "    if isinstance(x, dict):\n",
    "        for key in ('poi', 'poi_id', 'name', 'id'):\n",
    "            if key in x:\n",
    "                return str(x[key])\n",
    "        return json.dumps(x, sort_keys=True)\n",
    "    elif isinstance(x, (list, tuple)):\n",
    "        return tuple(map(poi_id, x))\n",
    "    else:\n",
    "        return str(x)\n",
    "\n",
    "def load_model_data(model_dir, model_name):\n",
    "    \"\"\"\n",
    "    Carica tutti i CSV di un modello e calcola le metriche.\n",
    "    \n",
    "    Args:\n",
    "        model_dir (str): Directory contenente i file CSV del modello\n",
    "        model_name (str): Nome del modello per logging\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame con tutte le predizioni e metriche calcolate\n",
    "    \"\"\"\n",
    "    print(f\"📂 Caricamento {model_name}...\")\n",
    "    \n",
    "    # Trova tutti i file CSV\n",
    "    csv_pattern = str(Path(model_dir) / '*_pred_*.csv')\n",
    "    csv_files = glob.glob(csv_pattern)\n",
    "    \n",
    "    if not csv_files:\n",
    "        print(f\"   ⚠️  Nessun file CSV trovato in {model_dir}\")\n",
    "        print(f\"       Pattern cercato: {csv_pattern}\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    print(f\"   📄 Trovati {len(csv_files)} file CSV\")\n",
    "    \n",
    "    dfs = []\n",
    "    total_rows = 0\n",
    "    \n",
    "    for csv_file in sorted(csv_files):\n",
    "        try:\n",
    "            # Caricamento con gestione errori\n",
    "            try:\n",
    "                df = pd.read_csv(csv_file)\n",
    "            except pd.errors.ParserError:\n",
    "                print(f\"   ⚠️  Parser error in {Path(csv_file).name}, utilizzando error handling...\")\n",
    "                df = pd.read_csv(csv_file, on_bad_lines='skip', engine='python')\n",
    "            \n",
    "            # Estrai anno dal nome file\n",
    "            filename = Path(csv_file).stem\n",
    "            year_token = next((part for part in filename.split('_')\n",
    "                             if part.isdigit() and len(part) == 4), None)\n",
    "            df['year'] = int(year_token) if year_token else np.nan\n",
    "            df['model'] = model_name\n",
    "            \n",
    "            # Parse prediction list\n",
    "            df['prediction_list'] = df['prediction'].apply(\n",
    "                lambda x: ast.literal_eval(x) if isinstance(x, str) else []\n",
    "            )\n",
    "            \n",
    "            # Filtra solo predizioni con esattamente 5 elementi\n",
    "            df = df[df['prediction_list'].apply(len) == 5]\n",
    "            \n",
    "            dfs.append(df)\n",
    "            total_rows += len(df)\n",
    "            print(f\"       {Path(csv_file).name}: {len(df):,} righe\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ❌ Errore caricando {Path(csv_file).name}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    if not dfs:\n",
    "        print(f\"   ❌ Nessun file caricato con successo per {model_name}\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Concatena tutti i dataframes\n",
    "    df_combined = pd.concat(dfs, ignore_index=True)\n",
    "    \n",
    "    # Normalizza POI per confronti consistenti\n",
    "    df_combined['prediction_norm'] = df_combined['prediction_list'].apply(\n",
    "        lambda lst: [poi_id(e) for e in lst]\n",
    "    )\n",
    "    df_combined['ground_truth_norm'] = df_combined['ground_truth'].apply(poi_id)\n",
    "    \n",
    "    print(f\"   ✅ {model_name}: {total_rows:,} righe totali\")\n",
    "    return df_combined\n",
    "\n",
    "def calculate_metrics(df):\n",
    "    \"\"\"\n",
    "    Calcola le metriche di valutazione standard per un DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame con colonne prediction_norm e ground_truth_norm\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dizionario con le metriche calcolate\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        return {\n",
    "            'top1_accuracy': 0.0,\n",
    "            'top5_hit_rate': 0.0,\n",
    "            'mrr': 0.0,\n",
    "            'catalogue_coverage': 0.0,\n",
    "            'total_predictions': 0,\n",
    "            'processing_time_mean': 0.0\n",
    "        }\n",
    "    \n",
    "    # Calcola hit@1\n",
    "    df['hit@1'] = df['prediction_norm'].str[0] == df['ground_truth_norm']\n",
    "    \n",
    "    # Calcola hit@5\n",
    "    df['hit@5'] = df.apply(\n",
    "        lambda row: row['ground_truth_norm'] in row['prediction_norm'][:5], axis=1\n",
    "    )\n",
    "    \n",
    "    # Calcola reciprocal rank\n",
    "    def reciprocal_rank(row, k=5):\n",
    "        try:\n",
    "            rank = row['prediction_norm'][:k].index(row['ground_truth_norm']) + 1\n",
    "            return 1.0 / rank\n",
    "        except ValueError:\n",
    "            return 0.0\n",
    "    \n",
    "    df['rr'] = df.apply(reciprocal_rank, axis=1)\n",
    "    \n",
    "    # Calcola catalogue coverage\n",
    "    coverage_set = {poi for preds in df['prediction_norm'] for poi in preds}\n",
    "    unique_ground_truth = df['ground_truth_norm'].nunique()\n",
    "    \n",
    "    # Processing time medio\n",
    "    proc_time_mean = df['processing_time'].mean() if 'processing_time' in df.columns else 0.0\n",
    "    \n",
    "    metrics = {\n",
    "        'top1_accuracy': df['hit@1'].mean(),\n",
    "        'top5_hit_rate': df['hit@5'].mean(),\n",
    "        'mrr': df['rr'].mean(),\n",
    "        'catalogue_coverage': len(coverage_set) / unique_ground_truth if unique_ground_truth > 0 else 0.0,\n",
    "        'total_predictions': len(df),\n",
    "        'processing_time_mean': proc_time_mean\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "print(\"🔧 Funzioni di utilità caricate!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caricamento dati per tutti i modelli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CARICAMENTO DATI PER TUTTI I MODELLI - VERSIONE DOPPIA STRATEGIA\n",
    "# ============================================================================\n",
    "\n",
    "print(\"🚀 Avvio caricamento dati multi-modello con doppia strategia...\\n\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Dizionari per contenere tutti i dati divisi per strategia\n",
    "model_data_base = {}\n",
    "model_data_geom = {}\n",
    "model_metrics_base = {}\n",
    "model_metrics_geom = {}\n",
    "successfully_loaded_base = []\n",
    "successfully_loaded_geom = []\n",
    "\n",
    "# Carica dati per ogni modello (entrambe le strategie)\n",
    "for model_key, model_config in MODELS.items():\n",
    "    print(f\"\\n📊 MODELLO: {model_config['name']}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # ==================== CARICA VERSIONE BASE ====================\n",
    "    print(f\"📈 Caricamento strategia BASE:\")\n",
    "    base_dir = Path(model_config['base_dir'])\n",
    "    if not base_dir.exists():\n",
    "        print(f\"   ❌ Directory BASE non trovata: {base_dir}\")\n",
    "    else:\n",
    "        df_base = load_model_data(model_config['base_dir'], f\"{model_config['name']} (Base)\")\n",
    "        if not df_base.empty:\n",
    "            metrics_base = calculate_metrics(df_base)\n",
    "            model_data_base[model_key] = df_base\n",
    "            model_metrics_base[model_key] = metrics_base\n",
    "            successfully_loaded_base.append(model_key)\n",
    "            \n",
    "            print(f\"   ✅ Base - Metriche:\")\n",
    "            print(f\"       Top-1 Accuracy: {metrics_base['top1_accuracy']:.3f} ({metrics_base['top1_accuracy']*100:.1f}%)\")\n",
    "            print(f\"       Top-5 Hit Rate: {metrics_base['top5_hit_rate']:.3f} ({metrics_base['top5_hit_rate']*100:.1f}%)\")\n",
    "            print(f\"       MRR:            {metrics_base['mrr']:.3f}\")\n",
    "            print(f\"       Predizioni:     {metrics_base['total_predictions']:,}\")\n",
    "    \n",
    "    # ==================== CARICA VERSIONE GEOM ====================\n",
    "    print(f\"🌍 Caricamento strategia GEOM:\")\n",
    "    geom_dir = Path(model_config['geom_dir'])\n",
    "    if not geom_dir.exists():\n",
    "        print(f\"   ❌ Directory GEOM non trovata: {geom_dir}\")\n",
    "    else:\n",
    "        df_geom = load_model_data(model_config['geom_dir'], f\"{model_config['name']} (Geom)\")\n",
    "        if not df_geom.empty:\n",
    "            metrics_geom = calculate_metrics(df_geom)\n",
    "            model_data_geom[model_key] = df_geom\n",
    "            model_metrics_geom[model_key] = metrics_geom\n",
    "            successfully_loaded_geom.append(model_key)\n",
    "            \n",
    "            print(f\"   ✅ Geom - Metriche:\")\n",
    "            print(f\"       Top-1 Accuracy: {metrics_geom['top1_accuracy']:.3f} ({metrics_geom['top1_accuracy']*100:.1f}%)\")\n",
    "            print(f\"       Top-5 Hit Rate: {metrics_geom['top5_hit_rate']:.3f} ({metrics_geom['top5_hit_rate']*100:.1f}%)\")\n",
    "            print(f\"       MRR:            {metrics_geom['mrr']:.3f}\")\n",
    "            print(f\"       Predizioni:     {metrics_geom['total_predictions']:,}\")\n",
    "    \n",
    "    # ==================== CONFRONTO DIRETTO ====================\n",
    "    if model_key in successfully_loaded_base and model_key in successfully_loaded_geom:\n",
    "        print(f\"   📊 Confronto Base vs Geom:\")\n",
    "        \n",
    "        # Calcola miglioramenti\n",
    "        top1_improvement = (metrics_geom['top1_accuracy'] - metrics_base['top1_accuracy']) * 100\n",
    "        top5_improvement = (metrics_geom['top5_hit_rate'] - metrics_base['top5_hit_rate']) * 100\n",
    "        mrr_improvement = (metrics_geom['mrr'] - metrics_base['mrr']) * 100\n",
    "        \n",
    "        print(f\"       Top-1 Δ: {top1_improvement:+.1f}% {'✅' if top1_improvement > 0 else '❌'}\")\n",
    "        print(f\"       Top-5 Δ: {top5_improvement:+.1f}% {'✅' if top5_improvement > 0 else '❌'}\")\n",
    "        print(f\"       MRR Δ:   {mrr_improvement:+.1f}% {'✅' if mrr_improvement > 0 else '❌'}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"📋 RIEPILOGO CARICAMENTO DOPPIA STRATEGIA:\")\n",
    "print(f\"   Modelli configurati:     {len(MODELS)}\")\n",
    "print(f\"   Modelli caricati (Base): {len(successfully_loaded_base)}\")\n",
    "print(f\"   Modelli caricati (Geom): {len(successfully_loaded_geom)}\")\n",
    "print(f\"   Modelli con entrambe:    {len(set(successfully_loaded_base) & set(successfully_loaded_geom))}\")\n",
    "\n",
    "# Identifica modelli disponibili per entrambe le strategie\n",
    "models_both_strategies = list(set(successfully_loaded_base) & set(successfully_loaded_geom))\n",
    "print(f\"   Modelli completi: {', '.join([MODELS[k]['name'] for k in models_both_strategies])}\")\n",
    "\n",
    "if len(models_both_strategies) < 1:\n",
    "    print(\"\\n⚠️  ATTENZIONE: Nessun modello ha entrambe le strategie!\")\n",
    "    print(\"   Verifica la struttura delle directory dei risultati\")\n",
    "else:\n",
    "    print(f\"\\n✅ Pronto per analisi comparativa con {len(models_both_strategies)} modelli completi!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📊 2. Creazione Tabella Comparativa Principale\n",
    "\n",
    "Genera la tabella di performance comparative che sarà integrata nel LaTeX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# TABELLA COMPARATIVA DOPPIA STRATEGIA\n",
    "# ============================================================================\n",
    "\n",
    "if len(models_both_strategies) == 0:\n",
    "    print(\"❌ Nessun modello con entrambe le strategie. Saltando creazione tabella.\")\n",
    "else:\n",
    "    print(\"📋 Creazione tabella comparativa doppia strategia...\")\n",
    "    \n",
    "    # Prepara dati per la tabella comparativa\n",
    "    comparison_data = []\n",
    "    \n",
    "    for model_key in models_both_strategies:\n",
    "        model_config = MODELS[model_key]\n",
    "        metrics_base = model_metrics_base[model_key]\n",
    "        metrics_geom = model_metrics_geom[model_key]\n",
    "        \n",
    "        # Calcola miglioramenti\n",
    "        top1_improvement = (metrics_geom['top1_accuracy'] - metrics_base['top1_accuracy']) * 100\n",
    "        top5_improvement = (metrics_geom['top5_hit_rate'] - metrics_base['top5_hit_rate']) * 100\n",
    "        mrr_improvement = (metrics_geom['mrr'] - metrics_base['mrr']) * 100\n",
    "        \n",
    "        # Riga per strategia BASE\n",
    "        comparison_data.append({\n",
    "            'Modello': model_config['name'],\n",
    "            'Strategia': 'Base',\n",
    "            'Parametri': model_config['params'],\n",
    "            'Top-1 Acc.': f\"{metrics_base['top1_accuracy']*100:.1f}%\",\n",
    "            'Top-5 HR': f\"{metrics_base['top5_hit_rate']*100:.1f}%\",\n",
    "            'MRR': f\"{metrics_base['mrr']*100:.1f}%\",\n",
    "            'Coverage': f\"{metrics_base['catalogue_coverage']:.2f}\",\n",
    "            'Predizioni': f\"{metrics_base['total_predictions']:,}\",\n",
    "            'Δ Top-1': f\"—\",\n",
    "            'Δ Top-5': f\"—\",\n",
    "            'Δ MRR': f\"—\"\n",
    "        })\n",
    "        \n",
    "        # Riga per strategia GEOM\n",
    "        comparison_data.append({\n",
    "            'Modello': model_config['name'],\n",
    "            'Strategia': 'Geom',\n",
    "            'Parametri': model_config['params'],\n",
    "            'Top-1 Acc.': f\"{metrics_geom['top1_accuracy']*100:.1f}%\",\n",
    "            'Top-5 HR': f\"{metrics_geom['top5_hit_rate']*100:.1f}%\",\n",
    "            'MRR': f\"{metrics_geom['mrr']*100:.1f}%\",\n",
    "            'Coverage': f\"{metrics_geom['catalogue_coverage']:.2f}\",\n",
    "            'Predizioni': f\"{metrics_geom['total_predictions']:,}\",\n",
    "            'Δ Top-1': f\"{top1_improvement:+.1f}%\",\n",
    "            'Δ Top-5': f\"{top5_improvement:+.1f}%\",\n",
    "            'Δ MRR': f\"{mrr_improvement:+.1f}%\"\n",
    "        })\n",
    "    \n",
    "    # Crea DataFrame per display\n",
    "    comparison_df = pd.DataFrame(comparison_data)\n",
    "    \n",
    "    print(\"\\n📊 TABELLA COMPARATIVA DOPPIA STRATEGIA:\")\n",
    "    print(\"=\" * 120)\n",
    "    print(comparison_df.to_string(index=False))\n",
    "    print(\"=\" * 120)\n",
    "    \n",
    "    # Salva la tabella per uso in LaTeX\n",
    "    latex_table_path = Path(OUTPUT_DIRS['comparison']) / 'dual_strategy_table.csv'\n",
    "    comparison_df.to_csv(latex_table_path, index=False)\n",
    "    print(f\"\\n💾 Tabella salvata in: {latex_table_path}\")\n",
    "    \n",
    "    # Genera codice LaTeX pronto all'uso\n",
    "    latex_code_path = Path(OUTPUT_DIRS['comparison']) / 'dual_strategy_table.tex'\n",
    "    \n",
    "    with open(latex_code_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(\"% Tabella generata automaticamente - Doppia Strategia\\n\")\n",
    "        f.write(\"\\\\begin{table}[H]\\n\")\n",
    "        f.write(\"\\\\centering\\n\")\n",
    "        f.write(\"\\\\caption{Confronto Performance: Strategia Base vs Geometrica}\\n\")\n",
    "        f.write(\"\\\\label{tab:dual_strategy_comparison}\\n\")\n",
    "        f.write(\"\\\\begin{tabular}{llcccccccc}\\n\")\n",
    "        f.write(\"\\\\toprule\\n\")\n",
    "        f.write(\"\\\\textbf{Modello} & \\\\textbf{Strategia} & \\\\textbf{Parametri} & \\\\textbf{Top-1} & \\\\textbf{Top-5} & \\\\textbf{MRR} & \\\\textbf{Δ Top-1} & \\\\textbf{Δ Top-5} & \\\\textbf{Δ MRR} \\\\\\\\\\n\")\n",
    "        f.write(\"\\\\midrule\\n\")\n",
    "        \n",
    "        current_model = None\n",
    "        for _, row in comparison_df.iterrows():\n",
    "            if current_model != row['Modello']:\n",
    "                if current_model is not None:\n",
    "                    f.write(\"\\\\midrule\\n\")\n",
    "                current_model = row['Modello']\n",
    "            \n",
    "            model_name = row['Modello'] if row['Strategia'] == 'Base' else ''\n",
    "            f.write(f\"{model_name} & {row['Strategia']} & {row['Parametri']} & {row['Top-1 Acc.']} & {row['Top-5 HR']} & {row['MRR']} & {row['Δ Top-1']} & {row['Δ Top-5']} & {row['Δ MRR']} \\\\\\\\\\n\")\n",
    "        \n",
    "        f.write(\"\\\\bottomrule\\n\")\n",
    "        f.write(\"\\\\end{tabular}\\n\")\n",
    "        f.write(\"\\\\end{table}\\n\")\n",
    "    \n",
    "    print(f\"📄 Codice LaTeX salvato in: {latex_code_path}\")\n",
    "    \n",
    "    # Analisi miglioramenti con geometria\n",
    "    print(\"\\n📈 ANALISI MIGLIORAMENTI CON GEOMETRIA:\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    total_improvements = {'top1': 0, 'top5': 0, 'mrr': 0}\n",
    "    positive_improvements = {'top1': 0, 'top5': 0, 'mrr': 0}\n",
    "    \n",
    "    for model_key in models_both_strategies:\n",
    "        model_name = MODELS[model_key]['name']\n",
    "        metrics_base = model_metrics_base[model_key]\n",
    "        metrics_geom = model_metrics_geom[model_key]\n",
    "        \n",
    "        top1_imp = (metrics_geom['top1_accuracy'] - metrics_base['top1_accuracy']) * 100\n",
    "        top5_imp = (metrics_geom['top5_hit_rate'] - metrics_base['top5_hit_rate']) * 100\n",
    "        mrr_imp = (metrics_geom['mrr'] - metrics_base['mrr']) * 100\n",
    "        \n",
    "        total_improvements['top1'] += top1_imp\n",
    "        total_improvements['top5'] += top5_imp  \n",
    "        total_improvements['mrr'] += mrr_imp\n",
    "        \n",
    "        if top1_imp > 0: positive_improvements['top1'] += 1\n",
    "        if top5_imp > 0: positive_improvements['top5'] += 1\n",
    "        if mrr_imp > 0: positive_improvements['mrr'] += 1\n",
    "        \n",
    "        print(f\"{model_name:>20s}: Top-1 {top1_imp:+.1f}%, Top-5 {top5_imp:+.1f}%, MRR {mrr_imp:+.1f}%\")\n",
    "    \n",
    "    # Media miglioramenti\n",
    "    n_models = len(models_both_strategies)\n",
    "    print(f\"\\n🎯 MEDIA MIGLIORAMENTI:\")\n",
    "    print(f\"   Top-1 Accuracy: {total_improvements['top1']/n_models:+.1f}% (positivi: {positive_improvements['top1']}/{n_models})\")\n",
    "    print(f\"   Top-5 Hit Rate: {total_improvements['top5']/n_models:+.1f}% (positivi: {positive_improvements['top5']}/{n_models})\")\n",
    "    print(f\"   MRR:            {total_improvements['mrr']/n_models:+.1f}% (positivi: {positive_improvements['mrr']}/{n_models})\")\n",
    "    \n",
    "    # Identifica best performers per strategia\n",
    "    print(f\"\\n🏆 BEST PERFORMERS PER STRATEGIA:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Base\n",
    "    best_base_top1 = max(models_both_strategies, key=lambda k: model_metrics_base[k]['top1_accuracy'])\n",
    "    best_base_mrr = max(models_both_strategies, key=lambda k: model_metrics_base[k]['mrr'])\n",
    "    \n",
    "    print(f\"BASE - Top-1: {MODELS[best_base_top1]['name']} ({model_metrics_base[best_base_top1]['top1_accuracy']*100:.1f}%)\")\n",
    "    print(f\"BASE - MRR:   {MODELS[best_base_mrr]['name']} ({model_metrics_base[best_base_mrr]['mrr']*100:.1f}%)\")\n",
    "    \n",
    "    # Geom\n",
    "    best_geom_top1 = max(models_both_strategies, key=lambda k: model_metrics_geom[k]['top1_accuracy'])\n",
    "    best_geom_mrr = max(models_both_strategies, key=lambda k: model_metrics_geom[k]['mrr'])\n",
    "    \n",
    "    print(f\"GEOM - Top-1: {MODELS[best_geom_top1]['name']} ({model_metrics_geom[best_geom_top1]['top1_accuracy']*100:.1f}%)\")\n",
    "    print(f\"GEOM - MRR:   {MODELS[best_geom_mrr]['name']} ({model_metrics_geom[best_geom_mrr]['mrr']*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎨 3. Generazione Grafici Comparativi Principali\n",
    "\n",
    "### 3.1 Grafico a Barre Comparativo Principale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# GRAFICI COMPARATIVI AFFIANCATI - BASE vs GEOM\n",
    "# ============================================================================\n",
    "\n",
    "if len(models_both_strategies) < 1:\n",
    "    print(\"⏭️  Saltando grafici affiancati: serve almeno 1 modello con entrambe le strategie\")\n",
    "else:\n",
    "    print(\"🎨 Creazione grafici comparativi affiancati (Base vs Geom)...\")\n",
    "    \n",
    "    # Prepara i dati per entrambe le strategie\n",
    "    models_list = [MODELS[k]['name'] for k in models_both_strategies]\n",
    "    colors_list = [MODELS[k]['color'] for k in models_both_strategies]\n",
    "    \n",
    "    # Metriche per entrambe le strategie\n",
    "    base_top1 = [model_metrics_base[k]['top1_accuracy'] * 100 for k in models_both_strategies]\n",
    "    geom_top1 = [model_metrics_geom[k]['top1_accuracy'] * 100 for k in models_both_strategies]\n",
    "    \n",
    "    base_top5 = [model_metrics_base[k]['top5_hit_rate'] * 100 for k in models_both_strategies]\n",
    "    geom_top5 = [model_metrics_geom[k]['top5_hit_rate'] * 100 for k in models_both_strategies]\n",
    "    \n",
    "    base_mrr = [model_metrics_base[k]['mrr'] * 100 for k in models_both_strategies]\n",
    "    geom_mrr = [model_metrics_geom[k]['mrr'] * 100 for k in models_both_strategies]\n",
    "    \n",
    "    # ==================== GRAFICO COMPARATIVO PRINCIPALE AFFIANCATO ====================\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "    fig.suptitle('Performance Comparison: Base Strategy vs Geometric Strategy', fontsize=20, fontweight='bold')\n",
    "    \n",
    "    # RIGA SUPERIORE: Strategia BASE\n",
    "    # Top-1 Accuracy BASE\n",
    "    bars1 = axes[0,0].bar(models_list, base_top1, color=colors_list, alpha=0.7, \n",
    "                         edgecolor='black', linewidth=0.5, label='Base Strategy')\n",
    "    axes[0,0].set_title('Top-1 Accuracy - Base Strategy', fontsize=14, fontweight='bold')\n",
    "    axes[0,0].set_ylabel('Accuracy (%)', fontsize=12)\n",
    "    axes[0,0].tick_params(axis='x', rotation=45)\n",
    "    axes[0,0].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    for bar, value in zip(bars1, base_top1):\n",
    "        axes[0,0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5, \n",
    "                      f'{value:.1f}%', ha='center', va='bottom', fontweight='bold', fontsize=9)\n",
    "    \n",
    "    # Top-5 Hit Rate BASE\n",
    "    bars2 = axes[0,1].bar(models_list, base_top5, color=colors_list, alpha=0.7, \n",
    "                         edgecolor='black', linewidth=0.5, label='Base Strategy')\n",
    "    axes[0,1].set_title('Top-5 Hit Rate - Base Strategy', fontsize=14, fontweight='bold')\n",
    "    axes[0,1].set_ylabel('Hit Rate (%)', fontsize=12)\n",
    "    axes[0,1].tick_params(axis='x', rotation=45)\n",
    "    axes[0,1].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    for bar, value in zip(bars2, base_top5):\n",
    "        axes[0,1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1, \n",
    "                      f'{value:.1f}%', ha='center', va='bottom', fontweight='bold', fontsize=9)\n",
    "    \n",
    "    # MRR BASE\n",
    "    bars3 = axes[0,2].bar(models_list, base_mrr, color=colors_list, alpha=0.7, \n",
    "                         edgecolor='black', linewidth=0.5, label='Base Strategy')\n",
    "    axes[0,2].set_title('Mean Reciprocal Rank - Base Strategy', fontsize=14, fontweight='bold')\n",
    "    axes[0,2].set_ylabel('MRR (%)', fontsize=12)\n",
    "    axes[0,2].tick_params(axis='x', rotation=45)\n",
    "    axes[0,2].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    for bar, value in zip(bars3, base_mrr):\n",
    "        axes[0,2].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5, \n",
    "                      f'{value:.1f}%', ha='center', va='bottom', fontweight='bold', fontsize=9)\n",
    "    \n",
    "    # RIGA INFERIORE: Strategia GEOM\n",
    "    # Top-1 Accuracy GEOM\n",
    "    bars4 = axes[1,0].bar(models_list, geom_top1, color=colors_list, alpha=0.9, \n",
    "                         edgecolor='black', linewidth=0.5, label='Geometric Strategy')\n",
    "    axes[1,0].set_title('Top-1 Accuracy - Geometric Strategy', fontsize=14, fontweight='bold')\n",
    "    axes[1,0].set_ylabel('Accuracy (%)', fontsize=12)\n",
    "    axes[1,0].tick_params(axis='x', rotation=45)\n",
    "    axes[1,0].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    for bar, value in zip(bars4, geom_top1):\n",
    "        axes[1,0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5, \n",
    "                      f'{value:.1f}%', ha='center', va='bottom', fontweight='bold', fontsize=9)\n",
    "    \n",
    "    # Top-5 Hit Rate GEOM\n",
    "    bars5 = axes[1,1].bar(models_list, geom_top5, color=colors_list, alpha=0.9, \n",
    "                         edgecolor='black', linewidth=0.5, label='Geometric Strategy')\n",
    "    axes[1,1].set_title('Top-5 Hit Rate - Geometric Strategy', fontsize=14, fontweight='bold')\n",
    "    axes[1,1].set_ylabel('Hit Rate (%)', fontsize=12)\n",
    "    axes[1,1].tick_params(axis='x', rotation=45)\n",
    "    axes[1,1].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    for bar, value in zip(bars5, geom_top5):\n",
    "        axes[1,1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1, \n",
    "                      f'{value:.1f}%', ha='center', va='bottom', fontweight='bold', fontsize=9)\n",
    "    \n",
    "    # MRR GEOM\n",
    "    bars6 = axes[1,2].bar(models_list, geom_mrr, color=colors_list, alpha=0.9, \n",
    "                         edgecolor='black', linewidth=0.5, label='Geometric Strategy')\n",
    "    axes[1,2].set_title('Mean Reciprocal Rank - Geometric Strategy', fontsize=14, fontweight='bold')\n",
    "    axes[1,2].set_ylabel('MRR (%)', fontsize=12)\n",
    "    axes[1,2].tick_params(axis='x', rotation=45)\n",
    "    axes[1,2].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    for bar, value in zip(bars6, geom_mrr):\n",
    "        axes[1,2].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5, \n",
    "                      f'{value:.1f}%', ha='center', va='bottom', fontweight='bold', fontsize=9)\n",
    "    \n",
    "    # Aggiungi linee divisorie visive\n",
    "    for ax in axes.flat:\n",
    "        ax.set_ylim(bottom=0)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Salva il grafico\n",
    "    output_path = Path(OUTPUT_DIRS['side_by_side']) / 'base_vs_geom_comparison.png'\n",
    "    plt.savefig(output_path, dpi=300, bbox_inches='tight', facecolor='white')\n",
    "    print(f\"💾 Salvato: {output_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # ==================== GRAFICI SOVRAPPOSTI PER CONFRONTO DIRETTO ====================\n",
    "    print(\"\\n🎨 Creazione grafici sovrapposti per confronto diretto...\")\n",
    "    \n",
    "    metrics_info = [\n",
    "        ('Top-1 Accuracy', base_top1, geom_top1, 'top1_base_vs_geom_overlaid.png'),\n",
    "        ('Top-5 Hit Rate', base_top5, geom_top5, 'top5_base_vs_geom_overlaid.png'),\n",
    "        ('Mean Reciprocal Rank', base_mrr, geom_mrr, 'mrr_base_vs_geom_overlaid.png')\n",
    "    ]\n",
    "    \n",
    "    x = np.arange(len(models_list))\n",
    "    width = 0.35\n",
    "    \n",
    "    for metric_name, base_values, geom_values, filename in metrics_info:\n",
    "        fig, ax = plt.subplots(figsize=(12, 8))\n",
    "        \n",
    "        # Barre affiancate\n",
    "        bars1 = ax.bar(x - width/2, base_values, width, label='Base Strategy', \n",
    "                      color=[c + '80' for c in colors_list], edgecolor='black', linewidth=0.5)\n",
    "        bars2 = ax.bar(x + width/2, geom_values, width, label='Geometric Strategy', \n",
    "                      color=colors_list, edgecolor='black', linewidth=0.5)\n",
    "        \n",
    "        ax.set_xlabel('Models', fontsize=12)\n",
    "        ax.set_ylabel(f'{metric_name} (%)', fontsize=12)\n",
    "        ax.set_title(f'{metric_name}: Base vs Geometric Strategy Comparison', \n",
    "                    fontsize=16, fontweight='bold')\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(models_list, rotation=45, ha='right')\n",
    "        ax.legend(loc='upper left', fontsize=11)\n",
    "        ax.grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        # Aggiungi valori sopra le barre\n",
    "        for bar, value in zip(bars1, base_values):\n",
    "            ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(base_values)*0.01, \n",
    "                   f'{value:.1f}%', ha='center', va='bottom', fontsize=9)\n",
    "        \n",
    "        for bar, value in zip(bars2, geom_values):\n",
    "            ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(geom_values)*0.01, \n",
    "                   f'{value:.1f}%', ha='center', va='bottom', fontsize=9)\n",
    "        \n",
    "        # Aggiungi frecce di miglioramento\n",
    "        for i, (base_val, geom_val) in enumerate(zip(base_values, geom_values)):\n",
    "            if geom_val > base_val:\n",
    "                ax.annotate('', xy=(i + width/2, geom_val), xytext=(i - width/2, base_val),\n",
    "                          arrowprops=dict(arrowstyle='->', color='green', lw=1.5, alpha=0.7))\n",
    "            elif geom_val < base_val:\n",
    "                ax.annotate('', xy=(i + width/2, geom_val), xytext=(i - width/2, base_val),\n",
    "                          arrowprops=dict(arrowstyle='->', color='red', lw=1.5, alpha=0.7))\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Salva\n",
    "        output_path = Path(OUTPUT_DIRS['side_by_side']) / filename\n",
    "        plt.savefig(output_path, dpi=300, bbox_inches='tight', facecolor='white')\n",
    "        print(f\"💾 Salvato: {output_path}\")\n",
    "        \n",
    "        plt.show()\n",
    "    \n",
    "    print(\"✅ Grafici comparativi affiancati completati!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Scatter Plot Performance vs Efficienza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# GRAFICI DI MIGLIORAMENTO E DELTA ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "if len(models_both_strategies) < 1:\n",
    "    print(\"⏭️  Saltando analisi miglioramenti: serve almeno 1 modello con entrambe le strategie\")\n",
    "else:\n",
    "    print(\"📈 Creazione grafici di analisi miglioramenti con strategia geometrica...\")\n",
    "    \n",
    "    # Calcola i delta per ogni modello\n",
    "    model_deltas = {}\n",
    "    for model_key in models_both_strategies:\n",
    "        base_metrics = model_metrics_base[model_key]\n",
    "        geom_metrics = model_metrics_geom[model_key]\n",
    "        \n",
    "        model_deltas[model_key] = {\n",
    "            'top1_delta': (geom_metrics['top1_accuracy'] - base_metrics['top1_accuracy']) * 100,\n",
    "            'top5_delta': (geom_metrics['top5_hit_rate'] - base_metrics['top5_hit_rate']) * 100,\n",
    "            'mrr_delta': (geom_metrics['mrr'] - base_metrics['mrr']) * 100,\n",
    "            'coverage_delta': (geom_metrics['catalogue_coverage'] - base_metrics['catalogue_coverage'])\n",
    "        }\n",
    "    \n",
    "    # ==================== GRAFICO DELTA IMPROVEMENTS ====================\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    fig.suptitle('Performance Improvements with Geometric Information', fontsize=20, fontweight='bold')\n",
    "    \n",
    "    models_list = [MODELS[k]['name'] for k in models_both_strategies]\n",
    "    colors_list = [MODELS[k]['color'] for k in models_both_strategies]\n",
    "    \n",
    "    # Delta Top-1\n",
    "    top1_deltas = [model_deltas[k]['top1_delta'] for k in models_both_strategies]\n",
    "    bars1 = ax1.bar(models_list, top1_deltas, color=colors_list, alpha=0.8, \n",
    "                   edgecolor='black', linewidth=0.5)\n",
    "    ax1.set_title('Top-1 Accuracy Improvement (Δ%)', fontsize=14, fontweight='bold')\n",
    "    ax1.set_ylabel('Improvement (%)', fontsize=12)\n",
    "    ax1.tick_params(axis='x', rotation=45)\n",
    "    ax1.grid(True, alpha=0.3, axis='y')\n",
    "    ax1.axhline(y=0, color='red', linestyle='--', alpha=0.7, linewidth=1)\n",
    "    \n",
    "    for bar, value in zip(bars1, top1_deltas):\n",
    "        color = 'green' if value > 0 else 'red'\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + (0.1 if value > 0 else -0.1), \n",
    "                f'{value:+.1f}%', ha='center', va='bottom' if value > 0 else 'top', \n",
    "                fontweight='bold', color=color, fontsize=10)\n",
    "    \n",
    "    # Delta Top-5\n",
    "    top5_deltas = [model_deltas[k]['top5_delta'] for k in models_both_strategies]\n",
    "    bars2 = ax2.bar(models_list, top5_deltas, color=colors_list, alpha=0.8, \n",
    "                   edgecolor='black', linewidth=0.5)\n",
    "    ax2.set_title('Top-5 Hit Rate Improvement (Δ%)', fontsize=14, fontweight='bold')\n",
    "    ax2.set_ylabel('Improvement (%)', fontsize=12)\n",
    "    ax2.tick_params(axis='x', rotation=45)\n",
    "    ax2.grid(True, alpha=0.3, axis='y')\n",
    "    ax2.axhline(y=0, color='red', linestyle='--', alpha=0.7, linewidth=1)\n",
    "    \n",
    "    for bar, value in zip(bars2, top5_deltas):\n",
    "        color = 'green' if value > 0 else 'red'\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + (0.1 if value > 0 else -0.1), \n",
    "                f'{value:+.1f}%', ha='center', va='bottom' if value > 0 else 'top', \n",
    "                fontweight='bold', color=color, fontsize=10)\n",
    "    \n",
    "    # Delta MRR\n",
    "    mrr_deltas = [model_deltas[k]['mrr_delta'] for k in models_both_strategies]\n",
    "    bars3 = ax3.bar(models_list, mrr_deltas, color=colors_list, alpha=0.8, \n",
    "                   edgecolor='black', linewidth=0.5)\n",
    "    ax3.set_title('MRR Improvement (Δ%)', fontsize=14, fontweight='bold')\n",
    "    ax3.set_ylabel('Improvement (%)', fontsize=12)\n",
    "    ax3.tick_params(axis='x', rotation=45)\n",
    "    ax3.grid(True, alpha=0.3, axis='y')\n",
    "    ax3.axhline(y=0, color='red', linestyle='--', alpha=0.7, linewidth=1)\n",
    "    \n",
    "    for bar, value in zip(bars3, mrr_deltas):\n",
    "        color = 'green' if value > 0 else 'red'\n",
    "        ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + (0.1 if value > 0 else -0.1), \n",
    "                f'{value:+.1f}%', ha='center', va='bottom' if value > 0 else 'top', \n",
    "                fontweight='bold', color=color, fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Salva\n",
    "    output_path = Path(OUTPUT_DIRS['side_by_side']) / 'geometric_improvements_delta.png'\n",
    "    plt.savefig(output_path, dpi=300, bbox_inches='tight', facecolor='white')\n",
    "    print(f\"💾 Salvato: {output_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # ==================== RADAR CHART COMPARISON ====================\n",
    "    print(\"\\n🎨 Creazione radar chart per confronto multi-dimensionale...\")\n",
    "    \n",
    "    # Prepara dati per radar chart\n",
    "    metrics_labels = ['Top-1\\\\nAccuracy', 'Top-5\\\\nHit Rate', 'Mean Reciprocal\\\\nRank', 'Catalogue\\\\nCoverage']\n",
    "    \n",
    "    fig, axes = plt.subplots(1, len(models_both_strategies), figsize=(6 * len(models_both_strategies), 6), \n",
    "                            subplot_kw=dict(projection='polar'))\n",
    "    \n",
    "    if len(models_both_strategies) == 1:\n",
    "        axes = [axes]  # Assicura che sia sempre una lista\n",
    "    \n",
    "    fig.suptitle('Multi-Dimensional Performance Comparison: Base vs Geometric', \n",
    "                fontsize=16, fontweight='bold')\n",
    "    \n",
    "    for i, model_key in enumerate(models_both_strategies):\n",
    "        ax = axes[i]\n",
    "        model_config = MODELS[model_key]\n",
    "        \n",
    "        # Dati base e geom normalizzati (0-100)\n",
    "        base_metrics = model_metrics_base[model_key]\n",
    "        geom_metrics = model_metrics_geom[model_key]\n",
    "        \n",
    "        base_values = [\n",
    "            base_metrics['top1_accuracy'] * 100,\n",
    "            base_metrics['top5_hit_rate'] * 100,\n",
    "            base_metrics['mrr'] * 100,\n",
    "            min(base_metrics['catalogue_coverage'] * 100, 100)  # Cap at 100%\n",
    "        ]\n",
    "        \n",
    "        geom_values = [\n",
    "            geom_metrics['top1_accuracy'] * 100,\n",
    "            geom_metrics['top5_hit_rate'] * 100,\n",
    "            geom_metrics['mrr'] * 100,\n",
    "            min(geom_metrics['catalogue_coverage'] * 100, 100)  # Cap at 100%\n",
    "        ]\n",
    "        \n",
    "        # Angoli per il radar chart\n",
    "        angles = np.linspace(0, 2 * np.pi, len(metrics_labels), endpoint=False).tolist()\n",
    "        angles += angles[:1]  # Chiude il cerchio\n",
    "        base_values += base_values[:1]\n",
    "        geom_values += geom_values[:1]\n",
    "        \n",
    "        # Plot\n",
    "        ax.plot(angles, base_values, 'o-', linewidth=2, label='Base Strategy', \n",
    "               color=model_config['color'], alpha=0.7)\n",
    "        ax.fill(angles, base_values, alpha=0.25, color=model_config['color'])\n",
    "        \n",
    "        ax.plot(angles, geom_values, 's-', linewidth=2, label='Geometric Strategy', \n",
    "               color=model_config['color'])\n",
    "        ax.fill(angles, geom_values, alpha=0.25, color=model_config['color'])\n",
    "        \n",
    "        # Configurazione\n",
    "        ax.set_xticks(angles[:-1])\n",
    "        ax.set_xticklabels(metrics_labels, fontsize=10)\n",
    "        ax.set_ylim(0, 100)\n",
    "        ax.set_title(f'{model_config[\"name\"]}', fontsize=14, fontweight='bold', pad=20)\n",
    "        ax.legend(loc='upper right', bbox_to_anchor=(0.1, 0.1))\n",
    "        ax.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Salva\n",
    "    output_path = Path(OUTPUT_DIRS['side_by_side']) / 'radar_chart_base_vs_geom.png'\n",
    "    plt.savefig(output_path, dpi=300, bbox_inches='tight', facecolor='white')\n",
    "    print(f\"💾 Salvato: {output_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # ==================== SUMMARY IMPROVEMENT ANALYSIS ====================\n",
    "    print(\"\\n📊 SUMMARY ANALYSIS - Impatto della Strategia Geometrica:\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Calcola statistiche aggregate\n",
    "    all_top1_deltas = [model_deltas[k]['top1_delta'] for k in models_both_strategies]\n",
    "    all_top5_deltas = [model_deltas[k]['top5_delta'] for k in models_both_strategies]\n",
    "    all_mrr_deltas = [model_deltas[k]['mrr_delta'] for k in models_both_strategies]\n",
    "    \n",
    "    print(f\"Miglioramenti medi con strategia geometrica:\")\n",
    "    print(f\"  📈 Top-1 Accuracy: {np.mean(all_top1_deltas):+.2f}% (±{np.std(all_top1_deltas):.2f}%)\")\n",
    "    print(f\"  📈 Top-5 Hit Rate: {np.mean(all_top5_deltas):+.2f}% (±{np.std(all_top5_deltas):.2f}%)\")\n",
    "    print(f\"  📈 MRR:            {np.mean(all_mrr_deltas):+.2f}% (±{np.std(all_mrr_deltas):.2f}%)\")\n",
    "    \n",
    "    # Conteggio miglioramenti positivi\n",
    "    positive_top1 = sum(1 for d in all_top1_deltas if d > 0)\n",
    "    positive_top5 = sum(1 for d in all_top5_deltas if d > 0)\n",
    "    positive_mrr = sum(1 for d in all_mrr_deltas if d > 0)\n",
    "    \n",
    "    n_models = len(models_both_strategies)\n",
    "    print(f\"\\nModelli con miglioramenti positivi:\")\n",
    "    print(f\"  ✅ Top-1 Accuracy: {positive_top1}/{n_models} ({positive_top1/n_models*100:.1f}%)\")\n",
    "    print(f\"  ✅ Top-5 Hit Rate: {positive_top5}/{n_models} ({positive_top5/n_models*100:.1f}%)\")\n",
    "    print(f\"  ✅ MRR:            {positive_mrr}/{n_models} ({positive_mrr/n_models*100:.1f}%)\")\n",
    "    \n",
    "    # Modello con maggiori miglioramenti\n",
    "    best_improvement_model = max(models_both_strategies, \n",
    "                               key=lambda k: sum([model_deltas[k]['top1_delta'], \n",
    "                                                model_deltas[k]['top5_delta'], \n",
    "                                                model_deltas[k]['mrr_delta']]))\n",
    "    \n",
    "    print(f\"\\n🏆 Modello che beneficia di più dalla strategia geometrica:\")\n",
    "    print(f\"   {MODELS[best_improvement_model]['name']}\")\n",
    "    best_deltas = model_deltas[best_improvement_model]\n",
    "    print(f\"   Top-1: {best_deltas['top1_delta']:+.1f}%, Top-5: {best_deltas['top5_delta']:+.1f}%, MRR: {best_deltas['mrr_delta']:+.1f}%\")\n",
    "    \n",
    "    print(\"\\n✅ Analisi miglioramenti completata!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Heat Map Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# HEAT MAP PERFORMANCE NORMALIZZATA\n",
    "# ============================================================================\n",
    "\n",
    "if len(successfully_loaded) < 2:\n",
    "    print(\"⏭️  Saltando heat map: servono almeno 2 modelli\")\n",
    "else:\n",
    "    print(\"🎨 Creazione heat map performance normalizzata...\")\n",
    "    \n",
    "    # Prepara dati per heat map\n",
    "    metrics_for_heatmap = ['top1_accuracy', 'top5_hit_rate', 'mrr', 'catalogue_coverage']\n",
    "    metric_labels = ['Top-1\\nAccuracy', 'Top-5\\nHit Rate', 'Mean Reciprocal\\nRank', 'Catalogue\\nCoverage']\n",
    "    \n",
    "    # Crea matrice dati\n",
    "    heatmap_data = []\n",
    "    model_labels = []\n",
    "    \n",
    "    for model_key in successfully_loaded:\n",
    "        model_config = MODELS[model_key]\n",
    "        metrics = model_metrics[model_key]\n",
    "        \n",
    "        row_data = []\n",
    "        for metric in metrics_for_heatmap:\n",
    "            if metric == 'catalogue_coverage':\n",
    "                # Coverage può essere > 1, quindi normalizziamo diversamente\n",
    "                value = min(metrics[metric] * 100, 100)  # Cap a 100%\n",
    "            else:\n",
    "                value = metrics[metric] * 100\n",
    "            row_data.append(value)\n",
    "        \n",
    "        heatmap_data.append(row_data)\n",
    "        model_labels.append(model_config['name'])\n",
    "    \n",
    "    # Converte a numpy array per facilità\n",
    "    heatmap_matrix = np.array(heatmap_data)\n",
    "    \n",
    "    # Crea heat map\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    \n",
    "    # Usa colormap personalizzata\n",
    "    im = ax.imshow(heatmap_matrix, cmap='RdYlGn', aspect='auto', vmin=0, vmax=100)\n",
    "    \n",
    "    # Configura assi\n",
    "    ax.set_xticks(range(len(metric_labels)))\n",
    "    ax.set_xticklabels(metric_labels, rotation=0, ha='center')\n",
    "    ax.set_yticks(range(len(model_labels)))\n",
    "    ax.set_yticklabels(model_labels)\n",
    "    \n",
    "    # Aggiungi valori nelle celle\n",
    "    for i in range(len(model_labels)):\n",
    "        for j in range(len(metric_labels)):\n",
    "            value = heatmap_matrix[i, j]\n",
    "            text_color = 'white' if value < 50 else 'black'\n",
    "            ax.text(j, i, f'{value:.1f}%', ha='center', va='center', \n",
    "                   color=text_color, fontweight='bold', fontsize=10)\n",
    "    \n",
    "    ax.set_title('Performance Heat Map - Model Comparison\\n(Normalized 0-100%)', \n",
    "                fontsize=16, fontweight='bold', pad=20)\n",
    "    \n",
    "    # Colorbar\n",
    "    cbar = plt.colorbar(im, ax=ax, shrink=0.8, aspect=20)\n",
    "    cbar.set_label('Performance (%)', rotation=270, labelpad=20, fontsize=12)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Salva\n",
    "    output_path = Path(OUTPUT_DIRS['comparison']) / 'models_performance_heatmap.png'\n",
    "    plt.savefig(output_path, dpi=300, bbox_inches='tight', facecolor='white')\n",
    "    print(f\"💾 Salvato: {output_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # Analisi best/worst per categoria\n",
    "    print(\"\\n📊 ANALISI HEAT MAP:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    for j, metric_name in enumerate(['Top-1 Accuracy', 'Top-5 Hit Rate', 'MRR', 'Coverage']):\n",
    "        values_for_metric = heatmap_matrix[:, j]\n",
    "        best_idx = np.argmax(values_for_metric)\n",
    "        worst_idx = np.argmin(values_for_metric)\n",
    "        \n",
    "        print(f\"{metric_name}:\")\n",
    "        print(f\"  🥇 Best:  {model_labels[best_idx]} ({values_for_metric[best_idx]:.1f}%)\")\n",
    "        print(f\"  🥉 Worst: {model_labels[worst_idx]} ({values_for_metric[worst_idx]:.1f}%)\")\n",
    "        print()\n",
    "    \n",
    "    print(\"✅ Heat map performance completata!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📈 4. Analisi Temporale Multi-Modello\n",
    "\n",
    "Confronto delle performance nel tempo per identificare stabilità e trend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# ANALISI TEMPORALE MULTI-MODELLO\n",
    "# ============================================================================\n",
    "\n",
    "if len(successfully_loaded) < 2:\n",
    "    print(\"⏭️  Saltando analisi temporale: servono almeno 2 modelli\")\n",
    "else:\n",
    "    print(\"📅 Analisi temporale multi-modello...\")\n",
    "    \n",
    "    # Verifica che abbiamo dati temporali\n",
    "    temporal_data_available = False\n",
    "    for model_key in successfully_loaded:\n",
    "        df = model_data[model_key]\n",
    "        if 'year' in df.columns and not df['year'].isna().all():\n",
    "            temporal_data_available = True\n",
    "            break\n",
    "    \n",
    "    if not temporal_data_available:\n",
    "        print(\"   ⚠️  Nessun dato temporale disponibile (colonna 'year' mancante o vuota)\")\n",
    "    else:\n",
    "        print(\"   ✅ Dati temporali trovati, creazione grafici...\")\n",
    "        \n",
    "        # Calcola metriche per anno per ogni modello\n",
    "        temporal_results = {}\n",
    "        all_years = set()\n",
    "        \n",
    "        for model_key in successfully_loaded:\n",
    "            df = model_data[model_key]\n",
    "            if 'year' in df.columns:\n",
    "                # Filtra anni validi\n",
    "                df_with_year = df[df['year'].notna()]\n",
    "                \n",
    "                if len(df_with_year) > 0:\n",
    "                    # Calcola metriche per anno\n",
    "                    yearly_metrics = {}\n",
    "                    for year in df_with_year['year'].unique():\n",
    "                        year_data = df_with_year[df_with_year['year'] == year]\n",
    "                        if len(year_data) > 0:\n",
    "                            metrics = calculate_metrics(year_data)\n",
    "                            yearly_metrics[int(year)] = metrics\n",
    "                            all_years.add(int(year))\n",
    "                    \n",
    "                    temporal_results[model_key] = yearly_metrics\n",
    "        \n",
    "        if len(temporal_results) == 0:\n",
    "            print(\"   ⚠️  Nessun risultato temporale calcolabile\")\n",
    "        else:\n",
    "            # Ordina gli anni\n",
    "            sorted_years = sorted(all_years)\n",
    "            print(f\"   📊 Anni disponibili: {sorted_years}\")\n",
    "            \n",
    "            # Crea grafici temporali\n",
    "            metrics_to_plot = [\n",
    "                ('top1_accuracy', 'Top-1 Accuracy (%)', 'temporal_top1_comparison.png'),\n",
    "                ('top5_hit_rate', 'Top-5 Hit Rate (%)', 'temporal_top5_comparison.png'),\n",
    "                ('mrr', 'Mean Reciprocal Rank (%)', 'temporal_mrr_comparison.png')\n",
    "            ]\n",
    "            \n",
    "            for metric_key, ylabel, filename in metrics_to_plot:\n",
    "                fig, ax = plt.subplots(figsize=(12, 6))\n",
    "                \n",
    "                # Plot linea per ogni modello\n",
    "                for model_key in successfully_loaded:\n",
    "                    if model_key in temporal_results:\n",
    "                        model_config = MODELS[model_key]\n",
    "                        \n",
    "                        # Prepara dati per questo modello\n",
    "                        x_data = []\n",
    "                        y_data = []\n",
    "                        \n",
    "                        for year in sorted_years:\n",
    "                            if year in temporal_results[model_key]:\n",
    "                                x_data.append(year)\n",
    "                                y_data.append(temporal_results[model_key][year][metric_key] * 100)\n",
    "                        \n",
    "                        if len(x_data) > 0:\n",
    "                            ax.plot(x_data, y_data, color=model_config['color'], \n",
    "                                   marker='o', linewidth=2, markersize=6,\n",
    "                                   label=model_config['name'])\n",
    "                \n",
    "                ax.set_xlabel('Year', fontsize=12)\n",
    "                ax.set_ylabel(ylabel, fontsize=12)\n",
    "                ax.set_title(f'{ylabel.replace(\" (%)\", \"\")} - Temporal Comparison', \n",
    "                           fontsize=14, fontweight='bold')\n",
    "                ax.legend(loc='best')\n",
    "                ax.grid(True, alpha=0.3)\n",
    "                \n",
    "                # Evidenzia periodo COVID se presente\n",
    "                covid_years = [2020, 2021]\n",
    "                if any(year in sorted_years for year in covid_years):\n",
    "                    for covid_year in covid_years:\n",
    "                        if covid_year in sorted_years:\n",
    "                            ax.axvline(covid_year, color='red', linestyle='--', \n",
    "                                     alpha=0.5, linewidth=1)\n",
    "                    ax.text(0.02, 0.98, 'Red lines: COVID-19 period', \n",
    "                           transform=ax.transAxes, fontsize=10, \n",
    "                           verticalalignment='top', \n",
    "                           bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                \n",
    "                # Salva\n",
    "                output_path = Path(OUTPUT_DIRS['comparison']) / filename\n",
    "                plt.savefig(output_path, dpi=300, bbox_inches='tight', facecolor='white')\n",
    "                print(f\"💾 Salvato: {output_path}\")\n",
    "                \n",
    "                plt.show()\n",
    "            \n",
    "            # Analisi di stabilità temporale\n",
    "            print(\"\\n📊 ANALISI STABILITÀ TEMPORALE:\")\n",
    "            print(\"-\" * 50)\n",
    "            \n",
    "            for model_key in successfully_loaded:\n",
    "                if model_key in temporal_results and len(temporal_results[model_key]) > 1:\n",
    "                    model_name = MODELS[model_key]['name']\n",
    "                    \n",
    "                    # Calcola variabilità per MRR\n",
    "                    mrr_values = [temporal_results[model_key][year]['mrr'] * 100 \n",
    "                                 for year in temporal_results[model_key].keys()]\n",
    "                    mrr_std = np.std(mrr_values)\n",
    "                    mrr_mean = np.mean(mrr_values)\n",
    "                    stability_score = mrr_mean / (mrr_std + 1e-6)  # Higher = more stable\n",
    "                    \n",
    "                    print(f\"{model_name:>15s}: MRR avg={mrr_mean:.1f}%, std={mrr_std:.1f}%, stability={stability_score:.1f}\")\n",
    "            \n",
    "            print(\"\\n✅ Analisi temporale completata!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔍 5. Analisi Errori Comparativa\n",
    "\n",
    "Confronto dei pattern di errore tra i diversi modelli per identificare bias e differenze qualitative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# ANALISI ERRORI COMPARATIVA\n",
    "# ============================================================================\n",
    "\n",
    "if len(successfully_loaded) < 2:\n",
    "    print(\"⏭️  Saltando analisi errori: servono almeno 2 modelli\")\n",
    "else:\n",
    "    print(\"🔍 Analisi comparativa degli errori...\")\n",
    "    \n",
    "    # Analizza i POI più problematici per ogni modello\n",
    "    print(\"\\n📊 POI PIÙ PROBLEMATICI PER MODELLO:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    model_error_patterns = {}\n",
    "    \n",
    "    for model_key in successfully_loaded:\n",
    "        df = model_data[model_key]\n",
    "        model_name = MODELS[model_key]['name']\n",
    "        \n",
    "        if 'ground_truth_norm' in df.columns and 'prediction_norm' in df.columns:\n",
    "            # Calcola hit@1 se non presente\n",
    "            if 'hit@1' not in df.columns:\n",
    "                df['hit@1'] = df['prediction_norm'].str[0] == df['ground_truth_norm']\n",
    "            \n",
    "            # Analizza errori\n",
    "            errors_df = df[~df['hit@1']].copy()\n",
    "            \n",
    "            if len(errors_df) > 0:\n",
    "                # Top POI problematici (ground truth con più errori)\n",
    "                error_counts = errors_df['ground_truth_norm'].value_counts().head(5)\n",
    "                total_counts = df['ground_truth_norm'].value_counts()\n",
    "                \n",
    "                print(f\"\\n🔴 {model_name}:\")\n",
    "                print(\"   Top POI con più errori:\")\n",
    "                \n",
    "                error_data = []\n",
    "                for poi, error_count in error_counts.items():\n",
    "                    total_count = total_counts.get(poi, error_count)\n",
    "                    error_rate = error_count / total_count\n",
    "                    print(f\"     {poi[:25]:>25s}: {error_count:>4d} errori ({error_rate:>5.1%})\")\n",
    "                    error_data.append((poi, error_count, error_rate))\n",
    "                \n",
    "                model_error_patterns[model_key] = error_data\n",
    "    \n",
    "    # Confronto bias geografici\n",
    "    print(f\"\\n📊 CONFRONTO BIAS NELLE PREDIZIONI:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for model_key in successfully_loaded:\n",
    "        df = model_data[model_key]\n",
    "        model_name = MODELS[model_key]['name']\n",
    "        \n",
    "        if 'prediction_norm' in df.columns:\n",
    "            # Analizza predizioni più frequenti (possibili bias)\n",
    "            all_predictions = []\n",
    "            for pred_list in df['prediction_norm']:\n",
    "                if isinstance(pred_list, list) and len(pred_list) > 0:\n",
    "                    all_predictions.append(pred_list[0])  # Solo top-1 prediction\n",
    "            \n",
    "            if all_predictions:\n",
    "                pred_freq = Counter(all_predictions)\n",
    "                top_predictions = pred_freq.most_common(5)\n",
    "                \n",
    "                print(f\"\\n🎯 {model_name} - Predizioni più frequenti:\")\n",
    "                total_preds = len(all_predictions)\n",
    "                for poi, count in top_predictions:\n",
    "                    percentage = (count / total_preds) * 100\n",
    "                    print(f\"     {poi[:25]:>25s}: {count:>5d} ({percentage:>4.1f}%)\")\n",
    "    \n",
    "    # Matrice di confusione comparativa (se fattibile)\n",
    "    if len(successfully_loaded) >= 2:\n",
    "        print(f\"\\n📊 Generazione matrici di confusione per confronto...\")\n",
    "        \n",
    "        # Identifica POI comuni più frequenti\n",
    "        all_gt_pois = set()\n",
    "        for model_key in successfully_loaded:\n",
    "            df = model_data[model_key]\n",
    "            if 'ground_truth_norm' in df.columns:\n",
    "                all_gt_pois.update(df['ground_truth_norm'].unique())\n",
    "        \n",
    "        # Prendi top 10 POI più comuni\n",
    "        poi_frequencies = Counter()\n",
    "        for model_key in successfully_loaded:\n",
    "            df = model_data[model_key]\n",
    "            if 'ground_truth_norm' in df.columns:\n",
    "                poi_frequencies.update(df['ground_truth_norm'].value_counts().to_dict())\n",
    "        \n",
    "        top_pois = [poi for poi, _ in poi_frequencies.most_common(10)]\n",
    "        \n",
    "        # Crea confusion matrix per ogni modello\n",
    "        fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "        axes = axes.flatten()\n",
    "        \n",
    "        for i, model_key in enumerate(successfully_loaded):\n",
    "            if i >= len(axes):\n",
    "                break\n",
    "                \n",
    "            df = model_data[model_key]\n",
    "            model_name = MODELS[model_key]['name']\n",
    "            ax = axes[i]\n",
    "            \n",
    "            if 'ground_truth_norm' in df.columns and 'prediction_norm' in df.columns:\n",
    "                # Filtra per top POI\n",
    "                mask = (df['ground_truth_norm'].isin(top_pois) & \n",
    "                       df['prediction_norm'].str[0].isin(top_pois))\n",
    "                df_filtered = df[mask]\n",
    "                \n",
    "                if len(df_filtered) > 0:\n",
    "                    # Crea confusion matrix\n",
    "                    cm = pd.crosstab(df_filtered['ground_truth_norm'],\n",
    "                                   df_filtered['prediction_norm'].str[0],\n",
    "                                   normalize='index')\n",
    "                    \n",
    "                    # Plot heatmap\n",
    "                    sns.heatmap(cm, annot=True, fmt='.2f', cmap='Blues', \n",
    "                              ax=ax, cbar_kws={'shrink': 0.5})\n",
    "                    ax.set_title(f'{model_name}\\nConfusion Matrix', fontweight='bold')\n",
    "                    ax.set_xlabel('Predicted')\n",
    "                    ax.set_ylabel('True')\n",
    "                    \n",
    "                    # Ruota etichette per leggibilità\n",
    "                    ax.tick_params(axis='x', rotation=45)\n",
    "                    ax.tick_params(axis='y', rotation=0)\n",
    "            else:\n",
    "                ax.text(0.5, 0.5, f'{model_name}\\nDati non disponibili', \n",
    "                       ha='center', va='center', transform=ax.transAxes)\n",
    "        \n",
    "        # Nascondi assi non utilizzati\n",
    "        for j in range(len(successfully_loaded), len(axes)):\n",
    "            axes[j].set_visible(False)\n",
    "        \n",
    "        plt.suptitle('Confusion Matrices Comparison - Top 10 POI', \n",
    "                    fontsize=16, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Salva\n",
    "        output_path = Path(OUTPUT_DIRS['comparison']) / 'confusion_matrices_comparison.png'\n",
    "        plt.savefig(output_path, dpi=300, bbox_inches='tight', facecolor='white')\n",
    "        print(f\"💾 Salvato: {output_path}\")\n",
    "        \n",
    "        plt.show()\n",
    "    \n",
    "    print(\"\\n✅ Analisi errori comparativa completata!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📋 6. Report Finale e Riepilogo\n",
    "\n",
    "Genera un report completo con tutti i risultati e le raccomandazioni per l'integrazione nella tesi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# REPORT FINALE E RIEPILOGO - VERSIONE DOPPIA STRATEGIA\n",
    "# ============================================================================\n",
    "\n",
    "print(\"📋 Generazione report finale (Base + Geometric strategies)...\\n\")\n",
    "print(\"=\" * 80)\n",
    "print(\"🎓 DUAL-STRATEGY MULTI-MODEL COMPARISON ANALYSIS - FINAL REPORT\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if len(models_both_strategies) == 0:\n",
    "    print(\"\\n❌ NESSUN MODELLO CON ENTRAMBE LE STRATEGIE - VERIFICA CONFIGURAZIONE\")\n",
    "    print(\"\\n💡 AZIONI RICHIESTE:\")\n",
    "    print(\"   1. Verifica che esistano le directory:\")\n",
    "    print(\"      - results_deepseek-coder_33b_base_version/\")\n",
    "    print(\"      - results_deepseek-coder_33b_with_geom/\")  \n",
    "    print(\"      - results_mixtr_with_geom/\")\n",
    "    print(\"      - results/mixtral_8x7b/base_version/\")\n",
    "    print(\"      - results/qwen2.5_7b/base_version/ e /with_geom/\")\n",
    "    print(\"      - results/llama3.1_8b/base_version/ e /with_geom_srv_univr/\")\n",
    "    print(\"   2. Sposta i file CSV nelle directory appropriate\")\n",
    "    print(\"   3. Ri-esegui questo notebook\")\n",
    "else:\n",
    "    # Riepilogo modelli con doppia strategia\n",
    "    print(f\"\\n📊 MODELLI ANALIZZATI CON DOPPIA STRATEGIA: {len(models_both_strategies)}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    total_predictions_base = sum(model_metrics_base[k]['total_predictions'] for k in models_both_strategies)\n",
    "    total_predictions_geom = sum(model_metrics_geom[k]['total_predictions'] for k in models_both_strategies)\n",
    "    \n",
    "    for model_key in models_both_strategies:\n",
    "        model_config = MODELS[model_key]\n",
    "        metrics_base = model_metrics_base[model_key]\n",
    "        metrics_geom = model_metrics_geom[model_key]\n",
    "        \n",
    "        print(f\"✅ {model_config['name']:>20s}:\")\n",
    "        print(f\"   Base:     {metrics_base['total_predictions']:>6,} predizioni\")\n",
    "        print(f\"   Geom:     {metrics_geom['total_predictions']:>6,} predizioni\")\n",
    "    \n",
    "    print(f\"\\n📈 TOTALE PREDIZIONI ANALIZZATE:\")\n",
    "    print(f\"   Base Strategy:      {total_predictions_base:,}\")\n",
    "    print(f\"   Geometric Strategy: {total_predictions_geom:,}\")\n",
    "    print(f\"   GRAN TOTALE:        {total_predictions_base + total_predictions_geom:,}\")\n",
    "    \n",
    "    # Best performers per strategia\n",
    "    print(f\"\\n🏆 BEST PERFORMERS PER STRATEGIA:\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    metrics_names = [\n",
    "        ('top1_accuracy', 'Top-1 Accuracy'),\n",
    "        ('top5_hit_rate', 'Top-5 Hit Rate'),\n",
    "        ('mrr', 'Mean Reciprocal Rank')\n",
    "    ]\n",
    "    \n",
    "    for metric_key, metric_name in metrics_names:\n",
    "        best_base = max(models_both_strategies, key=lambda k: model_metrics_base[k][metric_key])\n",
    "        best_geom = max(models_both_strategies, key=lambda k: model_metrics_geom[k][metric_key])\n",
    "        \n",
    "        best_base_value = model_metrics_base[best_base][metric_key] * 100\n",
    "        best_geom_value = model_metrics_geom[best_geom][metric_key] * 100\n",
    "        \n",
    "        print(f\"{metric_name:>18s}:\")\n",
    "        print(f\"   Base: {MODELS[best_base]['name']} ({best_base_value:.1f}%)\")\n",
    "        print(f\"   Geom: {MODELS[best_geom]['name']} ({best_geom_value:.1f}%)\")\n",
    "        \n",
    "        # Evidenzia se stesso modello vince in entrambe\n",
    "        if best_base == best_geom:\n",
    "            print(f\"   🏆 {MODELS[best_base]['name']} domina entrambe le strategie!\")\n",
    "        print()\n",
    "    \n",
    "    # Analisi miglioramenti complessiva\n",
    "    if 'model_deltas' in locals():\n",
    "        print(f\"\\n📈 IMPATTO DELLA STRATEGIA GEOMETRICA:\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        all_top1_deltas = [model_deltas[k]['top1_delta'] for k in models_both_strategies]\n",
    "        all_top5_deltas = [model_deltas[k]['top5_delta'] for k in models_both_strategies] \n",
    "        all_mrr_deltas = [model_deltas[k]['mrr_delta'] for k in models_both_strategies]\n",
    "        \n",
    "        print(f\"Miglioramenti medi:\")\n",
    "        print(f\"   Top-1 Accuracy: {np.mean(all_top1_deltas):+.2f}% ({'✅' if np.mean(all_top1_deltas) > 0 else '❌'})\")\n",
    "        print(f\"   Top-5 Hit Rate: {np.mean(all_top5_deltas):+.2f}% ({'✅' if np.mean(all_top5_deltas) > 0 else '❌'})\")\n",
    "        print(f\"   MRR:            {np.mean(all_mrr_deltas):+.2f}% ({'✅' if np.mean(all_mrr_deltas) > 0 else '❌'})\")\n",
    "        \n",
    "        positive_improvements = sum(1 for d in all_top1_deltas + all_top5_deltas + all_mrr_deltas if d > 0)\n",
    "        total_comparisons = len(all_top1_deltas) * 3\n",
    "        success_rate = positive_improvements / total_comparisons * 100\n",
    "        \n",
    "        print(f\"\\n🎯 Tasso di successo strategia geometrica: {success_rate:.1f}%\")\n",
    "        print(f\"   ({positive_improvements}/{total_comparisons} metriche migliorate)\")\n",
    "    \n",
    "    # Files generati\n",
    "    print(f\"\\n📁 FILES GENERATI:\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # Controlla tutte le directory di output\n",
    "    all_generated_files = []\n",
    "    for output_dir_name, output_dir_path in OUTPUT_DIRS.items():\n",
    "        dir_path = Path(output_dir_path)\n",
    "        if dir_path.exists():\n",
    "            files = list(dir_path.glob('*.png')) + list(dir_path.glob('*.csv')) + list(dir_path.glob('*.tex'))\n",
    "            all_generated_files.extend(files)\n",
    "    \n",
    "    # Raggruppa per tipo\n",
    "    file_types = {\n",
    "        'PNG (Grafici)': [f for f in all_generated_files if f.suffix == '.png'],\n",
    "        'CSV (Tabelle)': [f for f in all_generated_files if f.suffix == '.csv'],\n",
    "        'TEX (LaTeX)': [f for f in all_generated_files if f.suffix == '.tex']\n",
    "    }\n",
    "    \n",
    "    for file_type, files in file_types.items():\n",
    "        if files:\n",
    "            print(f\"\\n{file_type}:\")\n",
    "            for file_path in sorted(files):\n",
    "                file_size = file_path.stat().st_size / 1024  # KB\n",
    "                rel_path = file_path.relative_to(Path.cwd().parent)\n",
    "                print(f\"  ✅ {rel_path} ({file_size:.1f} KB)\")\n",
    "    \n",
    "    print(f\"\\n📊 TOTALE FILES GENERATI: {len(all_generated_files)}\")\n",
    "    \n",
    "    # Istruzioni per LaTeX\n",
    "    print(f\"\\n📄 INTEGRAZIONE LATEX AGGIORNATA:\")\n",
    "    print(\"-\" * 60)\n",
    "    print(\"1. 📊 Grafici salvati in:\")\n",
    "    print(\"   - img/multi_model_comparison/side_by_side/ (comparazioni affiancate)\")\n",
    "    print(\"   - img/multi_model_comparison/base/ (solo strategia base)\")\n",
    "    print(\"   - img/multi_model_comparison/geom/ (solo strategia geometrica)\")\n",
    "    \n",
    "    print(\"2. 📋 Tabelle LaTeX generate:\")\n",
    "    print(\"   - dual_strategy_table.tex (tabella comparativa completa)\")\n",
    "    print(\"   - performance_table.tex (se disponibile)\")\n",
    "    \n",
    "    print(\"3. 🖼️  Grafici chiave per la tesi:\")\n",
    "    print(\"   - base_vs_geom_comparison.png (griglia 2x3 affiancata)\")\n",
    "    print(\"   - top1_base_vs_geom_overlaid.png (confronto diretto Top-1)\")\n",
    "    print(\"   - geometric_improvements_delta.png (analisi miglioramenti)\")\n",
    "    print(\"   - radar_chart_base_vs_geom.png (confronto multidimensionale)\")\n",
    "    \n",
    "    print(\"4. 📝 Integrazione nel documento:\")\n",
    "    print(\"   - Usa \\\\input{img/multi_model_comparison/dual_strategy_table.tex}\")\n",
    "    print(\"   - \\\\includegraphics{img/multi_model_comparison/side_by_side/...}\")\n",
    "    \n",
    "    # Raccomandazioni strategiche\n",
    "    print(f\"\\n💡 RACCOMANDAZIONI STRATEGICHE:\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    if len(models_both_strategies) >= 2:\n",
    "        # Identifica modelli per diversi use case\n",
    "        most_improved_model = None\n",
    "        if 'model_deltas' in locals():\n",
    "            most_improved_model = max(models_both_strategies, \n",
    "                                    key=lambda k: sum([model_deltas[k]['top1_delta'], \n",
    "                                                     model_deltas[k]['top5_delta'], \n",
    "                                                     model_deltas[k]['mrr_delta']]))\n",
    "        \n",
    "        best_overall_base = max(models_both_strategies, \n",
    "                              key=lambda k: model_metrics_base[k]['top1_accuracy'])\n",
    "        best_overall_geom = max(models_both_strategies, \n",
    "                              key=lambda k: model_metrics_geom[k]['top1_accuracy'])\n",
    "        \n",
    "        print(f\"🎯 Per massima accuratezza (Base):      {MODELS[best_overall_base]['name']}\")\n",
    "        print(f\"🎯 Per massima accuratezza (Geom):      {MODELS[best_overall_geom]['name']}\")\n",
    "        \n",
    "        if most_improved_model:\n",
    "            improvement_sum = sum([model_deltas[most_improved_model]['top1_delta'],\n",
    "                                 model_deltas[most_improved_model]['top5_delta'],\n",
    "                                 model_deltas[most_improved_model]['mrr_delta']])\n",
    "            print(f\"🚀 Maggior beneficio da geometria:      {MODELS[most_improved_model]['name']} (+{improvement_sum:.1f}%)\")\n",
    "        \n",
    "        # Analisi cost-benefit se disponibile info sui parametri\n",
    "        print(f\"\\n⚖️  TRADE-OFF ANALISI:\")\n",
    "        for model_key in models_both_strategies:\n",
    "            model_name = MODELS[model_key]['name']\n",
    "            params = MODELS[model_key]['params']\n",
    "            base_acc = model_metrics_base[model_key]['top1_accuracy'] * 100\n",
    "            geom_acc = model_metrics_geom[model_key]['top1_accuracy'] * 100\n",
    "            \n",
    "            print(f\"  {model_name} ({params}): Base {base_acc:.1f}% → Geom {geom_acc:.1f}%\")\n",
    "    \n",
    "    # Status completamento\n",
    "    print(f\"\\n✅ ANALISI DOPPIA STRATEGIA COMPLETATA CON SUCCESSO!\")\n",
    "    print(f\"   📊 {len(models_both_strategies)} modelli analizzati\")\n",
    "    print(f\"   🔄 2 strategie di prompt confrontate\")\n",
    "    print(f\"   📈 {len(all_generated_files)} file generati\")\n",
    "    print(f\"   🎯 Pronto per integrazione nella tesi\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"🎓 DUAL-STRATEGY MULTI-MODEL ANALYSIS COMPLETED\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📝 Note Finali e Prossimi Passi - Dual Strategy Analysis\n",
    "\n",
    "### ✅ Checklist Completamento Dual-Strategy\n",
    "\n",
    "- [ ] **Auto-detection completata**: Directory rilevate automaticamente per entrambe le strategie\n",
    "- [ ] **Dati caricati**: Modelli con strategia base E geometrica identificati\n",
    "- [ ] **Grafici affiancati generati**: Files PNG creati in `img/multi_model_comparison/side_by_side/`\n",
    "- [ ] **Analisi miglioramenti**: Grafici delta e radar charts completati\n",
    "- [ ] **Tabelle dual-strategy**: File LaTeX con confronto base vs geom generato\n",
    "\n",
    "### 🔄 Prossimi Passi per Tesi\n",
    "\n",
    "1. **📊 Verifica grafici chiave**:\n",
    "   - `base_vs_geom_comparison.png` - Griglia 2x3 comparativa principale\n",
    "   - `top1_base_vs_geom_overlaid.png` - Confronto diretto Top-1\n",
    "   - `geometric_improvements_delta.png` - Analisi miglioramenti\n",
    "   - `radar_chart_base_vs_geom.png` - Confronto multidimensionale\n",
    "\n",
    "2. **📋 Integra tabelle LaTeX**: \n",
    "   - Copia `dual_strategy_table.tex` nel documento principale\n",
    "   - Include tabella con `\\input{img/multi_model_comparison/dual_strategy_table.tex}`\n",
    "\n",
    "3. **🖼️ Integra grafici**:\n",
    "   - Usa path: `\\includegraphics{img/multi_model_comparison/side_by_side/...}`\n",
    "   - I grafici sono ottimizzati per stampa (300 DPI, alta qualità)\n",
    "\n",
    "4. **📖 Narrativa tesi**: Usa i risultati per supportare:\n",
    "   - Efficacia delle informazioni geometriche\n",
    "   - Differenze tra modelli nell'utilizzo del contesto spaziale\n",
    "   - Trade-off accuratezza vs complessità\n",
    "\n",
    "### 🔍 Troubleshooting Struttura Ibrida\n",
    "\n",
    "- **Directory miste**: Configurazione automatica gestisce sia `results/{model}/` che directory separate\n",
    "- **DeepSeek**: Usa `results_deepseek-coder_33b_*` (directory separate)\n",
    "- **Mixtral geom**: Usa `results_mixtr_with_geom/` (directory separata)\n",
    "- **Altri modelli**: Seguono struttura standard `results/{model}/{strategy}/`\n",
    "\n",
    "### ⚠️ Troubleshooting Dual-Strategy\n",
    "\n",
    "- **Nessun confronto generato**: Controlla che esista almeno 1 modello con ENTRAMBE le strategie\n",
    "- **Grafici mancanti**: Verifica che directory `img/multi_model_comparison/` sia scrivibile\n",
    "- **Errori LaTeX**: Path relativi potrebbero necessitare aggiustamento (`../../img/...`)\n",
    "- **Performance inaspettate**: Alcuni modelli potrebbero non beneficiare dalle info geometriche\n",
    "\n",
    "### 🎯 Interpretazione Risultati Dual-Strategy\n",
    "\n",
    "**Miglioramenti positivi** (Δ > 0): Modello beneficia delle informazioni geometriche\n",
    "**Miglioramenti negativi** (Δ < 0): Strategia base supera quella geometrica  \n",
    "**Miglioramenti nulli** (Δ ≈ 0): Nessun impatto significativo delle info spaziali\n",
    "\n",
    "### 📊 Metriche Chiave per la Tesi\n",
    "\n",
    "1. **Tasso di successo geometrico**: % di metriche migliorate con strategia geometrica\n",
    "2. **Miglioramento medio**: Media dei Δ% per Top-1, Top-5, MRR\n",
    "3. **Modello più reattivo**: Quale modello beneficia di più dalle info geometriche\n",
    "4. **Consistenza miglioramenti**: Deviazione standard dei miglioramenti\n",
    "\n",
    "---\n",
    "\n",
    "**🎉 Analisi Dual-Strategy Completata!** \n",
    "\n",
    "Ora hai un sistema completo per confrontare l'impatto delle informazioni geometriche sui diversi modelli LLM. I grafici affiancati e l'analisi quantitativa dei miglioramenti forniranno una base solida per le conclusioni della tua tesi sulla mobilità turistica predittiva."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
