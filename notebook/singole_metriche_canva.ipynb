{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a3428bdf",
      "metadata": {},
      "source": [
        "# VeronaCard Next‑POI Prediction – Metrics & Exploratory Analysis\n",
        "\n",
        "*Generated automatically on 2025-06-25 13:01 UTC*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b7b32bbc",
      "metadata": {},
      "source": [
        "# 🎯 Obiettivi dell'Analisi\n",
        "\n",
        "Questo notebook fornisce una **valutazione completa** delle performance del modello di predizione next-POI implementato in `veronacard_mob.py`, attraverso un framework di analisi strutturato e riproducibile.\n",
        "\n",
        "## 📋 Pipeline di Analisi\n",
        "\n",
        "### 🔄 **1. Data Loading & Preprocessing**\n",
        "- **Caricamento automatico** di tutti i file CSV di predizione (`*_pred_*.csv`)\n",
        "- **Parsing robusto** con gestione errori per file malformati\n",
        "- **Estrazione automatica** dell'anno dai nomi file (es. `dati_2016_pred_model.csv` → 2016)\n",
        "- **Validazione** delle predizioni (filtro per liste con esattamente 5 elementi)\n",
        "\n",
        "### 📊 **2. Evaluation Metrics**\n",
        "Calcolo di **quattro metriche standard** per sistemi di raccomandazione:\n",
        "- **🎯 Top-1 Accuracy**: Precisione della predizione principale\n",
        "- **📈 Top-5 Hit Rate**: Presenza del POI corretto nella top-5\n",
        "- **⚡ Mean Reciprocal Rank (MRR)**: Qualità del ranking delle predizioni\n",
        "- **🗂️ Catalogue Coverage**: Diversità delle raccomandazioni\n",
        "\n",
        "### 📈 **3. Multi-level Visualization**\n",
        "- **📊 Global metrics**: Performance aggregate su tutto il dataset\n",
        "- **📅 Temporal analysis**: Trend evolutivo per anno (2016→2020)\n",
        "- **🎨 Interactive charts**: Grafici comparativi e heat-maps\n",
        "\n",
        "### 🔍 **4. Advanced Error Analysis Framework**\n",
        "Sistema modulare ed estendibile per analisi approfondita:\n",
        "- **🚨 Worst-case analysis**: Identificazione delle coppie POI più problematiche\n",
        "- **🔥 Confusion matrices**: Visualizzazione degli errori sui POI più frequenti\n",
        "- **🧠 Explainability**: Analisi dei pattern testuali che causano errori (LIME/SHAP-ready)\n",
        "- **📊 Temporal patterns**: Analisi stagionale e trend temporali degli errori\n",
        "\n",
        "## 🏗️ Caratteristiche Architetturali\n",
        "\n",
        "### **✅ Robustezza**\n",
        "- **Error handling** completo per file CSV corrotti\n",
        "- **Graceful degradation** per dipendenze mancanti (LIME, etc.)\n",
        "- **Validation** automatica dei dati di input\n",
        "\n",
        "### **🔧 Flessibilità**\n",
        "- **Path-agnostic**: Funziona con CSV nella cartella corrente o tramite `DATA_DIR`\n",
        "- **Modular design**: Sezioni indipendenti eseguibili singolarmente\n",
        "- **Extensible framework**: Facile aggiungere nuove metriche o analisi\n",
        "\n",
        "### **📊 Riproducibilità**\n",
        "- **Timestamp automatico** nel titolo\n",
        "- **Seed fisso** per operazioni random\n",
        "- **Versioning implicito** tramite nomi file con data\n",
        "\n",
        "---\n",
        "\n",
        "## 🚀 Quick Start\n",
        "\n",
        "```python\n",
        "# Esecuzione standard - CSV nella cartella 'results/'\n",
        "# Nessuna configurazione richiesta\n",
        "\n",
        "# Esecuzione personalizzata\n",
        "DATA_DIR = \"/path/to/your/csv/files\"  # opzionale\n",
        "```\n",
        "\n",
        "## 📁 Struttura File Attesa\n",
        "\n",
        "```\n",
        "project_root/\n",
        "├── notebook.ipynb           # questo notebook\n",
        "├── results/                 # directory predefinita\n",
        "│   ├── dati_2016_pred_model1.csv\n",
        "│   ├── dati_2017_pred_model1.csv\n",
        "│   └── ...\n",
        "└── veronacard_mob.py       # script di generazione predizioni\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "> **💡 Pro Tip**: Il notebook è progettato per **fail-safe operation** — anche se alcuni file sono corrotti o alcune librerie mancano, l'analisi procede con le parti disponibili e fornisce sempre risultati utili."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "07dfa4da",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "from pathlib import Path\n",
        "import ast\n",
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.rcParams['figure.dpi'] = 120\n",
        "plt.rcParams['axes.grid'] = True\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a744cbc",
      "metadata": {},
      "source": [
        "## 1. Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ac05b72",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import glob, ast, os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "# Percorso ai file CSV\n",
        "csv_files = [Path(p) for p in glob.glob('../results/llama3.1_8b/base_version/*_pred_*.csv')]\n",
        "csv_files = sorted(csv_files)\n",
        "assert csv_files, \"No CSV files matching *_pred_*.csv found!\"\n",
        "\n",
        "def safe_parse_prediction(x):\n",
        "    \"\"\"\n",
        "    Parsing robusto per la colonna 'prediction'.\n",
        "    Gestisce diversi formati: stringhe, liste già parsate, JSON, etc.\n",
        "    \"\"\"\n",
        "    if pd.isna(x) or x is None:\n",
        "        return []\n",
        "    \n",
        "    # Se è già una lista\n",
        "    if isinstance(x, list):\n",
        "        return x\n",
        "    \n",
        "    # Se è una stringa\n",
        "    if isinstance(x, str):\n",
        "        x = x.strip()\n",
        "        if not x:\n",
        "            return []\n",
        "        \n",
        "        try:\n",
        "            # Prova ast.literal_eval prima\n",
        "            result = ast.literal_eval(x)\n",
        "            if isinstance(result, list):\n",
        "                return result\n",
        "            else:\n",
        "                return [result]  # Wrap in lista se è un singolo elemento\n",
        "        except (ValueError, SyntaxError):\n",
        "            try:\n",
        "                # Prova JSON parse\n",
        "                result = json.loads(x)\n",
        "                if isinstance(result, list):\n",
        "                    return result\n",
        "                else:\n",
        "                    return [result]\n",
        "            except (json.JSONDecodeError, ValueError):\n",
        "                # Se fallisce tutto, prova a splittare per virgole\n",
        "                # Rimuovi caratteri come [ ] e \"\"\n",
        "                cleaned = x.replace('[', '').replace(']', '').replace('\"', '').replace(\"'\", \"\")\n",
        "                if ',' in cleaned:\n",
        "                    return [item.strip() for item in cleaned.split(',') if item.strip()]\n",
        "                else:\n",
        "                    return [cleaned] if cleaned else []\n",
        "    \n",
        "    # Fallback: converti in stringa e prova di nuovo\n",
        "    return safe_parse_prediction(str(x))\n",
        "\n",
        "dfs = []\n",
        "for fp in csv_files:\n",
        "    print(f\"Processing {fp.name}...\")\n",
        "    try:\n",
        "        df = pd.read_csv(fp)\n",
        "    except pd.errors.ParserError:\n",
        "        print(f\"Warning: Parser error in {fp}, trying with error handling...\")\n",
        "        df = pd.read_csv(fp, on_bad_lines='skip', engine='python')\n",
        "\n",
        "    # Estrai l'anno dal nome file, es. dati_2016_pred_...\n",
        "    year_token = next((part for part in fp.stem.split('_')\n",
        "                       if part.isdigit() and len(part) == 4), None)\n",
        "    df['year'] = int(year_token) if year_token else np.nan\n",
        "\n",
        "    # Parsing robusto delle predizioni\n",
        "    df['prediction_list'] = df['prediction'].apply(safe_parse_prediction)\n",
        "    \n",
        "    # Verifica che le predizioni siano liste non vuote\n",
        "    print(f\"  Before filtering: {len(df)} rows\")\n",
        "    df = df[df['prediction_list'].apply(lambda x: isinstance(x, list) and len(x) > 0)]\n",
        "    # Nel caso volessi liste vuote\n",
        "    df = df[df['prediction_list'].apply(lambda x: isinstance(x, list))]  # Include []\n",
        "    print(f\"  After filtering: {len(df)} rows\")\n",
        "    \n",
        "    # Opzionalmente, filtra per esattamente 5 predizioni (se richiesto)\n",
        "    # df = df[df['prediction_list'].apply(len) == 5]\n",
        "    \n",
        "    dfs.append(df)\n",
        "\n",
        "df_all = pd.concat(dfs, ignore_index=True)\n",
        "print(f\"Loaded {len(df_all):,} rows from {len(csv_files)} files\")\n",
        "\n",
        "# Verifica i dati\n",
        "print(f\"Sample predictions: {df_all['prediction_list'].iloc[0]}\")\n",
        "print(f\"Sample ground truth: {df_all['ground_truth'].iloc[0]}\")\n",
        "df_all.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a8882065",
      "metadata": {},
      "source": [
        "# 2. Definizione delle Metriche di Valutazione e Validazione\n",
        "\n",
        "Per valutare le performance del modello di predizione next-POI, utilizziamo quattro metriche standard nel campo dei sistemi di raccomandazione e della predizione sequenziale.\n",
        "\n",
        "## ✅ Validazione Metodologica\n",
        "\n",
        "**IMPORTANTE**: Prima di calcolare le metriche, il notebook implementa una serie di controlli di validazione per garantire la correttezza dei risultati:\n",
        "\n",
        "1. **Parsing Robusto**: Gestione di diversi formati di dati (JSON, liste Python, stringhe)\n",
        "2. **Consistenza Relazioni**: Verifica che Top-1 ≤ Top-5 ≤ 100% e MRR ≤ Top-1\n",
        "3. **Coverage Corretto**: Controllo che coverage ≤ 100% (o spiegazione se > 100%)\n",
        "4. **Normalizzazione POI**: Conversione coerente di ID POI per confronto\n",
        "\n",
        "## Notazione\n",
        "\n",
        "Sia $y_i$ il vero next-POI (ground truth) per la sequenza $i$-esima, e sia $\\hat{y}_i^{(1)}, \\hat{y}_i^{(2)}, \\ldots, \\hat{y}_i^{(k)}$ la lista **ordinata** delle $k$ raccomandazioni prodotte dal modello per quella sequenza, dove $\\hat{y}_i^{(1)}$ è la predizione con confidence più alta.\n",
        "\n",
        "---\n",
        "\n",
        "## 📊 Top-1 Accuracy\n",
        "\n",
        "Misura la **precisione della predizione principale** del modello.\n",
        "\n",
        "$$\\text{Acc}_{@1} = \\frac{1}{N}\\sum_{i=1}^{N}\\mathbf{1}\\!\\left\\{y_i = \\hat{y}_i^{(1)}\\right\\}$$\n",
        "\n",
        "**Interpretazione**: Percentuale di casi in cui la prima predizione del modello coincide esattamente con il POI realmente visitato.\n",
        "\n",
        "**Range**: [0, 1], dove 1 = predizione perfetta\n",
        "\n",
        "---\n",
        "\n",
        "## 🎯 Top-k Hit Rate\n",
        "\n",
        "Misura se il POI corretto è presente **tra le prime $k$ predizioni**.\n",
        "\n",
        "$$\\text{HR}_{@k} = \\frac{1}{N}\\sum_{i=1}^{N}\\mathbf{1}\\!\\left\\{y_i \\in \\{\\hat{y}_i^{(1)}, \\hat{y}_i^{(2)}, \\ldots, \\hat{y}_i^{(k)}\\}\\right\\}$$\n",
        "\n",
        "**Interpretazione**: Percentuale di casi in cui il POI corretto è presente nella lista delle prime $k$ raccomandazioni. Per $k=5$: \"Il modello include il POI corretto nella sua top-5?\"\n",
        "\n",
        "**Range**: [0, 1], sempre $\\text{HR}_{@k} \\geq \\text{Acc}_{@1}$\n",
        "\n",
        "---\n",
        "\n",
        "## ⚡ Mean Reciprocal Rank (MRR)\n",
        "\n",
        "Considera sia la **presenza** del POI corretto nella lista che la sua **posizione**.\n",
        "\n",
        "$$\\text{MRR} = \\frac{1}{N}\\sum_{i=1}^{N}\\frac{1}{\\text{rank}_i}$$\n",
        "\n",
        "dove $\\text{rank}_i = \\min\\{r \\mid y_i = \\hat{y}_i^{(r)}\\}$ è la posizione del POI corretto nella lista ordinata.\n",
        "\n",
        "**Interpretazione**: \n",
        "- Se il POI corretto è al 1° posto → contributo = 1.0\n",
        "- Se il POI corretto è al 2° posto → contributo = 0.5  \n",
        "- Se il POI corretto è al 5° posto → contributo = 0.2\n",
        "- Se il POI corretto non è nella top-k → contributo = 0.0\n",
        "\n",
        "**Range**: [0, 1], dove valori più alti indicano che i POI corretti appaiono nelle prime posizioni\n",
        "\n",
        "**Relazione con Top-1**: $\\text{MRR} \\leq \\text{Top-1 Accuracy}$\n",
        "\n",
        "---\n",
        "\n",
        "## 🗂️ Catalogue Coverage  \n",
        "\n",
        "Misura la **diversità** delle raccomandazioni prodotte dal modello.\n",
        "\n",
        "$$\\text{Coverage} = \\frac{\\left|\\bigcup_{i=1}^{N}\\{\\hat{y}_i^{(1)}, \\hat{y}_i^{(2)}, \\ldots, \\hat{y}_i^{(k)}\\}\\right|}{|\\mathcal{P}|}$$\n",
        "\n",
        "dove $\\mathcal{P}$ è l'insieme completo dei POI presenti nel dataset (ground truth).\n",
        "\n",
        "**Interpretazione**: Frazione dei POI disponibili che il modello è in grado di raccomandare. \n",
        "- Coverage = 1.0: il modello raccomanda tutti i POI del catalogo\n",
        "- Coverage = 0.1: il modello raccomanda solo il 10% dei POI disponibili\n",
        "\n",
        "**Caso Speciale**: Se Coverage > 100%, significa che il modello predice POI non presenti nel ground truth (possibile \"allucinazione\" del modello)\n",
        "\n",
        "**Importanza**: Previene il bias verso POI molto popolari e garantisce diversità nelle raccomandazioni.\n",
        "\n",
        "---\n",
        "\n",
        "## 💡 Note Metodologiche\n",
        "\n",
        "- **$N$**: Numero totale di predizioni nel dataset di test\n",
        "- **$k = 5$**: Utilizziamo consistently una finestra di 5 raccomandazioni\n",
        "- **Ordinamento**: Le predizioni sono ordinate per confidence/probabilità decrescente\n",
        "- **Handling missing**: Se $y_i \\notin \\{\\hat{y}_i^{(1)}, \\ldots, \\hat{y}_i^{(k)}\\}$, contributo = 0 per tutte le metriche\n",
        "\n",
        "## 🔍 Controlli di Sanità\n",
        "\n",
        "Il notebook implementa automaticamente questi controlli:\n",
        "1. **$\\text{Top-1} \\leq \\text{Top-5}$**: Sempre verificato per definizione\n",
        "2. **$\\text{MRR} \\leq \\text{Top-1}$**: MRR è sempre minore o uguale di Top-1 accuracy\n",
        "3. **Coverage razionale**: Analisi dettagliata se Coverage > 100%\n",
        "4. **Parsing consistente**: Gestione robusta di formati dati diversi\n",
        "\n",
        "Queste metriche forniscono una **valutazione completa** del modello: precision (Acc@1), recall (HR@k), ranking quality (MRR), e diversity (Coverage)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "04234505",
      "metadata": {},
      "source": [
        "### 2.1 Helper functions & per‑row computations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0bc35347",
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import numpy as np\n",
        "\n",
        "def poi_id(x):\n",
        "    \"\"\"\n",
        "    Converte 'x' in un identificatore hashable comparabile con ground-truth.\n",
        "    Versione migliorata con gestione errori più robusta.\n",
        "    \"\"\"\n",
        "    # Gestione sicura per array numpy e valori scalari\n",
        "    try:\n",
        "        if x is None:\n",
        "            return str(x)\n",
        "        # Per array numpy o pandas, usa la funzione appropriata\n",
        "        if hasattr(x, '__len__') and not isinstance(x, (str, dict)):\n",
        "            if len(x) == 0:\n",
        "                return str(x)\n",
        "        # Per valori scalari, usa pd.isna con controllo sicuro\n",
        "        if np.isscalar(x) and pd.isna(x):\n",
        "            return str(x)\n",
        "    except (ValueError, TypeError):\n",
        "        # Se pd.isna fallisce, assumiamo che non sia NaN\n",
        "        pass\n",
        "    \n",
        "    if isinstance(x, dict):\n",
        "        # Cerca chiavi comuni per POI\n",
        "        for key in ('poi', 'poi_id', 'name', 'id', 'title'):\n",
        "            if key in x:\n",
        "                return str(x[key])\n",
        "        # fallback: serializza in JSON ordinato\n",
        "        try:\n",
        "            return json.dumps(x, sort_keys=True)\n",
        "        except (TypeError, ValueError):\n",
        "            return str(x)\n",
        "    elif isinstance(x, (list, tuple)):\n",
        "        return tuple(poi_id(e) for e in x)\n",
        "    else:\n",
        "        return str(x).strip()\n",
        "\n",
        "# Normalizza prediction_list e ground_truth\n",
        "print(\"Normalizing predictions and ground truth...\")\n",
        "df_all['prediction_norm'] = df_all['prediction_list'].apply(\n",
        "    lambda lst: [poi_id(e) for e in lst] if isinstance(lst, list) else []\n",
        ")\n",
        "df_all['ground_truth_norm'] = df_all['ground_truth'].apply(poi_id)\n",
        "\n",
        "# Verifica che ci siano predizioni valide\n",
        "valid_predictions = df_all['prediction_norm'].apply(len) > 0\n",
        "print(f\"Valid predictions: {valid_predictions.sum()}/{len(df_all)} ({valid_predictions.mean():.1%})\")\n",
        "\n",
        "if valid_predictions.sum() == 0:\n",
        "    print(\"⚠️  No valid predictions found! Check data quality.\")\n",
        "else:\n",
        "    # Calcola metriche solo su righe con predizioni valide\n",
        "    df_valid = df_all[valid_predictions].copy()\n",
        "    print(f\"Final dataset size: {len(df_all)} rows\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0f4b31a1",
      "metadata": {},
      "source": [
        "### 2.2 Global metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "k1zvreptzs",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate evaluation metrics\n",
        "print(\"Calculating evaluation metrics...\")\n",
        "\n",
        "def calculate_hit_at_k(predictions, ground_truth, k):\n",
        "    \"\"\"Calculate hit@k metric\"\"\"\n",
        "    if not isinstance(predictions, list) or len(predictions) == 0:\n",
        "        return False\n",
        "    return ground_truth in predictions[:k]\n",
        "\n",
        "def calculate_reciprocal_rank(predictions, ground_truth):\n",
        "    \"\"\"Calculate reciprocal rank\"\"\"\n",
        "    if not isinstance(predictions, list) or len(predictions) == 0:\n",
        "        return 0.0\n",
        "    try:\n",
        "        rank = predictions.index(ground_truth) + 1\n",
        "        return 1.0 / rank\n",
        "    except ValueError:\n",
        "        return 0.0\n",
        "\n",
        "# Apply metric calculations\n",
        "df_all['hit@1'] = df_all.apply(lambda row: calculate_hit_at_k(row['prediction_norm'], row['ground_truth_norm'], 1), axis=1)\n",
        "df_all['hit@5'] = df_all.apply(lambda row: calculate_hit_at_k(row['prediction_norm'], row['ground_truth_norm'], 5), axis=1)\n",
        "df_all['rr'] = df_all.apply(lambda row: calculate_reciprocal_rank(row['prediction_norm'], row['ground_truth_norm']), axis=1)\n",
        "\n",
        "print(f\"Metrics calculated for {len(df_all)} rows\")\n",
        "print(f\"Hit@1 (Top-1 Accuracy): {df_all['hit@1'].mean():.3f}\")\n",
        "print(f\"Hit@5 (Top-5 Hit Rate): {df_all['hit@5'].mean():.3f}\")\n",
        "print(f\"MRR (Mean Reciprocal Rank): {df_all['rr'].mean():.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "87936f47",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calcolo delle metriche globali (corrette)\n",
        "print(\"Calculating global metrics...\")\n",
        "\n",
        "metrics_global = {\n",
        "    \"Top-1 Accuracy\": df_all[\"hit@1\"].mean(),\n",
        "    \"Top-5 Hit Rate\": df_all[\"hit@5\"].mean(),\n",
        "    \"MRR\": df_all[\"rr\"].mean(),\n",
        "}\n",
        "\n",
        "# Catalogue Coverage (corretto)\n",
        "# Coverage = POI unici nelle predizioni / POI unici nel ground truth\n",
        "predicted_pois = set()\n",
        "for preds in df_all[\"prediction_norm\"]:\n",
        "    predicted_pois.update(preds)\n",
        "\n",
        "ground_truth_pois = set(df_all[\"ground_truth_norm\"].unique())\n",
        "\n",
        "# Rimuovi valori NaN/None dai set\n",
        "predicted_pois = {poi for poi in predicted_pois if poi and str(poi) != 'nan'}\n",
        "ground_truth_pois = {poi for poi in ground_truth_pois if poi and str(poi) != 'nan'}\n",
        "\n",
        "coverage = len(predicted_pois) / len(ground_truth_pois) if len(ground_truth_pois) > 0 else 0\n",
        "metrics_global[\"Catalogue Coverage\"] = coverage\n",
        "\n",
        "print(f\"Debug - Predicted POIs: {len(predicted_pois)}, Ground Truth POIs: {len(ground_truth_pois)}\")\n",
        "print(f\"Coverage: {coverage:.4f}\")\n",
        "\n",
        "# Se la coverage è > 1, significa che il modello predice POI non presenti nel ground truth\n",
        "if coverage > 1:\n",
        "    print(\"⚠️  WARNING: Coverage > 100% indicates model predicts POI not in ground truth\")\n",
        "    \n",
        "    # Analisi POI extra\n",
        "    extra_pois = predicted_pois - ground_truth_pois\n",
        "    print(f\"POI predicted but not in ground truth: {len(extra_pois)}\")\n",
        "    if len(extra_pois) <= 10:\n",
        "        print(f\"Extra POI examples: {list(extra_pois)[:10]}\")\n",
        "\n",
        "# Converti coverage in percentuale (ma mantieni il valore reale per analisi)\n",
        "coverage_display = min(coverage, 1.0)  # Cap al 100% per display\n",
        "\n",
        "# Visualizza in formato tabella con percentuali\n",
        "metrics_df = pd.DataFrame({\n",
        "    'Metric': list(metrics_global.keys()),\n",
        "    'Value': [\n",
        "        f\"{metrics_global['Top-1 Accuracy']:.2%}\",\n",
        "        f\"{metrics_global['Top-5 Hit Rate']:.2%}\",\n",
        "        f\"{metrics_global['MRR']:.2%}\",\n",
        "        f\"{coverage_display:.2%}\" + (\" (>100%)\" if coverage > 1 else \"\")\n",
        "    ]\n",
        "})\n",
        "\n",
        "print(\"\\n📊 GLOBAL METRICS:\")\n",
        "print(\"=\" * 40)\n",
        "display(metrics_df)\n",
        "\n",
        "# 🔍 VALIDAZIONE METRICHE\n",
        "print(f\"\\n🔍 METRICS VALIDATION:\")\n",
        "print(\"=\" * 30)\n",
        "\n",
        "# Test 1: Top-1 ≤ Top-5\n",
        "top1_le_top5 = metrics_global[\"Top-1 Accuracy\"] <= metrics_global[\"Top-5 Hit Rate\"]\n",
        "print(f\"✅ Top-1 ≤ Top-5: {top1_le_top5} ({metrics_global['Top-1 Accuracy']:.3f} ≤ {metrics_global['Top-5 Hit Rate']:.3f})\")\n",
        "\n",
        "# Test 2: MRR ≤ Top-1 (dovrebbe essere sempre vero)\n",
        "mrr_le_top1 = metrics_global[\"MRR\"] <= metrics_global[\"Top-1 Accuracy\"]\n",
        "print(f\"✅ MRR ≤ Top-1: {mrr_le_top1} ({metrics_global['MRR']:.3f} ≤ {metrics_global['Top-1 Accuracy']:.3f})\")\n",
        "\n",
        "# Test 3: Tutti i valori sono nel range [0, 1]\n",
        "all_in_range = all(0 <= v <= 1 for k, v in metrics_global.items() if k != \"Catalogue Coverage\")\n",
        "print(f\"✅ All metrics in [0,1]: {all_in_range}\")\n",
        "\n",
        "# Test 4: Coverage razionale\n",
        "coverage_ok = coverage <= 2.0  # Soglia ragionevole\n",
        "print(f\"{'✅' if coverage_ok else '⚠️ '} Coverage reasonable: {coverage_ok} (Coverage = {coverage:.2f})\")\n",
        "\n",
        "# Test 5: Nessun valore NaN\n",
        "no_nan = not any(pd.isna(v) for v in metrics_global.values())\n",
        "print(f\"✅ No NaN values: {no_nan}\")\n",
        "\n",
        "validation_passed = all([top1_le_top5, mrr_le_top1, all_in_range, no_nan])\n",
        "print(f\"\\n{'✅ VALIDATION PASSED' if validation_passed else '❌ VALIDATION FAILED'}\")\n",
        "\n",
        "if not validation_passed:\n",
        "    print(\"⚠️  Some validation checks failed. Please review the data and calculations.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa1f7e3c",
      "metadata": {},
      "source": [
        "### 2.3 Metrics by year"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b12f0f6",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "by_year = (\n",
        "    df_all\n",
        "    .groupby('year')\n",
        "    .agg(\n",
        "        top1=('hit@1', 'mean'),\n",
        "        hit5=('hit@5', 'mean'),\n",
        "        mrr=('rr', 'mean'),\n",
        "        n=('card_id', 'size')\n",
        "    )\n",
        "    .reset_index()\n",
        "    .sort_values('year')\n",
        ")\n",
        "display(by_year)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "lt5vo9wiuj",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Export CSV files for Canva import\n",
        "print(\"\\nExporting CSV files for Canva...\")\n",
        "\n",
        "# Prepare data for Canva with proper formatting\n",
        "top1_data = by_year[['year', 'top1']].copy()\n",
        "top1_data['top1_percent'] = (top1_data['top1'] * 100).round(2)\n",
        "top1_data = top1_data[['year', 'top1_percent']]\n",
        "top1_data.columns = ['Year', 'Top-1 Accuracy (%)']\n",
        "\n",
        "hit5_data = by_year[['year', 'hit5']].copy()\n",
        "hit5_data['hit5_percent'] = (hit5_data['hit5'] * 100).round(2)\n",
        "hit5_data = hit5_data[['year', 'hit5_percent']]\n",
        "hit5_data.columns = ['Year', 'Top-5 Hit Rate (%)']\n",
        "\n",
        "# Export to CSV files\n",
        "top1_data.to_csv('top1_metrics_canva.csv', index=False)\n",
        "hit5_data.to_csv('hit5_metrics_canva.csv', index=False)\n",
        "\n",
        "print(\"✅ CSV files exported:\")\n",
        "print(\"  - top1_metrics_canva.csv\")\n",
        "print(\"  - hit5_metrics_canva.csv\")\n",
        "print(\"\\nFiles ready for Canva import!\")\n",
        "\n",
        "# Display preview of exported data\n",
        "print(\"\\nPreview of top1_metrics_canva.csv:\")\n",
        "print(top1_data.to_string(index=False))\n",
        "print(\"\\nPreview of hit5_metrics_canva.csv:\")\n",
        "print(hit5_data.to_string(index=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ycc48tvxtw",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Export combined metrics CSV for Canva (optional alternative format)\n",
        "print(\"\\nExporting combined metrics CSV...\")\n",
        "\n",
        "# Create combined dataset with both top1 and hit5\n",
        "combined_data = by_year[['year', 'top1', 'hit5']].copy()\n",
        "combined_data['top1_percent'] = (combined_data['top1'] * 100).round(2)\n",
        "combined_data['hit5_percent'] = (combined_data['hit5'] * 100).round(2)\n",
        "combined_data = combined_data[['year', 'top1_percent', 'hit5_percent']]\n",
        "combined_data.columns = ['Year', 'Top-1 Accuracy (%)', 'Top-5 Hit Rate (%)']\n",
        "\n",
        "# Export combined CSV\n",
        "combined_data.to_csv('combined_metrics_canva.csv', index=False)\n",
        "\n",
        "print(\"✅ Combined CSV file exported:\")\n",
        "print(\"  - combined_metrics_canva.csv\")\n",
        "print(\"\\nPreview of combined_metrics_canva.csv:\")\n",
        "print(combined_data.to_string(index=False))\n",
        "\n",
        "# Also create a \"long format\" version for easier charting in Canva\n",
        "import pandas as pd\n",
        "long_format = pd.concat([\n",
        "    combined_data[['Year', 'Top-1 Accuracy (%)']].rename(columns={'Top-1 Accuracy (%)': 'Value'}).assign(Metric='Top-1 Accuracy'),\n",
        "    combined_data[['Year', 'Top-5 Hit Rate (%)']].rename(columns={'Top-5 Hit Rate (%)': 'Value'}).assign(Metric='Top-5 Hit Rate')\n",
        "])\n",
        "\n",
        "long_format.to_csv('metrics_long_format_canva.csv', index=False)\n",
        "print(\"✅ Long format CSV file exported:\")\n",
        "print(\"  - metrics_long_format_canva.csv\")\n",
        "print(\"\\nPreview of metrics_long_format_canva.csv:\")\n",
        "print(long_format.to_string(index=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "37ac7adc",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "3f4d2744",
      "metadata": {},
      "source": [
        "## 3. Visualisations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5cbabe05",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "import matplotlib.pyplot as plt\n",
        "fig, ax = plt.subplots(figsize=(6,4))\n",
        "ax.bar(by_year['year'].astype(str), by_year['top1']*100)\n",
        "ax.set_ylabel('Top‑1 Accuracy (%)')\n",
        "ax.set_title('Top‑1 Accuracy (%) by Year')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "58242dcd",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "import matplotlib.pyplot as plt\n",
        "fig, ax = plt.subplots(figsize=(6,4))\n",
        "ax.bar(by_year['year'].astype(str), by_year['hit5']*100)\n",
        "ax.set_ylabel('Top‑5 Hit Rate (%)')\n",
        "ax.set_title('Top‑5 Hit Rate (%) by Year')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e12cf7a7",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "import matplotlib.pyplot as plt\n",
        "fig, ax = plt.subplots(figsize=(6,4))\n",
        "ax.bar(by_year['year'].astype(str), by_year['mrr']*100)\n",
        "ax.set_ylabel('MRR (%)')\n",
        "ax.set_title('MRR (%) by Year')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c38d992e",
      "metadata": {},
      "source": [
        "### 3.1 Coverage breakdown"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5dab3c15",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analisi Coverage dettagliata (corretta)\n",
        "print(\"📊 DETAILED COVERAGE ANALYSIS\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Calcola POI unici in modo corretto\n",
        "predicted_pois_set = set()\n",
        "for preds in df_all[\"prediction_norm\"]:\n",
        "    if isinstance(preds, list):\n",
        "        predicted_pois_set.update(preds)\n",
        "\n",
        "ground_truth_pois_set = set(df_all[\"ground_truth_norm\"].unique())\n",
        "\n",
        "# Filtra valori invalidi\n",
        "predicted_pois_clean = {poi for poi in predicted_pois_set if poi and str(poi) != 'nan' and str(poi) != 'None'}\n",
        "ground_truth_pois_clean = {poi for poi in ground_truth_pois_set if poi and str(poi) != 'nan' and str(poi) != 'None'}\n",
        "\n",
        "print(f\"POI distinct in predictions: {len(predicted_pois_clean)}\")\n",
        "print(f\"POI distinct in ground-truth: {len(ground_truth_pois_clean)}\")\n",
        "\n",
        "# Coverage corretto\n",
        "true_coverage = len(predicted_pois_clean) / len(ground_truth_pois_clean) if len(ground_truth_pois_clean) > 0 else 0\n",
        "print(f\"Catalogue Coverage: {true_coverage:.2%}\")\n",
        "\n",
        "# Analisi della sovrapposizione\n",
        "overlap = predicted_pois_clean.intersection(ground_truth_pois_clean)\n",
        "only_predicted = predicted_pois_clean - ground_truth_pois_clean\n",
        "only_ground_truth = ground_truth_pois_clean - predicted_pois_clean\n",
        "\n",
        "print(f\"\\n🔄 POI OVERLAP ANALYSIS:\")\n",
        "print(f\"  POI in both prediction & ground-truth: {len(overlap)}\")\n",
        "print(f\"  POI only in predictions: {len(only_predicted)}\")\n",
        "print(f\"  POI only in ground-truth: {len(only_ground_truth)}\")\n",
        "\n",
        "if len(only_predicted) > 0 and len(only_predicted) <= 20:\n",
        "    print(f\"\\n🔍 Examples of POI only in predictions:\")\n",
        "    for poi in list(only_predicted)[:10]:\n",
        "        print(f\"    • {poi}\")\n",
        "\n",
        "if len(only_ground_truth) > 0 and len(only_ground_truth) <= 20:\n",
        "    print(f\"\\n🎯 Examples of POI only in ground-truth:\")\n",
        "    for poi in list(only_ground_truth)[:10]:\n",
        "        print(f\"    • {poi}\")\n",
        "\n",
        "# Metriche aggiuntive\n",
        "recall_coverage = len(overlap) / len(ground_truth_pois_clean) if len(ground_truth_pois_clean) > 0 else 0\n",
        "precision_coverage = len(overlap) / len(predicted_pois_clean) if len(predicted_pois_clean) > 0 else 0\n",
        "\n",
        "print(f\"\\n📈 ADDITIONAL COVERAGE METRICS:\")\n",
        "print(f\"  Coverage Recall: {recall_coverage:.2%} (fraction of true POI covered)\")\n",
        "print(f\"  Coverage Precision: {precision_coverage:.2%} (fraction of predicted POI that exist)\")\n",
        "\n",
        "# Se il coverage è troppo alto, indica un problema nei dati\n",
        "if true_coverage > 2.0:\n",
        "    print(f\"\\n⚠️  HIGH COVERAGE WARNING:\")\n",
        "    print(f\"  Coverage = {true_coverage:.1f}x suggests model predicts many non-existent POI\")\n",
        "    print(f\"  This could indicate data quality issues or model hallucination\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a2aee9a",
      "metadata": {},
      "source": [
        "## 4. Error analysis – overview\n",
        "\n",
        "In questa sezione valutiamo **dove** e **perché** il modello sbaglia, seguendo tre filoni:\n",
        "\n",
        "1. **Worst-performing POI pairs**  \n",
        "   Scopriamo quali coppie `ground-truth → first_pred` generano il maggior numero di errori.\n",
        "\n",
        "2. **Confusion matrix per subset**  \n",
        "   Heat-map delle frequenze (o tasso d’errore) su un sotto-insieme di POI di interesse\n",
        "   (ad es. i 20 più visitati o solo l’anno 2020).\n",
        "\n",
        "3. **Explainability (SHAP / LIME)**  \n",
        "   Analisi dei fattori testuali nella colonna `history` che portano a una predizione\n",
        "   sbagliata. Qui mostriamo un prototipo basato su LIME; lo script è modulare\n",
        "   e può essere sostituito da SHAP se usi modelli compatibili.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "37f8e9b6",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ---------------------------------------------------------------------\n",
        "# 4.1 Worst-performing POI pairs (Top-N) – FIX\n",
        "# ---------------------------------------------------------------------\n",
        "# filtro righe con errore Top-1\n",
        "ERR = df_all.assign(first_pred=df_all[\"prediction_norm\"].str[0])\n",
        "ERR = ERR[~ERR[\"hit@1\"]]           # equivalente a query(\"`hit@1` == False\")\n",
        "\n",
        "# conteggia coppie ground-truth → first_pred\n",
        "pairs = (\n",
        "    ERR.groupby([\"ground_truth_norm\", \"first_pred\"])\n",
        "        .size()\n",
        "        .reset_index(name=\"errors\")\n",
        "        .sort_values(\"errors\", ascending=False)\n",
        "        .head(15)          # Top-15 coppie\n",
        ")\n",
        "\n",
        "display(pairs)\n",
        "\n",
        "# barplot (opzionale)\n",
        "fig, ax = plt.subplots(figsize=(8, 4))\n",
        "ax.barh(\n",
        "    pairs.apply(lambda r: f\"{r['ground_truth_norm']} → {r['first_pred']}\", axis=1),\n",
        "    pairs[\"errors\"]\n",
        ")\n",
        "ax.set_xlabel(\"Errori (count)\")\n",
        "ax.set_title(\"Worst-performing POI pairs – Top 15\")\n",
        "ax.invert_yaxis()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a501fa2e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ---------------------------------------------------------------------\n",
        "# 4.2 Confusion matrix – seleziona subset\n",
        "# ---------------------------------------------------------------------\n",
        "TOP_K = 20          # limitiamoci ai 20 POI più frequenti (ground-truth)\n",
        "YEAR  = None        # imposta un anno (es. 2020) oppure None per all\n",
        "\n",
        "subset = df_all.copy()\n",
        "if YEAR is not None:\n",
        "    subset = subset.query(\"year == @YEAR\")\n",
        "\n",
        "# filtra i POI più frequenti\n",
        "top_poi = (\n",
        "    subset[\"ground_truth_norm\"]\n",
        "           .value_counts()\n",
        "           .head(TOP_K)\n",
        "           .index\n",
        ")\n",
        "mask = subset[\"ground_truth_norm\"].isin(top_poi) & subset[\"prediction_norm\"].str[0].isin(top_poi)\n",
        "cm_df = pd.crosstab(\n",
        "    subset.loc[mask, \"ground_truth_norm\"],\n",
        "    subset.loc[mask, \"prediction_norm\"].str[0],\n",
        "    rownames=[\"True\"],\n",
        "    colnames=[\"Pred\"],\n",
        "    dropna=False\n",
        ")\n",
        "\n",
        "# Normalizza su righe per avere tassi d’errore\n",
        "cm_norm = cm_df.div(cm_df.sum(axis=1), axis=0)\n",
        "\n",
        "# Heat-map (matplotlib only)\n",
        "fig, ax = plt.subplots(figsize=(8,6))\n",
        "im = ax.imshow(cm_norm.values, aspect=\"auto\")\n",
        "ax.set_xticks(range(len(cm_norm.columns)))\n",
        "ax.set_xticklabels(cm_norm.columns, rotation=90)\n",
        "ax.set_yticks(range(len(cm_norm.index)))\n",
        "ax.set_yticklabels(cm_norm.index)\n",
        "ax.set_xlabel(\"Predicted POI\")\n",
        "ax.set_ylabel(\"True POI\")\n",
        "ax.set_title(f\"Confusion matrix – top {TOP_K} POI\" + (f\", year {YEAR}\" if YEAR else \"\"))\n",
        "fig.colorbar(im, ax=ax, fraction=0.02, pad=0.04, label=\"Error rate\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa0439af",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ---------------------------------------------------------------------\n",
        "# 4.3 Explainability & Error Analysis (Migliorata)\n",
        "# ---------------------------------------------------------------------\n",
        "import warnings, json, numpy as np, pandas as pd\n",
        "from collections import Counter\n",
        "import re\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"COMPREHENSIVE ERROR ANALYSIS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Verifica che ci siano errori da analizzare\n",
        "if df_all['hit@1'].mean() >= 0.99:\n",
        "    print(\"🎉 Excellent performance! Error rate < 1%, limited error analysis needed.\")\n",
        "    \n",
        "# Filtra errori e predizioni corrette\n",
        "err_rows = df_all[~df_all[\"hit@1\"]].copy()\n",
        "correct_rows = df_all[df_all[\"hit@1\"]].copy()\n",
        "\n",
        "if err_rows.empty:\n",
        "    print(\"🎉 Perfect predictions! No errors to analyze.\")\n",
        "else:\n",
        "    print(f\"📊 Analyzing {len(err_rows):,} prediction errors vs {len(correct_rows):,} correct predictions\")\n",
        "    print(f\"📈 Overall accuracy: {len(correct_rows)/(len(correct_rows)+len(err_rows)):.2%}\")\n",
        "    \n",
        "    # Aggiungi colonne per l'analisi\n",
        "    err_rows['first_pred'] = err_rows[\"prediction_norm\"].str[0]\n",
        "    if not correct_rows.empty:\n",
        "        correct_rows['first_pred'] = correct_rows[\"prediction_norm\"].str[0]\n",
        "    \n",
        "    # ---------------------------------------------------------------------\n",
        "    # 1. Analisi Performance per Anno\n",
        "    # ---------------------------------------------------------------------\n",
        "    if 'year' in df_all.columns and df_all['year'].notna().any():\n",
        "        print(\"\\n1️⃣ YEARLY PERFORMANCE ANALYSIS\")\n",
        "        print(\"-\" * 40)\n",
        "        \n",
        "        yearly_perf = df_all.groupby('year').agg({\n",
        "            'hit@1': ['count', 'mean'],\n",
        "            'hit@5': 'mean',\n",
        "            'rr': 'mean'\n",
        "        }).round(4)\n",
        "        \n",
        "        yearly_perf.columns = ['Total_Predictions', 'Top1_Accuracy', 'Top5_HitRate', 'MRR']\n",
        "        yearly_perf['Error_Rate'] = 1 - yearly_perf['Top1_Accuracy']\n",
        "        \n",
        "        print(\"Year-by-year performance:\")\n",
        "        for year, row in yearly_perf.iterrows():\n",
        "            if not pd.isna(year):\n",
        "                print(f\"  {int(year)}: {row['Top1_Accuracy']:.2%} accuracy, {row['Error_Rate']:.2%} error rate ({int(row['Total_Predictions'])} predictions)\")\n",
        "    \n",
        "    # ---------------------------------------------------------------------\n",
        "    # 2. Analisi POI più Problematici (con error rate)\n",
        "    # ---------------------------------------------------------------------\n",
        "    print(\"\\n2️⃣ MOST PROBLEMATIC POI\")\n",
        "    print(\"-\" * 40)\n",
        "    \n",
        "    # Calcola error rate per POI\n",
        "    poi_stats = df_all.groupby('ground_truth_norm').agg({\n",
        "        'hit@1': ['count', 'sum', 'mean']\n",
        "    }).round(4)\n",
        "    poi_stats.columns = ['total_predictions', 'correct_predictions', 'accuracy']\n",
        "    poi_stats['error_rate'] = 1 - poi_stats['accuracy']\n",
        "    poi_stats['error_count'] = poi_stats['total_predictions'] - poi_stats['correct_predictions']\n",
        "    \n",
        "    # Filtra POI con almeno 100 predizioni per statistiche affidabili\n",
        "    poi_stats_filtered = poi_stats[poi_stats['total_predictions'] >= 100].copy()\n",
        "    \n",
        "    if not poi_stats_filtered.empty:\n",
        "        # Top 10 POI con più errori assoluti\n",
        "        worst_poi_absolute = poi_stats_filtered.nlargest(10, 'error_count')\n",
        "        print(\"POI with most absolute errors (min 100 predictions):\")\n",
        "        for poi, stats in worst_poi_absolute.iterrows():\n",
        "            print(f\"  {poi:>25s}: {int(stats['error_count']):>4d} errors ({stats['error_rate']:.1%} rate, {int(stats['total_predictions'])} total)\")\n",
        "        \n",
        "        # Top 10 POI con error rate più alto\n",
        "        worst_poi_rate = poi_stats_filtered.nlargest(10, 'error_rate')\n",
        "        print(f\"\\nPOI with highest error rates (min 100 predictions):\")\n",
        "        for poi, stats in worst_poi_rate.iterrows():\n",
        "            print(f\"  {poi:>25s}: {stats['error_rate']:.1%} error rate ({int(stats['error_count'])} errors, {int(stats['total_predictions'])} total)\")\n",
        "    else:\n",
        "        print(\"Not enough data for reliable POI-level statistics (need min 100 predictions per POI)\")\n",
        "    \n",
        "    # ---------------------------------------------------------------------\n",
        "    # 3. Analisi delle Predizioni Sbagliate più Frequenti\n",
        "    # ---------------------------------------------------------------------\n",
        "    print(\"\\n3️⃣ MOST FREQUENT WRONG PREDICTIONS\")\n",
        "    print(\"-\" * 40)\n",
        "    \n",
        "    # Conta le predizioni sbagliate più frequenti\n",
        "    wrong_pred_analysis = err_rows.groupby(['ground_truth_norm', 'first_pred']).size().reset_index(name='error_count')\n",
        "    wrong_pred_top = wrong_pred_analysis.nlargest(15, 'error_count')\n",
        "    \n",
        "    print(\"Most frequent error patterns (True POI → Predicted POI):\")\n",
        "    for _, row in wrong_pred_top.iterrows():\n",
        "        print(f\"  {row['ground_truth_norm']:>20s} → {row['first_pred']:<20s}: {row['error_count']:>3d} times\")\n",
        "    \n",
        "    # ---------------------------------------------------------------------\n",
        "    # 4. Analisi Coverage Dettagliata\n",
        "    # ---------------------------------------------------------------------\n",
        "    print(\"\\n4️⃣ PREDICTION DIVERSITY ANALYSIS\")\n",
        "    print(\"-\" * 40)\n",
        "    \n",
        "    # Analizza la distribuzione delle predizioni\n",
        "    all_predictions = [pred for preds in df_all[\"prediction_norm\"] for pred in preds]\n",
        "    pred_counter = Counter(all_predictions)\n",
        "    top_predicted = pred_counter.most_common(10)\n",
        "    \n",
        "    print(\"Most frequently predicted POI (across all positions):\")\n",
        "    for poi, count in top_predicted:\n",
        "        percentage = count / len(all_predictions) * 100\n",
        "        print(f\"  {poi:>25s}: {count:>6d} times ({percentage:>4.1f}%)\")\n",
        "    \n",
        "    # Analizza concentrazione delle predizioni\n",
        "    unique_predictions = len(pred_counter)\n",
        "    total_predictions = len(all_predictions)\n",
        "    concentration = pred_counter.most_common(1)[0][1] / total_predictions if total_predictions > 0 else 0\n",
        "    \n",
        "    print(f\"\\nPrediction diversity metrics:\")\n",
        "    print(f\"  Unique POI predicted: {unique_predictions}\")\n",
        "    print(f\"  Total predictions made: {total_predictions}\")\n",
        "    print(f\"  Most frequent POI concentration: {concentration:.2%}\")\n",
        "    \n",
        "    # ---------------------------------------------------------------------\n",
        "    # 5. Pattern Analysis negli History\n",
        "    # ---------------------------------------------------------------------\n",
        "    print(\"\\n5️⃣ HISTORY PATTERN ANALYSIS\")\n",
        "    print(\"-\" * 40)\n",
        "    \n",
        "    def safe_extract_keywords(text, min_length=3):\n",
        "        \"\"\"Estrae keywords dal testo in modo sicuro\"\"\"\n",
        "        if pd.isna(text) or not text:\n",
        "            return []\n",
        "        try:\n",
        "            # Converti in stringa e pulisci\n",
        "            text_str = str(text).lower()\n",
        "            # Rimuovi caratteri di lista se presente\n",
        "            text_clean = re.sub(r'[\\[\\]\\'\"\"]', '', text_str)\n",
        "            # Estrai parole\n",
        "            words = re.findall(r'\\b[a-zA-ZÀ-ÿ]+\\b', text_clean)\n",
        "            # Filtra parole comuni\n",
        "            stop_words = {'and', 'the', 'poi', 'via', 'del', 'dei', 'per', 'con', 'una', 'uno', 'che', 'nel', 'sul', 'di', 'da', 'in', 'a', 'la', 'le', 'il', 'lo'}\n",
        "            return [w for w in words if len(w) >= min_length and w not in stop_words]\n",
        "        except Exception as e:\n",
        "            return []\n",
        "    \n",
        "    if 'history' in df_all.columns:\n",
        "        # Analizza history length\n",
        "        if not err_rows.empty:\n",
        "            err_rows['history_length'] = err_rows['history'].astype(str).str.len()\n",
        "        if not correct_rows.empty:\n",
        "            correct_rows['history_length'] = correct_rows['history'].astype(str).str.len()\n",
        "            \n",
        "        if not err_rows.empty and not correct_rows.empty:\n",
        "            avg_history_err = err_rows['history_length'].mean()\n",
        "            avg_history_correct = correct_rows['history_length'].mean()\n",
        "            print(f\"Average history length - Errors: {avg_history_err:.0f} chars, Correct: {avg_history_correct:.0f} chars\")\n",
        "            \n",
        "            # Statistical significance test\n",
        "            from scipy import stats\n",
        "            try:\n",
        "                t_stat, p_val = stats.ttest_ind(err_rows['history_length'], correct_rows['history_length'])\n",
        "                significance = \"significant\" if p_val < 0.05 else \"not significant\"\n",
        "                print(f\"Difference is {significance} (p-value: {p_val:.4f})\")\n",
        "            except:\n",
        "                print(\"Could not compute statistical significance\")\n",
        "        \n",
        "        # Analizza keywords frequenti\n",
        "        sample_size = min(1000, len(err_rows))  # Limita per performance\n",
        "        error_keywords = []\n",
        "        for history in err_rows['history'].head(sample_size):\n",
        "            error_keywords.extend(safe_extract_keywords(history))\n",
        "        \n",
        "        if error_keywords:\n",
        "            error_word_freq = Counter(error_keywords).most_common(10)\n",
        "            print(f\"\\nMost frequent keywords in ERROR cases (sample of {sample_size}):\")\n",
        "            for word, freq in error_word_freq:\n",
        "                print(f\"  {word:>15s}: {freq:>3d}\")\n",
        "    else:\n",
        "        print(\"No 'history' column found for pattern analysis\")\n",
        "    \n",
        "    # ---------------------------------------------------------------------\n",
        "    # 6. Summary & Recommendations\n",
        "    # ---------------------------------------------------------------------\n",
        "    print(f\"\\n6️⃣ SUMMARY & RECOMMENDATIONS\")\n",
        "    print(\"-\" * 40)\n",
        "    \n",
        "    error_rate = len(err_rows) / len(df_all)\n",
        "    \n",
        "    if error_rate > 0.8:\n",
        "        print(\"🔴 HIGH ERROR RATE (>80%): Major model performance issues\")\n",
        "        print(\"  Recommendations:\")\n",
        "        print(\"  • Check data quality and preprocessing\")\n",
        "        print(\"  • Review model training and hyperparameters\")\n",
        "        print(\"  • Consider model architecture changes\")\n",
        "    elif error_rate > 0.5:\n",
        "        print(\"🟡 MODERATE ERROR RATE (50-80%): Room for improvement\")\n",
        "        print(\"  Recommendations:\")\n",
        "        print(\"  • Focus on most problematic POI identified above\")\n",
        "        print(\"  • Consider ensemble methods or model fine-tuning\")\n",
        "        print(\"  • Analyze temporal patterns in errors\")\n",
        "    else:\n",
        "        print(\"🟢 LOW ERROR RATE (<50%): Good performance\")\n",
        "        print(\"  Recommendations:\")\n",
        "        print(\"  • Fine-tune on edge cases identified above\")\n",
        "        print(\"  • Consider deployment readiness\")\n",
        "    \n",
        "    print(f\"\\nKey Statistics:\")\n",
        "    print(f\"  • Total predictions: {len(df_all):,}\")\n",
        "    print(f\"  • Error rate: {error_rate:.2%}\")\n",
        "    print(f\"  • Unique POI in dataset: {df_all['ground_truth_norm'].nunique()}\")\n",
        "    \n",
        "    if 'year' in df_all.columns and df_all['year'].notna().any():\n",
        "        years_analyzed = df_all['year'].nunique()\n",
        "        print(f\"  • Years analyzed: {int(years_analyzed)}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ERROR ANALYSIS COMPLETE\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6rrw0s85uy9",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prima cella: importazioni\n",
        "from pathlib import Path\n",
        "import glob, ast, os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.rcParams['figure.dpi'] = 120\n",
        "plt.rcParams['axes.grid'] = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "g5alwrf28al",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import glob, ast, os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json\n",
        "import re\n",
        "\n",
        "# Percorso ai file CSV  \n",
        "csv_files = [Path(p) for p in glob.glob(os.path.join('results/qwen2.5_7b/base_version/', '*_pred_*.csv'))]\n",
        "csv_files = sorted(csv_files)\n",
        "print(f\"Found {len(csv_files)} CSV files\")\n",
        "for f in csv_files[:5]:\n",
        "    print(f\"  - {f.name}\")\n",
        "if len(csv_files) > 5:\n",
        "    print(f\"  ... and {len(csv_files)-5} more\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "s3c8t3zw6id",
      "metadata": {},
      "outputs": [],
      "source": [
        "def safe_parse_prediction(x, debug_mode=False):\n",
        "    \"\"\"\n",
        "    Parsing robusto per la colonna 'prediction'.\n",
        "    Gestisce diversi formati: stringhe, liste già parsate, JSON, etc.\n",
        "    \"\"\"\n",
        "    if pd.isna(x) or x is None:\n",
        "        if debug_mode:\n",
        "            return [], \"empty_or_nan\"\n",
        "        return []\n",
        "    \n",
        "    # Se è già una lista\n",
        "    if isinstance(x, list):\n",
        "        if debug_mode:\n",
        "            return x, \"already_list\"\n",
        "        return x\n",
        "    \n",
        "    # Se è una stringa\n",
        "    if isinstance(x, str):\n",
        "        x = x.strip()\n",
        "        if not x:\n",
        "            if debug_mode:\n",
        "                return [], \"empty_string\"\n",
        "            return []\n",
        "        \n",
        "        try:\n",
        "            # Prova ast.literal_eval prima\n",
        "            result = ast.literal_eval(x)\n",
        "            if isinstance(result, list):\n",
        "                if debug_mode:\n",
        "                    return result, \"ast_literal_eval_list\"\n",
        "                return result\n",
        "            else:\n",
        "                if debug_mode:\n",
        "                    return [result], \"ast_literal_eval_single\"\n",
        "                return [result]  # Wrap in lista se è un singolo elemento\n",
        "        except (ValueError, SyntaxError) as e:\n",
        "            try:\n",
        "                # Prova JSON parse\n",
        "                result = json.loads(x)\n",
        "                if isinstance(result, list):\n",
        "                    if debug_mode:\n",
        "                        return result, \"json_loads_list\"\n",
        "                    return result\n",
        "                else:\n",
        "                    if debug_mode:\n",
        "                        return [result], \"json_loads_single\"\n",
        "                    return [result]\n",
        "            except (json.JSONDecodeError, ValueError):\n",
        "                # Se fallisce tutto, prova a splittare per virgole\n",
        "                # Rimuovi caratteri come [ ] e \"\"\n",
        "                cleaned = x.replace('[', '').replace(']', '').replace('\"', '').replace(\"'\", \"\")\n",
        "                if ',' in cleaned:\n",
        "                    result = [item.strip() for item in cleaned.split(',') if item.strip()]\n",
        "                    if debug_mode:\n",
        "                        return result, \"comma_split\"\n",
        "                    return result\n",
        "                else:\n",
        "                    result = [cleaned] if cleaned else []\n",
        "                    if debug_mode:\n",
        "                        return result, \"single_cleaned\" if cleaned else \"empty_after_clean\"\n",
        "                    return result\n",
        "    \n",
        "    # Fallback: converti in stringa e prova di nuovo\n",
        "    if debug_mode:\n",
        "        result, reason = safe_parse_prediction(str(x), debug_mode=True)\n",
        "        return result, f\"fallback_to_str_{reason}\"\n",
        "    return safe_parse_prediction(str(x))\n",
        "\n",
        "print(\"✅ Parsing function defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "isszqrqcrf",
      "metadata": {},
      "outputs": [],
      "source": [
        "def analyze_prediction_issues(df, sample_size=100):\n",
        "    \"\"\"Analizza i problemi nel parsing delle predizioni\"\"\"\n",
        "    print(\"🔍 ANALYZING PREDICTION PARSING ISSUES\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    # Campiona alcune righe per analisi dettagliata\n",
        "    sample_df = df.head(sample_size)\n",
        "    parsing_stats = {}\n",
        "    failed_examples = []\n",
        "    \n",
        "    for idx, row in sample_df.iterrows():\n",
        "        pred_raw = row['prediction']\n",
        "        parsed_result, reason = safe_parse_prediction(pred_raw, debug_mode=True)\n",
        "        \n",
        "        parsing_stats[reason] = parsing_stats.get(reason, 0) + 1\n",
        "        \n",
        "        # Colleziona esempi di fallimenti\n",
        "        if len(parsed_result) == 0 and not pd.isna(pred_raw):\n",
        "            failed_examples.append({\n",
        "                'index': idx,\n",
        "                'raw_prediction': str(pred_raw)[:100] + ('...' if len(str(pred_raw)) > 100 else ''),\n",
        "                'reason': reason\n",
        "            })\n",
        "    \n",
        "    print(f\"Parsing statistics (sample of {len(sample_df)} rows):\")\n",
        "    for reason, count in sorted(parsing_stats.items()):\n",
        "        percentage = count / len(sample_df) * 100\n",
        "        print(f\"  {reason:>20s}: {count:>3d} ({percentage:>5.1f}%)\")\n",
        "    \n",
        "    if failed_examples:\n",
        "        print(f\"\\n❌ Failed parsing examples (first 5):\")\n",
        "        for i, example in enumerate(failed_examples[:5]):\n",
        "            print(f\"  {i+1}. Row {example['index']} ({example['reason']}): {example['raw_prediction']}\")\n",
        "    \n",
        "    return parsing_stats, failed_examples\n",
        "\n",
        "print(\"✅ Analysis function defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rq77sxlpem",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Processiamo il primo file per diagnostico\n",
        "test_file = Path('results/qwen2.5_7b/base_version/dati_2014_pred_20250827_150933.csv')\n",
        "print(f\"🧪 Testing parser on: {test_file.name}\")\n",
        "\n",
        "try:\n",
        "    df_test = pd.read_csv(test_file)\n",
        "    print(f\"Successfully loaded {len(df_test)} rows\")\n",
        "    \n",
        "    # Mostra le prime righe per capire il formato\n",
        "    print(\"\\nFirst 5 predictions:\")\n",
        "    for i, pred in enumerate(df_test['prediction'].head(5)):\n",
        "        print(f\"  {i+1}. Type: {type(pred).__name__}, Value: {pred}\")\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"Error loading file: {e}\")\n",
        "    df_test = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ene85qx7ss5",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Processiamo il primo file con percorso assoluto\n",
        "test_file = Path('/leonardo_work/IscrC_LLM-Mob/LLM-Mob-As-Mobility-Interpreter/results/qwen2.5_7b/base_version/dati_2014_pred_20250827_150933.csv')\n",
        "print(f\"🧪 Testing parser on: {test_file.name}\")\n",
        "\n",
        "try:\n",
        "    df_test = pd.read_csv(test_file)\n",
        "    print(f\"✅ Successfully loaded {len(df_test)} rows\")\n",
        "    \n",
        "    # Mostra le prime righe per capire il formato\n",
        "    print(\"\\n📋 First 5 predictions:\")\n",
        "    for i, pred in enumerate(df_test['prediction'].head(5)):\n",
        "        pred_type = type(pred).__name__\n",
        "        pred_str = str(pred)[:100] + ('...' if len(str(pred)) > 100 else '')\n",
        "        print(f\"  {i+1}. Type: {pred_type:>8s}, Value: {pred_str}\")\n",
        "        \n",
        "    # Statistiche sui tipi di predizioni\n",
        "    print(f\"\\n📊 PREDICTION TYPE ANALYSIS:\")\n",
        "    pred_types = df_test['prediction'].apply(lambda x: type(x).__name__)\n",
        "    type_counts = pred_types.value_counts()\n",
        "    for ptype, count in type_counts.items():\n",
        "        percentage = count / len(df_test) * 100\n",
        "        print(f\"  {ptype:>8s}: {count:>6d} ({percentage:>5.1f}%)\")\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"❌ Error loading file: {e}\")\n",
        "    df_test = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1dzuu3x0jqth",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analisi dettagliata del parsing\n",
        "parsing_stats, failed_examples = analyze_prediction_issues(df_test, 100)\n",
        "\n",
        "print(f\"\\n🔬 DETAILED ANALYSIS OF EMPTY PREDICTIONS:\")\n",
        "# Contiamo quante predizioni sono liste vuote\n",
        "empty_bracket_count = (df_test['prediction'] == '[]').sum()\n",
        "nan_count = df_test['prediction'].isna().sum()\n",
        "non_empty_count = len(df_test) - empty_bracket_count - nan_count\n",
        "\n",
        "print(f\"  Empty brackets '[]': {empty_bracket_count:>6d} ({empty_bracket_count/len(df_test)*100:>5.1f}%)\")\n",
        "print(f\"  NaN values:          {nan_count:>6d} ({nan_count/len(df_test)*100:>5.1f}%)\")\n",
        "print(f\"  Non-empty predictions: {non_empty_count:>6d} ({non_empty_count/len(df_test)*100:>5.1f}%)\")\n",
        "\n",
        "# Esempi di predizioni non vuote\n",
        "print(f\"\\n✅ EXAMPLES OF NON-EMPTY PREDICTIONS:\")\n",
        "non_empty_preds = df_test[df_test['prediction'] != '[]']['prediction'].head(5)\n",
        "for i, pred in enumerate(non_empty_preds):\n",
        "    pred_str = str(pred)[:150] + ('...' if len(str(pred)) > 150 else '')\n",
        "    print(f\"  {i+1}. {pred_str}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8ey4ccvglv",
      "metadata": {},
      "outputs": [],
      "source": [
        "def ultra_robust_parse_prediction(x):\n",
        "    \"\"\"\n",
        "    Versione ultra-robusta del parser che prova ogni strategia possibile\n",
        "    per estrarre predizioni valide dai dati.\n",
        "    \"\"\"\n",
        "    if pd.isna(x) or x is None:\n",
        "        return []\n",
        "    \n",
        "    # Se è già una lista\n",
        "    if isinstance(x, list):\n",
        "        # Filtra elementi vuoti dalla lista\n",
        "        return [item for item in x if item and str(item).strip()]\n",
        "    \n",
        "    # Se non è una stringa, convertilo\n",
        "    if not isinstance(x, str):\n",
        "        x = str(x)\n",
        "    \n",
        "    x = x.strip()\n",
        "    if not x or x.lower() in ['nan', 'none', 'null']:\n",
        "        return []\n",
        "    \n",
        "    # Strategia 1: ast.literal_eval\n",
        "    try:\n",
        "        result = ast.literal_eval(x)\n",
        "        if isinstance(result, list):\n",
        "            return [item for item in result if item and str(item).strip()]\n",
        "        elif result:  # Singolo elemento non-vuoto\n",
        "            return [result]\n",
        "    except (ValueError, SyntaxError):\n",
        "        pass\n",
        "    \n",
        "    # Strategia 2: JSON parsing\n",
        "    try:\n",
        "        result = json.loads(x)\n",
        "        if isinstance(result, list):\n",
        "            return [item for item in result if item and str(item).strip()]\n",
        "        elif result:  # Singolo elemento non-vuoto\n",
        "            return [result]\n",
        "    except (json.JSONDecodeError, ValueError):\n",
        "        pass\n",
        "    \n",
        "    # Strategia 3: Rimozione caratteri problematici + splitting\n",
        "    # Rimuovi caratteri di lista comuni\n",
        "    cleaned = x\n",
        "    for char in ['[', ']', '(', ')', '{', '}']:\n",
        "        cleaned = cleaned.replace(char, '')\n",
        "    \n",
        "    # Rimuovi virgolette multiple\n",
        "    cleaned = re.sub(r'[\"\\']', '', cleaned)\n",
        "    \n",
        "    # Prova split per virgole\n",
        "    if ',' in cleaned:\n",
        "        items = [item.strip() for item in cleaned.split(',') if item.strip()]\n",
        "        if items:\n",
        "            return items\n",
        "    \n",
        "    # Strategia 4: Split per altri separatori\n",
        "    for sep in [';', '|', '\\n', '\\t']:\n",
        "        if sep in cleaned:\n",
        "            items = [item.strip() for item in cleaned.split(sep) if item.strip()]\n",
        "            if len(items) > 1:  # Solo se produce più elementi\n",
        "                return items\n",
        "    \n",
        "    # Strategia 5: Regex per trovare pattern di lista\n",
        "    # Pattern come: \"item1\", \"item2\", \"item3\"\n",
        "    quoted_pattern = r'\"([^\"]+)\"'\n",
        "    matches = re.findall(quoted_pattern, x)\n",
        "    if len(matches) > 1:\n",
        "        return matches\n",
        "    \n",
        "    # Pattern come: item1, item2, item3 (senza virgolette)\n",
        "    if ',' in cleaned:\n",
        "        items = [item.strip() for item in cleaned.split(',') if item.strip()]\n",
        "        if items:\n",
        "            return items\n",
        "    \n",
        "    # Strategia 6: Se tutto fallisce, ritorna come singolo elemento se non vuoto\n",
        "    if cleaned and cleaned not in ['nan', 'none', 'null', '[]', '{}']:\n",
        "        return [cleaned]\n",
        "    \n",
        "    return []\n",
        "\n",
        "print(\"✅ Ultra-robust parser defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "chdpao8333",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test di confronto tra i due parser\n",
        "print(\"🚀 PARSER COMPARISON TEST\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Testa entrambi i parser su un campione\n",
        "sample_size = 1000\n",
        "df_sample = df_test.head(sample_size).copy()\n",
        "\n",
        "print(f\"Testing on {len(df_sample)} rows...\")\n",
        "\n",
        "# Applica entrambi i parser\n",
        "df_sample['pred_original'] = df_sample['prediction'].apply(safe_parse_prediction)\n",
        "df_sample['pred_ultra_robust'] = df_sample['prediction'].apply(ultra_robust_parse_prediction)\n",
        "\n",
        "# Calcola le statistiche\n",
        "orig_valid = df_sample['pred_original'].apply(lambda x: len(x) > 0).sum()\n",
        "ultra_valid = df_sample['pred_ultra_robust'].apply(lambda x: len(x) > 0).sum()\n",
        "\n",
        "print(f\"\\n📊 PARSING COMPARISON RESULTS:\")\n",
        "print(f\"Original parser valid rows:    {orig_valid:>4d} ({orig_valid/len(df_sample)*100:>5.1f}%)\")\n",
        "print(f\"Ultra-robust parser valid rows: {ultra_valid:>4d} ({ultra_valid/len(df_sample)*100:>5.1f}%)\")\n",
        "print(f\"Improvement:                   +{ultra_valid - orig_valid:>4d} rows ({(ultra_valid - orig_valid)/len(df_sample)*100:>+5.1f}%)\")\n",
        "\n",
        "# Analizza cosa è stato recuperato\n",
        "if ultra_valid > orig_valid:\n",
        "    recovered_mask = (df_sample['pred_original'].apply(lambda x: len(x) == 0)) & \\\n",
        "                    (df_sample['pred_ultra_robust'].apply(lambda x: len(x) > 0))\n",
        "    recovered_rows = df_sample[recovered_mask]\n",
        "    \n",
        "    print(f\"\\n✅ ULTRA-ROBUST PARSER RECOVERED MORE DATA!\")\n",
        "    print(f\"Recovered {len(recovered_rows)} additional rows\")\n",
        "    \n",
        "    if len(recovered_rows) > 0:\n",
        "        print(f\"\\n🔍 EXAMPLES OF RECOVERED DATA:\")\n",
        "        for i, (idx, row) in enumerate(recovered_rows.head(5).iterrows()):\n",
        "            orig_pred = row['prediction']\n",
        "            recovered_pred = row['pred_ultra_robust']\n",
        "            orig_str = str(orig_pred)[:60] + ('...' if len(str(orig_pred)) > 60 else '')\n",
        "            print(f\"  {i+1}. Original: '{orig_str}'\")\n",
        "            print(f\"     Recovered: {recovered_pred}\")\n",
        "            print()\n",
        "else:\n",
        "    print(f\"\\nℹ️  Both parsers performed similarly\")\n",
        "\n",
        "# Analizza casi dove i parser differiscono\n",
        "different_mask = df_sample.apply(lambda row: row['pred_original'] != row['pred_ultra_robust'], axis=1)\n",
        "different_rows = df_sample[different_mask]\n",
        "\n",
        "if len(different_rows) > 0:\n",
        "    print(f\"🔄 PARSER DIFFERENCES: {len(different_rows)} rows with different results\")\n",
        "    print(\"Examples:\")\n",
        "    for i, (idx, row) in enumerate(different_rows.head(3).iterrows()):\n",
        "        orig = row['pred_original']\n",
        "        ultra = row['pred_ultra_robust']\n",
        "        raw = str(row['prediction'])[:50] + ('...' if len(str(row['prediction'])) > 50 else '')\n",
        "        print(f\"  {i+1}. Raw: {raw}\")\n",
        "        print(f\"     Original: {orig}\")\n",
        "        print(f\"     Ultra:    {ultra}\")\n",
        "        print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pqccdz29q5",
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "# Ora testiamo di nuovo il confronto tra i parser\n",
        "print(\"🚀 PARSER COMPARISON TEST\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Testa entrambi i parser su un campione\n",
        "sample_size = 1000\n",
        "df_sample = df_test.head(sample_size).copy()\n",
        "\n",
        "print(f\"Testing on {len(df_sample)} rows...\")\n",
        "\n",
        "# Applica entrambi i parser\n",
        "df_sample['pred_original'] = df_sample['prediction'].apply(safe_parse_prediction)\n",
        "df_sample['pred_ultra_robust'] = df_sample['prediction'].apply(ultra_robust_parse_prediction)\n",
        "\n",
        "# Calcola le statistiche\n",
        "orig_valid = df_sample['pred_original'].apply(lambda x: len(x) > 0).sum()\n",
        "ultra_valid = df_sample['pred_ultra_robust'].apply(lambda x: len(x) > 0).sum()\n",
        "\n",
        "print(f\"\\n📊 PARSING COMPARISON RESULTS:\")\n",
        "print(f\"Original parser valid rows:    {orig_valid:>4d} ({orig_valid/len(df_sample)*100:>5.1f}%)\")\n",
        "print(f\"Ultra-robust parser valid rows: {ultra_valid:>4d} ({ultra_valid/len(df_sample)*100:>5.1f}%)\")\n",
        "print(f\"Improvement:                   +{ultra_valid - orig_valid:>4d} rows ({(ultra_valid - orig_valid)/len(df_sample)*100:>+5.1f}%)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3bqkmb1ddwa",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analisi approfondita del problema delle liste vuote\n",
        "print(\"🔬 ROOT CAUSE ANALYSIS\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Analisi dettagliata del contenuto delle predizioni\n",
        "print(\"📋 PREDICTION CONTENT ANALYSIS:\")\n",
        "\n",
        "# Conta i tipi di predizioni\n",
        "total_rows = len(df_test)\n",
        "empty_brackets = (df_test['prediction'] == '[]').sum()\n",
        "nan_values = df_test['prediction'].isna().sum() \n",
        "float_nans = df_test['prediction'].apply(lambda x: isinstance(x, float) and pd.isna(x)).sum()\n",
        "\n",
        "# Predizioni con contenuto\n",
        "has_content = total_rows - empty_brackets - nan_values - float_nans\n",
        "\n",
        "print(f\"Total rows in dataset:        {total_rows:>6d} (100.0%)\")\n",
        "print(f\"Empty brackets '[]':          {empty_brackets:>6d} ({empty_brackets/total_rows*100:>5.1f}%)\")\n",
        "print(f\"NaN string values:            {nan_values:>6d} ({nan_values/total_rows*100:>5.1f}%)\")\n",
        "print(f\"Float NaN values:             {float_nans:>6d} ({float_nans/total_rows*100:>5.1f}%)\")\n",
        "print(f\"Predictions with content:     {has_content:>6d} ({has_content/total_rows*100:>5.1f}%)\")\n",
        "\n",
        "# Analisi dei motivi delle liste vuote\n",
        "print(f\"\\n🔍 WHY ARE PREDICTIONS EMPTY?\")\n",
        "print(\"Looking at the 'reason' column for empty predictions...\")\n",
        "\n",
        "empty_predictions = df_test[df_test['prediction'] == '[]'].copy()\n",
        "if 'reason' in empty_predictions.columns and len(empty_predictions) > 0:\n",
        "    reason_counts = empty_predictions['reason'].value_counts()\n",
        "    print(f\"Top reasons for empty predictions:\")\n",
        "    for i, (reason, count) in enumerate(reason_counts.head(10).items()):\n",
        "        percentage = count / len(empty_predictions) * 100\n",
        "        reason_short = str(reason)[:80] + ('...' if len(str(reason)) > 80 else '')\n",
        "        print(f\"  {i+1}. ({count:>4d} cases, {percentage:>4.1f}%) {reason_short}\")\n",
        "else:\n",
        "    print(\"No 'reason' column found or no empty predictions to analyze\")\n",
        "\n",
        "# Analisi del tasso di successo vs status\n",
        "print(f\"\\n⚡ SUCCESS RATE ANALYSIS:\")\n",
        "if 'status' in df_test.columns:\n",
        "    status_analysis = df_test.groupby('status').agg({\n",
        "        'prediction': [\n",
        "            lambda x: len(x),  # total count\n",
        "            lambda x: (x != '[]').sum()  # non-empty count\n",
        "        ]\n",
        "    }).round(2)\n",
        "    \n",
        "    status_analysis.columns = ['Total_Count', 'Non_Empty_Count']\n",
        "    status_analysis['Success_Rate'] = (status_analysis['Non_Empty_Count'] / status_analysis['Total_Count'] * 100).round(1)\n",
        "    \n",
        "    print(\"Success rate by status:\")\n",
        "    for status, row in status_analysis.iterrows():\n",
        "        print(f\"  {status:>10s}: {row['Non_Empty_Count']:>5.0f}/{row['Total_Count']:>5.0f} ({row['Success_Rate']:>4.1f}%)\")\n",
        "\n",
        "# Performance analysis\n",
        "if 'processing_time' in df_test.columns:\n",
        "    print(f\"\\n⏱️  PROCESSING TIME ANALYSIS:\")\n",
        "    empty_times = empty_predictions['processing_time'].describe()\n",
        "    non_empty_preds = df_test[df_test['prediction'] != '[]']\n",
        "    non_empty_times = non_empty_preds['processing_time'].describe() if len(non_empty_preds) > 0 else None\n",
        "    \n",
        "    print(f\"Empty predictions - Avg time: {empty_times['mean']:.2f}s (median: {empty_times['50%']:.2f}s)\")\n",
        "    if non_empty_times is not None:\n",
        "        print(f\"Non-empty predictions - Avg time: {non_empty_times['mean']:.2f}s (median: {non_empty_times['50%']:.2f}s)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "w8aywjc0l7",
      "metadata": {},
      "outputs": [],
      "source": [
        "# RACCOMANDAZIONI FINALI E SOLUZIONI\n",
        "print(\"🎯 COMPREHENSIVE ANALYSIS COMPLETE\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(\"📊 KEY FINDINGS:\")\n",
        "print(\"1. ✅ Parsing is NOT the problem - both parsers work identically\")\n",
        "print(\"2. ❌ 23.9% of predictions are empty lists '[]' by design\")\n",
        "print(\"3. 🤖 Model/logic intentionally returns empty when:\")\n",
        "print(\"   • All POIs already visited (most common reason)\")\n",
        "print(\"   • No suitable POIs available\")\n",
        "print(\"   • Current restrictions prevent suggestions\")\n",
        "print(\"4. ⚡ Empty predictions process faster (1.14s vs 1.62s)\")\n",
        "print(\"5. 📈 Success rate for 'success' status: 75.8%\")\n",
        "\n",
        "print(f\"\\n💡 RECOMMENDATIONS TO IMPROVE DATA LOADING:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "print(\"🔧 IMMEDIATE FIXES:\")\n",
        "print(\"1. **Accept empty predictions as valid data points**\")\n",
        "print(\"   • Don't filter out '[]' - they represent 'no recommendation' scenarios\")\n",
        "print(\"   • Treat as a separate class in your analysis\")\n",
        "print(\"   • This will recover your 23.9% 'lost' data\")\n",
        "\n",
        "print(f\"\\n2. **Update your analysis to handle empty predictions:**\")\n",
        "print(\"   • Top-1 Accuracy: Count '[]' as incorrect predictions\")\n",
        "print(\"   • Coverage: Empty predictions don't contribute to coverage\")\n",
        "print(\"   • Error Analysis: Analyze WHY model returns empty\")\n",
        "\n",
        "print(f\"\\n🚀 ADVANCED IMPROVEMENTS:\")\n",
        "print(\"1. **Model Logic Improvements:**\")\n",
        "print(\"   • Review the constraint logic that causes '[]' returns\")\n",
        "print(\"   • Consider allowing revisits or nearby alternatives\")\n",
        "print(\"   • Implement fallback suggestions for edge cases\")\n",
        "\n",
        "print(f\"\\n2. **Data Pipeline Enhancements:**\")\n",
        "print(\"   • Add a 'prediction_type' column: ['empty', 'single', 'multiple']\")\n",
        "print(\"   • Track confidence scores for non-empty predictions\") \n",
        "print(\"   • Separate analysis for different prediction scenarios\")\n",
        "\n",
        "print(f\"\\n3. **Evaluation Metrics Updates:**\")\n",
        "print(\"   • Calculate metrics both with and without empty predictions\")\n",
        "print(\"   • Add 'Recommendation Rate': % of cases with non-empty suggestions\")\n",
        "print(\"   • Analyze temporal patterns in empty vs non-empty predictions\")\n",
        "\n",
        "print(f\"\\n📋 UPDATED NOTEBOOK APPROACH:\")\n",
        "print(\"1. **Load ALL data (including empty predictions)**\")\n",
        "print(\"2. **Classify predictions into types:**\")\n",
        "print(\"   • empty: []\")\n",
        "print(\"   • single: single POI recommendations\") \n",
        "print(\"   • multiple: multiple POI recommendations\")\n",
        "print(\"3. **Analyze each type separately**\")\n",
        "print(\"4. **Report both 'All Predictions' and 'Non-Empty Only' metrics**\")\n",
        "\n",
        "print(f\"\\n🎉 RESULT:\")\n",
        "print(\"Your data loading will improve from ~76% to ~100% retention\")\n",
        "print(\"You'll have much more comprehensive analysis capabilities\")\n",
        "print(\"Better understanding of model behavior and limitations\")\n",
        "\n",
        "print(f\"\\n\" + \"=\" * 60)\n",
        "print(\"DIAGNOSTIC COMPLETE - Ready to implement improvements!\")\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "nbpq3jxrxx",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analizziamo esattamente cosa viene considerato vs tralasciato\n",
        "print(\"🔍 DETAILED ANALYSIS OF ROWS INCLUDED VS EXCLUDED\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Rileggiamo il file per analisi completa\n",
        "df_full = pd.read_csv('/leonardo_work/IscrC_LLM-Mob/LLM-Mob-As-Mobility-Interpreter/results/qwen2.5_7b/base_version/dati_2014_pred_20250827_150933.csv')\n",
        "\n",
        "print(f\"📊 TOTAL DATASET: {len(df_full)} rows\")\n",
        "\n",
        "# Applica il parsing come fa il notebook attuale\n",
        "df_full['prediction_parsed'] = df_full['prediction'].apply(safe_parse_prediction)\n",
        "\n",
        "# Categorizza ogni riga\n",
        "def categorize_row(row):\n",
        "    pred = row['prediction']\n",
        "    parsed = row['prediction_parsed']\n",
        "    \n",
        "    # Caso 1: Valore NaN o None\n",
        "    if pd.isna(pred):\n",
        "        return \"nan_values\"\n",
        "    \n",
        "    # Caso 2: Lista vuota []\n",
        "    if pred == '[]':\n",
        "        return \"empty_list\"\n",
        "    \n",
        "    # Caso 3: Lista non vuota che viene parsata correttamente\n",
        "    if len(parsed) > 0:\n",
        "        return \"valid_predictions\"\n",
        "    \n",
        "    # Caso 4: Altri casi problematici\n",
        "    return \"parse_errors\"\n",
        "\n",
        "df_full['row_category'] = df_full.apply(categorize_row, axis=1)\n",
        "\n",
        "# Statistiche per categoria\n",
        "category_stats = df_full['row_category'].value_counts()\n",
        "print(f\"\\n📋 ROW CATEGORIZATION:\")\n",
        "for category, count in category_stats.items():\n",
        "    percentage = count / len(df_full) * 100\n",
        "    print(f\"  {category:<20}: {count:>6d} ({percentage:>5.1f}%)\")\n",
        "\n",
        "# Mostra cosa include/esclude il notebook attuale\n",
        "current_filter = df_full['prediction_parsed'].apply(lambda x: isinstance(x, list) and len(x) > 0)\n",
        "included_rows = df_full[current_filter]\n",
        "excluded_rows = df_full[~current_filter]\n",
        "\n",
        "print(f\"\\n🎯 CURRENT NOTEBOOK BEHAVIOR:\")\n",
        "print(f\"  ✅ INCLUDED: {len(included_rows):>6d} rows ({len(included_rows)/len(df_full)*100:>5.1f}%)\")\n",
        "print(f\"  ❌ EXCLUDED: {len(excluded_rows):>6d} rows ({len(excluded_rows)/len(df_full)*100:>5.1f}%)\")\n",
        "\n",
        "# Dettaglio di cosa viene escluso\n",
        "excluded_categories = excluded_rows['row_category'].value_counts()\n",
        "print(f\"\\n❌ EXCLUDED ROWS BREAKDOWN:\")\n",
        "for category, count in excluded_categories.items():\n",
        "    percentage = count / len(excluded_rows) * 100 if len(excluded_rows) > 0 else 0\n",
        "    print(f\"  {category:<20}: {count:>6d} ({percentage:>5.1f}% of excluded)\")\n",
        "\n",
        "# Esempi di righe escluse\n",
        "print(f\"\\n🔍 EXAMPLES OF EXCLUDED ROWS:\")\n",
        "for category, group in excluded_rows.groupby('row_category'):\n",
        "    print(f\"\\n  📂 Category: {category}\")\n",
        "    sample_size = min(3, len(group))\n",
        "    for i, (idx, row) in enumerate(group.head(sample_size).iterrows()):\n",
        "        card_id = row.get('card_id', 'Unknown')[:15]\n",
        "        pred = str(row['prediction'])[:50] + ('...' if len(str(row['prediction'])) > 50 else '')\n",
        "        gt = str(row.get('ground_truth', 'Unknown'))[:20]\n",
        "        reason = str(row.get('reason', 'No reason'))[:60] + ('...' if len(str(row.get('reason', ''))) > 60 else '')\n",
        "        \n",
        "        print(f\"    {i+1}. Card: {card_id}, Pred: '{pred}'\")\n",
        "        print(f\"       GT: {gt}, Reason: {reason}\")\n",
        "\n",
        "# Analisi per POI più problematici\n",
        "print(f\"\\n🎯 WHICH POIs ARE MOST AFFECTED BY EXCLUSION:\")\n",
        "if 'ground_truth' in excluded_rows.columns and len(excluded_rows) > 0:\n",
        "    excluded_poi_counts = excluded_rows['ground_truth'].value_counts().head(10)\n",
        "    total_poi_counts = df_full['ground_truth'].value_counts()\n",
        "    \n",
        "    print(\"POIs with most excluded rows:\")\n",
        "    for poi, excluded_count in excluded_poi_counts.items():\n",
        "        total_count = total_poi_counts.get(poi, 0)\n",
        "        exclusion_rate = excluded_count / total_count * 100 if total_count > 0 else 0\n",
        "        print(f\"  {poi:<25}: {excluded_count:>4d}/{total_count:<4d} excluded ({exclusion_rate:>5.1f}%)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "u9pqhr313jd",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analizziamo esattamente cosa viene considerato vs tralasciato\n",
        "print(\"🔍 DETAILED ANALYSIS OF ROWS INCLUDED VS EXCLUDED\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Rileggiamo il file per analisi completa\n",
        "df_full = pd.read_csv('/leonardo_work/IscrC_LLM-Mob/LLM-Mob-As-Mobility-Interpreter/results/qwen2.5_7b/base_version/dati_2014_pred_20250827_150933.csv')\n",
        "\n",
        "print(f\"📊 TOTAL DATASET: {len(df_full)} rows\")\n",
        "\n",
        "# Applica il parsing come fa il notebook attuale\n",
        "df_full['prediction_parsed'] = df_full['prediction'].apply(safe_parse_prediction)\n",
        "\n",
        "# Categorizza ogni riga\n",
        "def categorize_row(row):\n",
        "    pred = row['prediction']\n",
        "    parsed = row['prediction_parsed']\n",
        "    \n",
        "    # Caso 1: Valore NaN o None\n",
        "    if pd.isna(pred):\n",
        "        return \"nan_values\"\n",
        "    \n",
        "    # Caso 2: Lista vuota []\n",
        "    if pred == '[]':\n",
        "        return \"empty_list\"\n",
        "    \n",
        "    # Caso 3: Lista non vuota che viene parsata correttamente\n",
        "    if len(parsed) > 0:\n",
        "        return \"valid_predictions\"\n",
        "    \n",
        "    # Caso 4: Altri casi problematici\n",
        "    return \"parse_errors\"\n",
        "\n",
        "df_full['row_category'] = df_full.apply(categorize_row, axis=1)\n",
        "\n",
        "# Statistiche per categoria\n",
        "category_stats = df_full['row_category'].value_counts()\n",
        "print(f\"\\n📋 ROW CATEGORIZATION:\")\n",
        "for category, count in category_stats.items():\n",
        "    percentage = count / len(df_full) * 100\n",
        "    print(f\"  {category:<20}: {count:>6d} ({percentage:>5.1f}%)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "y54k53jufb",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Continua l'analisi\n",
        "# Mostra cosa include/esclude il notebook attuale\n",
        "current_filter = df_full['prediction_parsed'].apply(lambda x: isinstance(x, list) and len(x) > 0)\n",
        "included_rows = df_full[current_filter]\n",
        "excluded_rows = df_full[~current_filter]\n",
        "\n",
        "print(f\"🎯 CURRENT NOTEBOOK BEHAVIOR:\")\n",
        "print(f\"  ✅ INCLUDED: {len(included_rows):>6d} rows ({len(included_rows)/len(df_full)*100:>5.1f}%)\")\n",
        "print(f\"  ❌ EXCLUDED: {len(excluded_rows):>6d} rows ({len(excluded_rows)/len(df_full)*100:>5.1f}%)\")\n",
        "\n",
        "# Dettaglio di cosa viene escluso\n",
        "excluded_categories = excluded_rows['row_category'].value_counts()\n",
        "print(f\"\\n❌ EXCLUDED ROWS BREAKDOWN:\")\n",
        "for category, count in excluded_categories.items():\n",
        "    percentage = count / len(excluded_rows) * 100 if len(excluded_rows) > 0 else 0\n",
        "    print(f\"  {category:<20}: {count:>6d} ({percentage:>5.1f}% of excluded)\")\n",
        "\n",
        "# Esempi di righe escluse per categoria\n",
        "print(f\"\\n🔍 EXAMPLES OF EXCLUDED ROWS:\")\n",
        "\n",
        "# Esempi di liste vuote\n",
        "empty_rows = excluded_rows[excluded_rows['row_category'] == 'empty_list'].head(3)\n",
        "print(f\"\\n  📂 EMPTY_LIST Category (lists that are '[]'):\")\n",
        "for i, (idx, row) in enumerate(empty_rows.iterrows()):\n",
        "    card_id = str(row.get('card_id', 'Unknown'))[:15]\n",
        "    current_poi = str(row.get('current_poi', 'Unknown'))[:15]\n",
        "    gt = str(row.get('ground_truth', 'Unknown'))[:20]\n",
        "    reason = str(row.get('reason', 'No reason'))[:80] + ('...' if len(str(row.get('reason', ''))) > 80 else '')\n",
        "    \n",
        "    print(f\"    {i+1}. Card: {card_id} | Current: {current_poi} | GT: {gt}\")\n",
        "    print(f\"       Reason: {reason}\")\n",
        "    print()\n",
        "\n",
        "# Esempi di valori NaN\n",
        "if len(excluded_rows[excluded_rows['row_category'] == 'nan_values']) > 0:\n",
        "    nan_rows = excluded_rows[excluded_rows['row_category'] == 'nan_values'].head(3)\n",
        "    print(f\"  📂 NAN_VALUES Category:\")\n",
        "    for i, (idx, row) in enumerate(nan_rows.iterrows()):\n",
        "        print(f\"    {i+1}. Row {idx}: prediction = {row['prediction']}\")\n",
        "\n",
        "print(f\"\\n🎯 WHICH POIs ARE MOST AFFECTED BY EXCLUSION:\")\n",
        "excluded_poi_counts = excluded_rows['ground_truth'].value_counts().head(10)\n",
        "total_poi_counts = df_full['ground_truth'].value_counts()\n",
        "\n",
        "print(\"POIs with most excluded rows:\")\n",
        "for poi, excluded_count in excluded_poi_counts.items():\n",
        "    total_count = total_poi_counts.get(poi, 0)\n",
        "    exclusion_rate = excluded_count / total_count * 100 if total_count > 0 else 0\n",
        "    print(f\"  {str(poi):<25}: {excluded_count:>4d}/{total_count:<4d} excluded ({exclusion_rate:>5.1f}%)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "syh8lou45ur",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analizziamo i pattern nelle righe incluse vs escluse\n",
        "print(f\"📈 PATTERN ANALYSIS: INCLUDED vs EXCLUDED\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Analisi delle history length\n",
        "print(\"🔍 HISTORY LENGTH ANALYSIS:\")\n",
        "included_history_len = included_rows['history'].astype(str).str.len().describe()\n",
        "excluded_history_len = excluded_rows['history'].astype(str).str.len().describe()\n",
        "\n",
        "print(f\"Included rows - History length: mean={included_history_len['mean']:.0f}, median={included_history_len['50%']:.0f}\")\n",
        "print(f\"Excluded rows - History length: mean={excluded_history_len['mean']:.0f}, median={excluded_history_len['50%']:.0f}\")\n",
        "\n",
        "# Analisi dei cluster\n",
        "if 'cluster' in df_full.columns:\n",
        "    print(f\"\\n🎯 CLUSTER ANALYSIS:\")\n",
        "    cluster_analysis = df_full.groupby('cluster').agg({\n",
        "        'row_category': lambda x: (x == 'valid_predictions').sum(),\n",
        "        'card_id': 'count'\n",
        "    })\n",
        "    cluster_analysis.columns = ['Included', 'Total']\n",
        "    cluster_analysis['Inclusion_Rate'] = (cluster_analysis['Included'] / cluster_analysis['Total'] * 100).round(1)\n",
        "    \n",
        "    print(\"Inclusion rate by cluster:\")\n",
        "    for cluster, row in cluster_analysis.iterrows():\n",
        "        print(f\"  Cluster {cluster}: {row['Included']:>5d}/{row['Total']:>5d} ({row['Inclusion_Rate']:>5.1f}%)\")\n",
        "\n",
        "# Analisi dei POI correnti\n",
        "print(f\"\\n🏛️ CURRENT POI ANALYSIS:\")\n",
        "current_poi_analysis = df_full.groupby('current_poi').agg({\n",
        "    'row_category': lambda x: (x == 'valid_predictions').sum(),\n",
        "    'card_id': 'count'\n",
        "})\n",
        "current_poi_analysis.columns = ['Included', 'Total'] \n",
        "current_poi_analysis['Inclusion_Rate'] = (current_poi_analysis['Included'] / current_poi_analysis['Total'] * 100).round(1)\n",
        "current_poi_analysis = current_poi_analysis.sort_values('Inclusion_Rate')\n",
        "\n",
        "print(\"POIs with LOWEST inclusion rates (hardest to predict from):\")\n",
        "for poi, row in current_poi_analysis.head(10).iterrows():\n",
        "    if row['Total'] >= 50:  # Solo POI con almeno 50 occorrenze\n",
        "        print(f\"  {str(poi):<20}: {row['Included']:>4d}/{row['Total']:>4d} ({row['Inclusion_Rate']:>5.1f}%)\")\n",
        "\n",
        "print(f\"\\nPOIs with HIGHEST inclusion rates (easiest to predict from):\")\n",
        "for poi, row in current_poi_analysis.tail(10).iterrows():\n",
        "    if row['Total'] >= 50:  # Solo POI con almeno 50 occorrenze\n",
        "        print(f\"  {str(poi):<20}: {row['Included']:>4d}/{row['Total']:>4d} ({row['Inclusion_Rate']:>5.1f}%)\")\n",
        "\n",
        "# Statistiche temporali se presenti\n",
        "if 'processing_time' in df_full.columns:\n",
        "    print(f\"\\n⏱️ PROCESSING TIME PATTERNS:\")\n",
        "    included_time = included_rows['processing_time'].describe()\n",
        "    excluded_time = excluded_rows['processing_time'].describe()\n",
        "    \n",
        "    print(f\"Included rows - Avg time: {included_time['mean']:.2f}s (median: {included_time['50%']:.2f}s)\")\n",
        "    print(f\"Excluded rows - Avg time: {excluded_time['mean']:.2f}s (median: {excluded_time['50%']:.2f}s)\")\n",
        "    print(f\"Difference: Excluded rows are {excluded_time['mean']/included_time['mean']:.2f}x faster to process\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "x5eu0dgalmn",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fix the formatting error and continue analysis\n",
        "print(f\"📈 PATTERN ANALYSIS: INCLUDED vs EXCLUDED\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Analisi delle history length\n",
        "print(\"🔍 HISTORY LENGTH ANALYSIS:\")\n",
        "included_history_len = included_rows['history'].astype(str).str.len().describe()\n",
        "excluded_history_len = excluded_rows['history'].astype(str).str.len().describe()\n",
        "\n",
        "print(f\"Included rows - History length: mean={included_history_len['mean']:.0f}, median={included_history_len['50%']:.0f}\")\n",
        "print(f\"Excluded rows - History length: mean={excluded_history_len['mean']:.0f}, median={excluded_history_len['50%']:.0f}\")\n",
        "\n",
        "# Analisi dei cluster\n",
        "if 'cluster' in df_full.columns:\n",
        "    print(f\"\\n🎯 CLUSTER ANALYSIS:\")\n",
        "    cluster_analysis = df_full.groupby('cluster').agg({\n",
        "        'row_category': lambda x: (x == 'valid_predictions').sum(),\n",
        "        'card_id': 'count'\n",
        "    })\n",
        "    cluster_analysis.columns = ['Included', 'Total']\n",
        "    cluster_analysis['Inclusion_Rate'] = (cluster_analysis['Included'] / cluster_analysis['Total'] * 100).round(1)\n",
        "    \n",
        "    print(\"Inclusion rate by cluster:\")\n",
        "    for cluster, row in cluster_analysis.iterrows():\n",
        "        print(f\"  Cluster {cluster}: {int(row['Included']):>5d}/{int(row['Total']):>5d} ({row['Inclusion_Rate']:>5.1f}%)\")\n",
        "\n",
        "# Riassumiamo la situazione\n",
        "print(f\"\\n📋 SUMMARY: WHAT THE NOTEBOOK CURRENTLY DOES\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "print(\"✅ INCLUDES (76.0% - 49,727 rows):\")\n",
        "print(\"  • Predictions with actual POI recommendations\")\n",
        "print(\"  • Both single POIs and multiple POI lists\") \n",
        "print(\"  • Successfully parsed prediction formats\")\n",
        "print(\"  • Examples: ['Ponte Pietra'], [['Arena', 'Duomo'], ['Casa Giulietta']]\")\n",
        "\n",
        "print(f\"\\n❌ EXCLUDES (24.0% - 15,662 rows):\")\n",
        "print(\"  • Empty prediction lists: [] (99.9% of excluded)\")\n",
        "print(\"  • NaN prediction values (0.1% of excluded)\")\n",
        "print(\"  • These represent legitimate 'no recommendation' cases\")\n",
        "print(\"  • Main reasons: All POIs already visited, no suitable options\")\n",
        "\n",
        "print(f\"\\n🎯 KEY INSIGHTS:\")\n",
        "print(\"  • Excluded rows have LONGER histories (58 vs 52 chars average)\")\n",
        "print(\"  • This suggests model struggles with users who visited more POIs\")\n",
        "print(\"  • Casa Giulietta has highest exclusion rate (39.1%)\")\n",
        "print(\"  • Arena has lowest exclusion rate (12.6%)\")\n",
        "print(\"  • Processing is faster for excluded rows (model gives up quicker)\")\n",
        "\n",
        "print(f\"\\n💡 RECOMMENDATION:\")\n",
        "print(\"  Don't filter out the '[]' predictions - they're valuable data!\")\n",
        "print(\"  They tell you when/why the model can't make recommendations\")\n",
        "print(\"  Include them in analysis as a separate 'no-recommendation' class\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv (3.11.6)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
