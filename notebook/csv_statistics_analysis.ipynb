{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSV Statistics Analysis\n",
    "\n",
    "This notebook analyzes prediction statistics from a CSV file containing POI (Point of Interest) predictions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import ast\n",
    "from collections import defaultdict, Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the folder path containing CSV files\n",
    "folder_path = \"/leonardo_work/IscrC_LLM-Mob/LLM-Mob-As-Mobility-Interpreter/results/results_mixtral_8x7b_with_geom/\"  # Modify this path to point to your CSV files folder\n",
    "\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# Find all CSV files matching the pattern dati_YEAR_*.csv\n",
    "csv_pattern = os.path.join(folder_path, \"dati_*.csv\")\n",
    "csv_files = glob.glob(csv_pattern)\n",
    "csv_files.sort()  # Sort files for consistent processing order\n",
    "\n",
    "print(f\"Found {len(csv_files)} CSV files:\")\n",
    "for file in csv_files:\n",
    "    print(f\"  - {os.path.basename(file)}\")\n",
    "\n",
    "# Load and combine all CSV files\n",
    "df_list = []\n",
    "file_info = []\n",
    "\n",
    "for file_path in csv_files:\n",
    "    try:\n",
    "        # Extract year from filename (after first underscore)\n",
    "        filename = os.path.basename(file_path)\n",
    "        year = filename.split('_')[1]\n",
    "        \n",
    "        df_temp = pd.read_csv(file_path)\n",
    "        df_temp['year'] = year  # Add year column\n",
    "        df_temp['source_file'] = filename  # Add source file info\n",
    "        \n",
    "        df_list.append(df_temp)\n",
    "        file_info.append({'file': filename, 'year': year, 'records': len(df_temp)})\n",
    "        \n",
    "        print(f\"Loaded {filename}: {len(df_temp)} records (Year: {year})\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file_path}: {e}\")\n",
    "\n",
    "# Combine all dataframes\n",
    "if df_list:\n",
    "    df = pd.concat(df_list, ignore_index=True)\n",
    "    print(f\"\\nCombined dataset shape: {df.shape}\")\n",
    "    \n",
    "    # Show file distribution\n",
    "    print(f\"\\nRecords per file:\")\n",
    "    for info in file_info:\n",
    "        print(f\"  {info['file']}: {info['records']:,} records\")\n",
    "    \n",
    "    print(f\"\\nRecords per year:\")\n",
    "    year_counts = df['year'].value_counts().sort_index()\n",
    "    for year, count in year_counts.items():\n",
    "        print(f\"  {year}: {count:,} records\")\n",
    "        \n",
    "else:\n",
    "    print(\"No CSV files found or loaded successfully!\")\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "print(\"\\nColumn names:\")\n",
    "print(df.columns.tolist())\n",
    "print(\"\\nFirst few rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprocessing\n",
    "def safe_eval(x):\n",
    "    \"\"\"Safely evaluate string representations of lists\"\"\"\n",
    "    try:\n",
    "        return ast.literal_eval(x) if isinstance(x, str) else x\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "# Convert string representations of lists to actual lists\n",
    "df['history'] = df['history'].apply(safe_eval)\n",
    "df['prediction'] = df['prediction'].apply(safe_eval)\n",
    "\n",
    "# Filter successful predictions only\n",
    "df_success = df[df['status'] == 'success'].copy()\n",
    "print(f\"Successful predictions: {len(df_success)} out of {len(df)} total ({len(df_success)/len(df)*100:.1f}%)\")\n",
    "\n",
    "# Basic statistics\n",
    "print(f\"\\nBasic Statistics:\")\n",
    "print(f\"- Unique cards: {df_success['card_id'].nunique()}\")\n",
    "print(f\"- Unique current POIs: {df_success['current_poi'].nunique()}\")\n",
    "print(f\"- Unique ground truth POIs: {df_success['ground_truth'].nunique()}\")\n",
    "print(f\"- Average processing time: {df_success['processing_time'].mean():.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Top 1 Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_top1_accuracy(df):\n",
    "    \"\"\"Calculate Top-1 accuracy\"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        predictions = row['prediction']\n",
    "        ground_truth = row['ground_truth']\n",
    "        \n",
    "        if predictions and len(predictions) > 0:\n",
    "            total += 1\n",
    "            if predictions[0] == ground_truth:\n",
    "                correct += 1\n",
    "    \n",
    "    return correct / total if total > 0 else 0\n",
    "\n",
    "top1_accuracy = calculate_top1_accuracy(df_success)\n",
    "print(f\"Top-1 Accuracy: {top1_accuracy:.4f} ({top1_accuracy*100:.2f}%)\")\n",
    "\n",
    "# Alternative calculation using the 'hit' column if it represents top-1\n",
    "hit_based_accuracy = df_success['hit'].mean()\n",
    "print(f\"Hit-based Accuracy: {hit_based_accuracy:.4f} ({hit_based_accuracy*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Top 5 Hit Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_top5_hit_rate(df):\n",
    "    \"\"\"Calculate Top-5 hit rate\"\"\"\n",
    "    hits = 0\n",
    "    total = 0\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        predictions = row['prediction']\n",
    "        ground_truth = row['ground_truth']\n",
    "        \n",
    "        if predictions:\n",
    "            total += 1\n",
    "            # Take top 5 predictions (or all if less than 5)\n",
    "            top5_preds = predictions[:5]\n",
    "            if ground_truth in top5_preds:\n",
    "                hits += 1\n",
    "    \n",
    "    return hits / total if total > 0 else 0\n",
    "\n",
    "top5_hit_rate = calculate_top5_hit_rate(df_success)\n",
    "print(f\"Top-5 Hit Rate: {top5_hit_rate:.4f} ({top5_hit_rate*100:.2f}%)\")\n",
    "\n",
    "# Calculate hit rates for different top-k values\n",
    "def calculate_topk_hit_rates(df, max_k=5):\n",
    "    hit_rates = {}\n",
    "    \n",
    "    for k in range(1, max_k + 1):\n",
    "        hits = 0\n",
    "        total = 0\n",
    "        \n",
    "        for _, row in df.iterrows():\n",
    "            predictions = row['prediction']\n",
    "            ground_truth = row['ground_truth']\n",
    "            \n",
    "            if predictions:\n",
    "                total += 1\n",
    "                topk_preds = predictions[:k]\n",
    "                if ground_truth in topk_preds:\n",
    "                    hits += 1\n",
    "        \n",
    "        hit_rates[f'Top-{k}'] = hits / total if total > 0 else 0\n",
    "    \n",
    "    return hit_rates\n",
    "\n",
    "hit_rates = calculate_topk_hit_rates(df_success)\n",
    "print(\"\\nHit Rates by Top-K:\")\n",
    "for k, rate in hit_rates.items():\n",
    "    print(f\"{k}: {rate:.4f} ({rate*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Top-K hit rates\n",
    "plt.figure(figsize=(10, 6))\n",
    "k_values = list(range(1, 6))\n",
    "rates = [hit_rates[f'Top-{k}'] for k in k_values]\n",
    "\n",
    "plt.plot(k_values, rates, marker='o', linewidth=2, markersize=8)\n",
    "plt.xlabel('K (Top-K)')\n",
    "plt.ylabel('Hit Rate')\n",
    "plt.title('Top-K Hit Rates')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xticks(k_values)\n",
    "\n",
    "# Add value labels on points\n",
    "for k, rate in zip(k_values, rates):\n",
    "    plt.annotate(f'{rate:.3f}', (k, rate), textcoords=\"offset points\", xytext=(0,10), ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. MRR (Mean Reciprocal Rank) per Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mrr(df):\n",
    "    \"\"\"Calculate Mean Reciprocal Rank\"\"\"\n",
    "    reciprocal_ranks = []\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        predictions = row['prediction']\n",
    "        ground_truth = row['ground_truth']\n",
    "        \n",
    "        if predictions:\n",
    "            try:\n",
    "                rank = predictions.index(ground_truth) + 1  # 1-indexed rank\n",
    "                reciprocal_ranks.append(1.0 / rank)\n",
    "            except ValueError:\n",
    "                reciprocal_ranks.append(0.0)  # Ground truth not in predictions\n",
    "    \n",
    "    return np.mean(reciprocal_ranks) if reciprocal_ranks else 0\n",
    "\n",
    "# Calculate overall MRR\n",
    "overall_mrr = calculate_mrr(df_success)\n",
    "print(f\"Overall MRR: {overall_mrr:.4f}\")\n",
    "\n",
    "# Try to extract year information if available in the data\n",
    "# This assumes there might be year information in card_id or we need to add a year column\n",
    "print(\"\\nNote: To calculate MRR per year, we need year information in the dataset.\")\n",
    "print(\"If you have year data, please add a 'year' column to the CSV file.\")\n",
    "\n",
    "# Check if there's any pattern in card_id that might indicate year\n",
    "sample_card_ids = df_success['card_id'].head(10).tolist()\n",
    "print(f\"\\nSample card IDs (to check for year patterns): {sample_card_ids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If year column exists or can be extracted, calculate MRR per year\n",
    "# Example implementation assuming year column exists:\n",
    "\n",
    "if 'year' in df_success.columns:\n",
    "    mrr_by_year = {}\n",
    "    for year in df_success['year'].unique():\n",
    "        year_data = df_success[df_success['year'] == year]\n",
    "        mrr_by_year[year] = calculate_mrr(year_data)\n",
    "    \n",
    "    print(\"MRR per Year:\")\n",
    "    for year, mrr in sorted(mrr_by_year.items()):\n",
    "        print(f\"{year}: {mrr:.4f}\")\n",
    "    \n",
    "    # Visualize MRR by year\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    years = sorted(mrr_by_year.keys())\n",
    "    mrr_values = [mrr_by_year[year] for year in years]\n",
    "    \n",
    "    plt.plot(years, mrr_values, marker='o', linewidth=2, markersize=8)\n",
    "    plt.xlabel('Year')\n",
    "    plt.ylabel('Mean Reciprocal Rank (MRR)')\n",
    "    plt.title('MRR Trend Over Years')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    for year, mrr in zip(years, mrr_values):\n",
    "        plt.annotate(f'{mrr:.3f}', (year, mrr), textcoords=\"offset points\", xytext=(0,10), ha='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Year column not found. Add year information to calculate MRR per year.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Worst Performing Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_worst_performing_pairs(df, min_occurrences=5):\n",
    "    \"\"\"Analyze worst performing current_poi -> ground_truth pairs\"\"\"\n",
    "    pair_stats = defaultdict(lambda: {'total': 0, 'correct': 0, 'top5_hits': 0})\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        current_poi = row['current_poi']\n",
    "        ground_truth = row['ground_truth']\n",
    "        predictions = row['prediction']\n",
    "        \n",
    "        pair = (current_poi, ground_truth)\n",
    "        pair_stats[pair]['total'] += 1\n",
    "        \n",
    "        if predictions:\n",
    "            # Check top-1 accuracy\n",
    "            if len(predictions) > 0 and predictions[0] == ground_truth:\n",
    "                pair_stats[pair]['correct'] += 1\n",
    "            \n",
    "            # Check top-5 hit\n",
    "            if ground_truth in predictions[:5]:\n",
    "                pair_stats[pair]['top5_hits'] += 1\n",
    "    \n",
    "    # Calculate accuracy for each pair\n",
    "    pair_performance = []\n",
    "    for pair, stats in pair_stats.items():\n",
    "        if stats['total'] >= min_occurrences:\n",
    "            accuracy = stats['correct'] / stats['total']\n",
    "            top5_rate = stats['top5_hits'] / stats['total']\n",
    "            pair_performance.append({\n",
    "                'current_poi': pair[0],\n",
    "                'ground_truth': pair[1],\n",
    "                'total_occurrences': stats['total'],\n",
    "                'top1_accuracy': accuracy,\n",
    "                'top5_hit_rate': top5_rate\n",
    "            })\n",
    "    \n",
    "    return sorted(pair_performance, key=lambda x: x['top1_accuracy'])\n",
    "\n",
    "worst_pairs = analyze_worst_performing_pairs(df_success)\n",
    "\n",
    "print(f\"Worst Performing Pairs (minimum {5} occurrences):\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, pair in enumerate(worst_pairs[:15]):\n",
    "    print(f\"{i+1:2d}. {pair['current_poi']} â†’ {pair['ground_truth']}\")\n",
    "    print(f\"    Occurrences: {pair['total_occurrences']}\")\n",
    "    print(f\"    Top-1 Accuracy: {pair['top1_accuracy']:.3f} ({pair['top1_accuracy']*100:.1f}%)\")\n",
    "    print(f\"    Top-5 Hit Rate: {pair['top5_hit_rate']:.3f} ({pair['top5_hit_rate']*100:.1f}%)\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize worst performing pairs\n",
    "if len(worst_pairs) >= 10:\n",
    "    worst_10 = worst_pairs[:10]\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 12))\n",
    "    \n",
    "    # Top-1 Accuracy\n",
    "    pair_labels = [f\"{p['current_poi']} â†’ {p['ground_truth']}\" for p in worst_10]\n",
    "    accuracies = [p['top1_accuracy'] for p in worst_10]\n",
    "    \n",
    "    bars1 = ax1.barh(range(len(pair_labels)), accuracies)\n",
    "    ax1.set_yticks(range(len(pair_labels)))\n",
    "    ax1.set_yticklabels(pair_labels)\n",
    "    ax1.set_xlabel('Top-1 Accuracy')\n",
    "    ax1.set_title('Worst Performing Pairs - Top-1 Accuracy')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, (bar, acc) in enumerate(zip(bars1, accuracies)):\n",
    "        ax1.text(acc + 0.01, i, f'{acc:.3f}', va='center')\n",
    "    \n",
    "    # Top-5 Hit Rate\n",
    "    hit_rates = [p['top5_hit_rate'] for p in worst_10]\n",
    "    \n",
    "    bars2 = ax2.barh(range(len(pair_labels)), hit_rates)\n",
    "    ax2.set_yticks(range(len(pair_labels)))\n",
    "    ax2.set_yticklabels(pair_labels)\n",
    "    ax2.set_xlabel('Top-5 Hit Rate')\n",
    "    ax2.set_title('Worst Performing Pairs - Top-5 Hit Rate')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, (bar, rate) in enumerate(zip(bars2, hit_rates)):\n",
    "        ax2.text(rate + 0.01, i, f'{rate:.3f}', va='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create confusion matrix for most common POIs\n",
    "def create_confusion_matrix(df, top_n_pois=15):\n",
    "    \"\"\"Create confusion matrix for top N most common POIs\"\"\"\n",
    "    \n",
    "    # Get most common POIs from ground truth\n",
    "    poi_counts = df['ground_truth'].value_counts()\n",
    "    top_pois = poi_counts.head(top_n_pois).index.tolist()\n",
    "    \n",
    "    # Filter data to include only top POIs\n",
    "    df_filtered = df[df['ground_truth'].isin(top_pois)].copy()\n",
    "    \n",
    "    # Get top-1 predictions\n",
    "    df_filtered['top1_prediction'] = df_filtered['prediction'].apply(\n",
    "        lambda x: x[0] if x and len(x) > 0 else 'NO_PREDICTION'\n",
    "    )\n",
    "    \n",
    "    # Create confusion matrix\n",
    "    y_true = df_filtered['ground_truth']\n",
    "    y_pred = df_filtered['top1_prediction']\n",
    "    \n",
    "    # Get unique labels (combination of true and predicted)\n",
    "    all_labels = sorted(list(set(y_true.unique()) | set(y_pred.unique())))\n",
    "    \n",
    "    cm = confusion_matrix(y_true, y_pred, labels=all_labels)\n",
    "    \n",
    "    return cm, all_labels, df_filtered\n",
    "\n",
    "cm, labels, df_filtered = create_confusion_matrix(df_success)\n",
    "\n",
    "print(f\"Confusion Matrix created for {len(labels)} POIs\")\n",
    "print(f\"Matrix shape: {cm.shape}\")\n",
    "print(f\"Filtered dataset size: {len(df_filtered)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize confusion matrix\n",
    "plt.figure(figsize=(16, 14))\n",
    "\n",
    "# Calculate percentages for better interpretation\n",
    "cm_percent = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100\n",
    "cm_percent = np.nan_to_num(cm_percent)  # Replace NaN with 0\n",
    "\n",
    "# Create heatmap\n",
    "mask = cm == 0  # Mask zero values for better visualization\n",
    "sns.heatmap(cm_percent, \n",
    "            xticklabels=labels, \n",
    "            yticklabels=labels,\n",
    "            annot=True, \n",
    "            fmt='.1f',\n",
    "            cmap='Blues',\n",
    "            mask=mask,\n",
    "            square=True,\n",
    "            cbar_kws={'label': 'Percentage (%)'})\n",
    "\n",
    "plt.title('Confusion Matrix - Top-1 Predictions (Percentage)', fontsize=16, pad=20)\n",
    "plt.xlabel('Predicted POI', fontsize=12)\n",
    "plt.ylabel('True POI', fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative confusion matrix with raw counts\n",
    "plt.figure(figsize=(16, 14))\n",
    "\n",
    "mask = cm == 0\n",
    "sns.heatmap(cm, \n",
    "            xticklabels=labels, \n",
    "            yticklabels=labels,\n",
    "            annot=True, \n",
    "            fmt='d',\n",
    "            cmap='Oranges',\n",
    "            mask=mask,\n",
    "            square=True,\n",
    "            cbar_kws={'label': 'Count'})\n",
    "\n",
    "plt.title('Confusion Matrix - Top-1 Predictions (Raw Counts)', fontsize=16, pad=20)\n",
    "plt.xlabel('Predicted POI', fontsize=12)\n",
    "plt.ylabel('True POI', fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"COMPREHENSIVE STATISTICS SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nðŸ“Š DATASET OVERVIEW:\")\n",
    "print(f\"   â€¢ Total records: {len(df):,}\")\n",
    "print(f\"   â€¢ Successful predictions: {len(df_success):,} ({len(df_success)/len(df)*100:.1f}%)\")\n",
    "print(f\"   â€¢ Unique cards: {df_success['card_id'].nunique():,}\")\n",
    "print(f\"   â€¢ Unique POIs: {df_success['ground_truth'].nunique()}\")\n",
    "print(f\"   â€¢ Average processing time: {df_success['processing_time'].mean():.2f}s\")\n",
    "\n",
    "print(f\"\\nðŸŽ¯ ACCURACY METRICS:\")\n",
    "print(f\"   â€¢ Top-1 Accuracy: {top1_accuracy:.4f} ({top1_accuracy*100:.2f}%)\")\n",
    "print(f\"   â€¢ Top-5 Hit Rate: {top5_hit_rate:.4f} ({top5_hit_rate*100:.2f}%)\")\n",
    "print(f\"   â€¢ Mean Reciprocal Rank (MRR): {overall_mrr:.4f}\")\n",
    "\n",
    "print(f\"\\nðŸ“ˆ TOP-K PERFORMANCE:\")\n",
    "for k, rate in hit_rates.items():\n",
    "    print(f\"   â€¢ {k} Hit Rate: {rate:.4f} ({rate*100:.2f}%)\")\n",
    "\n",
    "print(f\"\\nâš ï¸  WORST PERFORMING PAIRS:\")\n",
    "for i, pair in enumerate(worst_pairs[:5]):\n",
    "    print(f\"   {i+1}. {pair['current_poi']} â†’ {pair['ground_truth']}\")\n",
    "    print(f\"      Accuracy: {pair['top1_accuracy']:.3f}, Occurrences: {pair['total_occurrences']}\")\n",
    "\n",
    "print(f\"\\nðŸ” CONFUSION MATRIX INFO:\")\n",
    "print(f\"   â€¢ Matrix size: {cm.shape[0]}Ã—{cm.shape[1]}\")\n",
    "print(f\"   â€¢ POIs analyzed: {len(labels)}\")\n",
    "print(f\"   â€¢ Samples in matrix: {len(df_filtered):,}\")\n",
    "\n",
    "diagonal_sum = np.diag(cm).sum()\n",
    "total_predictions = cm.sum()\n",
    "matrix_accuracy = diagonal_sum / total_predictions if total_predictions > 0 else 0\n",
    "print(f\"   â€¢ Matrix accuracy: {matrix_accuracy:.4f} ({matrix_accuracy*100:.2f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
