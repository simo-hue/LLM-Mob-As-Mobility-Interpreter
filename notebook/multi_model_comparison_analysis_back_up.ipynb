{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üî¨ Multi-Model Comparison Analysis for Tourism Mobility Prediction\n",
    "\n",
    "*Automated analysis and visualization for comparing multiple LLM models on VeronaCard dataset*\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Obiettivi del Notebook\n",
    "\n",
    "Questo notebook fornisce un **framework completo** per confrontare le performance di diversi modelli LLM sulla predizione next-POI turistici, generando automaticamente:\n",
    "\n",
    "- **üìä Grafici comparativi** pronti per inclusione nella tesi\n",
    "- **üìà Analisi statistiche** dettagliate per ogni modello\n",
    "- **üé® Visualizzazioni professionali** con stili consistenti\n",
    "- **üìã Report automatici** in formato LaTeX-ready\n",
    "\n",
    "## üìÅ Struttura Repository Richiesta\n",
    "\n",
    "**‚ö†Ô∏è IMPORTANTE**: Prima di eseguire questo notebook, organizza la repository come segue:\n",
    "\n",
    "```\n",
    "LLM-Mob-As-Mobility-Interpreter/\n",
    "‚îú‚îÄ‚îÄ notebook/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ veronacard_analysis.ipynb              # tuo notebook esistente\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ multi_model_comparison_analysis.ipynb  # questo notebook\n",
    "‚îú‚îÄ‚îÄ results/                                   # NUOVA CARTELLA DA CREARE\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ llama3.1_8b/\n",
    "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ dati_2014_pred_*.csv\n",
    "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ dati_2015_pred_*.csv\n",
    "‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ... (tutti i file CSV per LLaMA)\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ mixtral_8x7b/\n",
    "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ dati_2014_pred_*.csv\n",
    "‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ... (tutti i file CSV per Mixtral)\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ qwen2.5_7b/\n",
    "‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ... (file CSV per Qwen)\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ deepseek_v3_8b/\n",
    "‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ... (file CSV per DeepSeek)\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ gemma3_8b/\n",
    "‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ... (file CSV per Gemma 8B)\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ gemma3_27b/\n",
    "‚îÇ       ‚îî‚îÄ‚îÄ ... (file CSV per Gemma 27B)\n",
    "‚îî‚îÄ‚îÄ img/                                       # cartelle gi√† create\n",
    "    ‚îú‚îÄ‚îÄ multi_model_comparison/\n",
    "    ‚îú‚îÄ‚îÄ llama3.1_8b_extended/\n",
    "    ‚îî‚îÄ‚îÄ ... (altre cartelle modelli)\n",
    "```\n",
    "\n",
    "## üöÄ Workflow di Utilizzo\n",
    "\n",
    "1. **Prepara i dati**: Sposta tutti i file `*_pred_*.csv` nelle rispettive cartelle sotto `results/`\n",
    "2. **Configura il notebook**: Modifica la lista `MODELS` se necessario\n",
    "3. **Esegui tutte le celle**: Il notebook generer√† automaticamente tutti i grafici\n",
    "4. **Verifica output**: Controlla che i file vengano salvati in `img/`\n",
    "5. **Integra in LaTeX**: I grafici sono pronti per l'inclusione nella tesi\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ Setup e Configurazione\n",
    "\n",
    "### Configurazione dei modelli da analizzare\n",
    "Modifica questa configurazione in base ai modelli che hai testato:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import ast\n",
    "import json\n",
    "import warnings\n",
    "from collections import Counter, defaultdict\n",
    "import os\n",
    "\n",
    "# Configurazione stile grafici\n",
    "plt.rcParams['figure.dpi'] = 300  # Alta qualit√† per la tesi\n",
    "plt.rcParams['axes.grid'] = True\n",
    "plt.rcParams['grid.alpha'] = 0.3\n",
    "plt.rcParams['font.size'] = 12\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['axes.titlesize'] = 16\n",
    "plt.rcParams['legend.fontsize'] = 11\n",
    "\n",
    "# Sopprime warning non critici\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "\n",
    "print(\"üì¶ Setup completato!\")\n",
    "print(f\"üìä Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configurazione modelli e colori"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CONFIGURAZIONE MODELLI - Modifica questa sezione se necessario\n",
    "# ============================================================================\n",
    "\n",
    "MODELS = {\n",
    "    'llama3.1_8b': {\n",
    "        'name': 'LLaMA 3.1 8B',\n",
    "        'dir': '../results/llama3.1_8b/',\n",
    "        'color': '#1f77b4',  # Blu\n",
    "        'params': '8B',\n",
    "        'type': 'Baseline'\n",
    "    },\n",
    "    'mixtral_8x7b': {\n",
    "        'name': 'Mixtral 8x7B',\n",
    "        'dir': '../results/mixtral_8x7b/',\n",
    "        'color': '#ff7f0e',  # Arancione\n",
    "        'params': '8x7B (47B active)',\n",
    "        'type': 'MoE'\n",
    "    },\n",
    "    'qwen2.5_7b': {\n",
    "        'name': 'Qwen 2.5 7B',\n",
    "        'dir': '../results/qwen2.5_7b/',\n",
    "        'color': '#2ca02c',  # Verde\n",
    "        'params': '7B',\n",
    "        'type': 'Multilingual'\n",
    "    },\n",
    "    'deepseek_v3_8b': {\n",
    "        'name': 'DeepSeek-V3 8B',\n",
    "        'dir': '../results/deepseek_v3_8b/',\n",
    "        'color': '#d62728',  # Rosso\n",
    "        'params': '8B',\n",
    "        'type': 'Reasoning'\n",
    "    },\n",
    "    'gemma3_8b': {\n",
    "        'name': 'Gemma 3 8B',\n",
    "        'dir': '../results/gemma3_8b/',\n",
    "        'color': '#9467bd',  # Viola\n",
    "        'params': '8B',\n",
    "        'type': 'Google'\n",
    "    },\n",
    "    'gemma3_27b': {\n",
    "        'name': 'Gemma 3 27B',\n",
    "        'dir': '../results/gemma3_27b/',\n",
    "        'color': '#8c564b',  # Marrone\n",
    "        'params': '27B',\n",
    "        'type': 'Large-scale'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Directory per salvare i grafici\n",
    "OUTPUT_DIRS = {\n",
    "    'comparison': '../img/multi_model_comparison/',\n",
    "    'individual': '../img/'  # + model_key per grafici specifici\n",
    "}\n",
    "\n",
    "# Crea directory se non esistono\n",
    "for dir_path in OUTPUT_DIRS.values():\n",
    "    Path(dir_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"üé® Configurazione modelli completata:\")\n",
    "for key, model in MODELS.items():\n",
    "    print(f\"  {model['name']:>15s} ‚Üí {model['dir']} [{model['color']}]\")\n",
    "    \n",
    "print(f\"\\nüìÅ Output directory: {OUTPUT_DIRS['comparison']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìà 1. Caricamento e Preprocessing Dati\n",
    "\n",
    "### Funzioni di utilit√† per il caricamento dati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def poi_id(x):\n",
    "    \"\"\"\n",
    "    Normalizza identificatori POI per confronti consistenti.\n",
    "    Gestisce dict, liste, e stringhe/numeri.\n",
    "    \"\"\"\n",
    "    if isinstance(x, dict):\n",
    "        for key in ('poi', 'poi_id', 'name', 'id'):\n",
    "            if key in x:\n",
    "                return str(x[key])\n",
    "        return json.dumps(x, sort_keys=True)\n",
    "    elif isinstance(x, (list, tuple)):\n",
    "        return tuple(map(poi_id, x))\n",
    "    else:\n",
    "        return str(x)\n",
    "\n",
    "def load_model_data(model_dir, model_name):\n",
    "    \"\"\"\n",
    "    Carica tutti i CSV di un modello e calcola le metriche.\n",
    "    \n",
    "    Args:\n",
    "        model_dir (str): Directory contenente i file CSV del modello\n",
    "        model_name (str): Nome del modello per logging\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame con tutte le predizioni e metriche calcolate\n",
    "    \"\"\"\n",
    "    print(f\"üìÇ Caricamento {model_name}...\")\n",
    "    \n",
    "    # Trova tutti i file CSV\n",
    "    csv_pattern = str(Path(model_dir) / '*_pred_*.csv')\n",
    "    csv_files = glob.glob(csv_pattern)\n",
    "    \n",
    "    if not csv_files:\n",
    "        print(f\"   ‚ö†Ô∏è  Nessun file CSV trovato in {model_dir}\")\n",
    "        print(f\"       Pattern cercato: {csv_pattern}\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    print(f\"   üìÑ Trovati {len(csv_files)} file CSV\")\n",
    "    \n",
    "    dfs = []\n",
    "    total_rows = 0\n",
    "    \n",
    "    for csv_file in sorted(csv_files):\n",
    "        try:\n",
    "            # Caricamento con gestione errori\n",
    "            try:\n",
    "                df = pd.read_csv(csv_file)\n",
    "            except pd.errors.ParserError:\n",
    "                print(f\"   ‚ö†Ô∏è  Parser error in {Path(csv_file).name}, utilizzando error handling...\")\n",
    "                df = pd.read_csv(csv_file, on_bad_lines='skip', engine='python')\n",
    "            \n",
    "            # Estrai anno dal nome file\n",
    "            filename = Path(csv_file).stem\n",
    "            year_token = next((part for part in filename.split('_')\n",
    "                             if part.isdigit() and len(part) == 4), None)\n",
    "            df['year'] = int(year_token) if year_token else np.nan\n",
    "            df['model'] = model_name\n",
    "            \n",
    "            # Parse prediction list\n",
    "            df['prediction_list'] = df['prediction'].apply(\n",
    "                lambda x: ast.literal_eval(x) if isinstance(x, str) else []\n",
    "            )\n",
    "            \n",
    "            # Filtra solo predizioni con esattamente 5 elementi\n",
    "            df = df[df['prediction_list'].apply(len) == 5]\n",
    "            \n",
    "            dfs.append(df)\n",
    "            total_rows += len(df)\n",
    "            print(f\"       {Path(csv_file).name}: {len(df):,} righe\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Errore caricando {Path(csv_file).name}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    if not dfs:\n",
    "        print(f\"   ‚ùå Nessun file caricato con successo per {model_name}\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Concatena tutti i dataframes\n",
    "    df_combined = pd.concat(dfs, ignore_index=True)\n",
    "    \n",
    "    # Normalizza POI per confronti consistenti\n",
    "    df_combined['prediction_norm'] = df_combined['prediction_list'].apply(\n",
    "        lambda lst: [poi_id(e) for e in lst]\n",
    "    )\n",
    "    df_combined['ground_truth_norm'] = df_combined['ground_truth'].apply(poi_id)\n",
    "    \n",
    "    print(f\"   ‚úÖ {model_name}: {total_rows:,} righe totali\")\n",
    "    return df_combined\n",
    "\n",
    "def calculate_metrics(df):\n",
    "    \"\"\"\n",
    "    Calcola le metriche di valutazione standard per un DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame con colonne prediction_norm e ground_truth_norm\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dizionario con le metriche calcolate\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        return {\n",
    "            'top1_accuracy': 0.0,\n",
    "            'top5_hit_rate': 0.0,\n",
    "            'mrr': 0.0,\n",
    "            'catalogue_coverage': 0.0,\n",
    "            'total_predictions': 0,\n",
    "            'processing_time_mean': 0.0\n",
    "        }\n",
    "    \n",
    "    # Calcola hit@1\n",
    "    df['hit@1'] = df['prediction_norm'].str[0] == df['ground_truth_norm']\n",
    "    \n",
    "    # Calcola hit@5\n",
    "    df['hit@5'] = df.apply(\n",
    "        lambda row: row['ground_truth_norm'] in row['prediction_norm'][:5], axis=1\n",
    "    )\n",
    "    \n",
    "    # Calcola reciprocal rank\n",
    "    def reciprocal_rank(row, k=5):\n",
    "        try:\n",
    "            rank = row['prediction_norm'][:k].index(row['ground_truth_norm']) + 1\n",
    "            return 1.0 / rank\n",
    "        except ValueError:\n",
    "            return 0.0\n",
    "    \n",
    "    df['rr'] = df.apply(reciprocal_rank, axis=1)\n",
    "    \n",
    "    # Calcola catalogue coverage\n",
    "    coverage_set = {poi for preds in df['prediction_norm'] for poi in preds}\n",
    "    unique_ground_truth = df['ground_truth_norm'].nunique()\n",
    "    \n",
    "    # Processing time medio\n",
    "    proc_time_mean = df['processing_time'].mean() if 'processing_time' in df.columns else 0.0\n",
    "    \n",
    "    metrics = {\n",
    "        'top1_accuracy': df['hit@1'].mean(),\n",
    "        'top5_hit_rate': df['hit@5'].mean(),\n",
    "        'mrr': df['rr'].mean(),\n",
    "        'catalogue_coverage': len(coverage_set) / unique_ground_truth if unique_ground_truth > 0 else 0.0,\n",
    "        'total_predictions': len(df),\n",
    "        'processing_time_mean': proc_time_mean\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "print(\"üîß Funzioni di utilit√† caricate!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caricamento dati per tutti i modelli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CARICAMENTO DATI PER TUTTI I MODELLI\n",
    "# ============================================================================\n",
    "\n",
    "print(\"üöÄ Avvio caricamento dati multi-modello...\\n\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Dizionario per contenere tutti i dati\n",
    "model_data = {}\n",
    "model_metrics = {}\n",
    "successfully_loaded = []\n",
    "\n",
    "# Carica dati per ogni modello\n",
    "for model_key, model_config in MODELS.items():\n",
    "    print(f\"\\nüìä MODELLO: {model_config['name']}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Verifica esistenza directory\n",
    "    model_dir = Path(model_config['dir'])\n",
    "    if not model_dir.exists():\n",
    "        print(f\"   ‚ùå Directory non trovata: {model_dir}\")\n",
    "        print(f\"   üí° Crea la directory e inserisci i file CSV del modello\")\n",
    "        continue\n",
    "    \n",
    "    # Carica i dati\n",
    "    df = load_model_data(model_config['dir'], model_config['name'])\n",
    "    \n",
    "    if df.empty:\n",
    "        print(f\"   ‚è≠Ô∏è  Saltando {model_config['name']} (nessun dato)\")\n",
    "        continue\n",
    "    \n",
    "    # Calcola metriche\n",
    "    metrics = calculate_metrics(df)\n",
    "    \n",
    "    # Salva nei dizionari\n",
    "    model_data[model_key] = df\n",
    "    model_metrics[model_key] = metrics\n",
    "    successfully_loaded.append(model_key)\n",
    "    \n",
    "    # Report per questo modello\n",
    "    print(f\"   ‚úÖ Metriche {model_config['name']}:\")\n",
    "    print(f\"       Top-1 Accuracy: {metrics['top1_accuracy']:.3f} ({metrics['top1_accuracy']*100:.1f}%)\")\n",
    "    print(f\"       Top-5 Hit Rate: {metrics['top5_hit_rate']:.3f} ({metrics['top5_hit_rate']*100:.1f}%)\")\n",
    "    print(f\"       MRR:            {metrics['mrr']:.3f}\")\n",
    "    print(f\"       Coverage:       {metrics['catalogue_coverage']:.2f}\")\n",
    "    print(f\"       Predizioni:     {metrics['total_predictions']:,}\")\n",
    "    \n",
    "    if metrics['processing_time_mean'] > 0:\n",
    "        print(f\"       Tempo/Card:     {metrics['processing_time_mean']:.2f}s\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"üìã RIEPILOGO CARICAMENTO:\")\n",
    "print(f\"   Modelli configurati: {len(MODELS)}\")\n",
    "print(f\"   Modelli caricati:    {len(successfully_loaded)}\")\n",
    "print(f\"   Modelli disponibili: {', '.join([MODELS[k]['name'] for k in successfully_loaded])}\")\n",
    "\n",
    "if len(successfully_loaded) < 2:\n",
    "    print(\"\\n‚ö†Ô∏è  ATTENZIONE: Servono almeno 2 modelli per i confronti!\")\n",
    "    print(\"   Verifica la struttura delle directory in results/\")\n",
    "else:\n",
    "    print(f\"\\n‚úÖ Pronto per analisi multi-modello con {len(successfully_loaded)} modelli!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä 2. Creazione Tabella Comparativa Principale\n",
    "\n",
    "Genera la tabella di performance comparative che sar√† integrata nel LaTeX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# TABELLA COMPARATIVA PRINCIPALE\n",
    "# ============================================================================\n",
    "\n",
    "if len(successfully_loaded) == 0:\n",
    "    print(\"‚ùå Nessun modello caricato. Saltando creazione tabella.\")\n",
    "else:\n",
    "    print(\"üìã Creazione tabella comparativa...\")\n",
    "    \n",
    "    # Prepara dati per la tabella\n",
    "    comparison_data = []\n",
    "    \n",
    "    for model_key in successfully_loaded:\n",
    "        model_config = MODELS[model_key]\n",
    "        metrics = model_metrics[model_key]\n",
    "        \n",
    "        # Calcola cards per second se disponibile processing time\n",
    "        if metrics['processing_time_mean'] > 0:\n",
    "            cards_per_sec = 1.0 / metrics['processing_time_mean']\n",
    "            time_display = f\"{metrics['processing_time_mean']:.1f}s\"\n",
    "            efficiency = f\"{cards_per_sec:.1f}\"\n",
    "        else:\n",
    "            time_display = \"N/A\"\n",
    "            efficiency = \"N/A\"\n",
    "        \n",
    "        comparison_data.append({\n",
    "            'Modello': model_config['name'],\n",
    "            'Parametri': model_config['params'],\n",
    "            'Top-1 Acc.': f\"{metrics['top1_accuracy']*100:.1f}%\",\n",
    "            'Top-5 HR': f\"{metrics['top5_hit_rate']*100:.1f}%\",\n",
    "            'MRR': f\"{metrics['mrr']*100:.1f}%\",\n",
    "            'Coverage': f\"{metrics['catalogue_coverage']:.2f}\",\n",
    "            'Tempo/Card': time_display,\n",
    "            'Cards/Sec': efficiency,\n",
    "            'Predizioni': f\"{metrics['total_predictions']:,}\"\n",
    "        })\n",
    "    \n",
    "    # Crea DataFrame per display\n",
    "    comparison_df = pd.DataFrame(comparison_data)\n",
    "    \n",
    "    print(\"\\nüìä TABELLA COMPARATIVA PERFORMANCE:\")\n",
    "    print(\"=\" * 80)\n",
    "    print(comparison_df.to_string(index=False))\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Salva la tabella per uso in LaTeX\n",
    "    latex_table_path = Path(OUTPUT_DIRS['comparison']) / 'performance_table.csv'\n",
    "    comparison_df.to_csv(latex_table_path, index=False)\n",
    "    print(f\"\\nüíæ Tabella salvata in: {latex_table_path}\")\n",
    "    \n",
    "    # Genera anche codice LaTeX pronto all'uso\n",
    "    latex_code_path = Path(OUTPUT_DIRS['comparison']) / 'performance_table.tex'\n",
    "    \n",
    "    with open(latex_code_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(\"% Tabella generata automaticamente\\n\")\n",
    "        f.write(\"\\\\begin{table}[H]\\n\")\n",
    "        f.write(\"\\\\centering\\n\")\n",
    "        f.write(\"\\\\caption{Performance Comparative tra Modelli LLM (Strategia Geospaziale)}\\n\")\n",
    "        f.write(\"\\\\label{tab:models_comparison}\\n\")\n",
    "        f.write(\"\\\\begin{tabular}{lcccccc}\\n\")\n",
    "        f.write(\"\\\\toprule\\n\")\n",
    "        f.write(\"\\\\textbf{Modello} & \\\\textbf{Parametri} & \\\\textbf{Top-1 Acc.} & \\\\textbf{Top-5 HR} & \\\\textbf{MRR} & \\\\textbf{Coverage} & \\\\textbf{Tempo/Card} \\\\\\\\\\n\")\n",
    "        f.write(\"\\\\midrule\\n\")\n",
    "        \n",
    "        for _, row in comparison_df.iterrows():\n",
    "            f.write(f\"{row['Modello']} & {row['Parametri']} & {row['Top-1 Acc.']} & {row['Top-5 HR']} & {row['MRR']} & {row['Coverage']} & {row['Tempo/Card']} \\\\\\\\\\n\")\n",
    "        \n",
    "        f.write(\"\\\\bottomrule\\n\")\n",
    "        f.write(\"\\\\end{tabular}\\n\")\n",
    "        f.write(\"\\\\end{table}\\n\")\n",
    "    \n",
    "    print(f\"üìÑ Codice LaTeX salvato in: {latex_code_path}\")\n",
    "    \n",
    "    # Identifica il modello best-performing per ogni metrica\n",
    "    print(\"\\nüèÜ MODELLI BEST-PERFORMING PER METRICA:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Top-1 Accuracy\n",
    "    best_top1 = max(successfully_loaded, key=lambda k: model_metrics[k]['top1_accuracy'])\n",
    "    print(f\"Top-1 Accuracy: {MODELS[best_top1]['name']} ({model_metrics[best_top1]['top1_accuracy']*100:.1f}%)\")\n",
    "    \n",
    "    # Top-5 Hit Rate\n",
    "    best_top5 = max(successfully_loaded, key=lambda k: model_metrics[k]['top5_hit_rate'])\n",
    "    print(f\"Top-5 Hit Rate: {MODELS[best_top5]['name']} ({model_metrics[best_top5]['top5_hit_rate']*100:.1f}%)\")\n",
    "    \n",
    "    # MRR\n",
    "    best_mrr = max(successfully_loaded, key=lambda k: model_metrics[k]['mrr'])\n",
    "    print(f\"MRR:            {MODELS[best_mrr]['name']} ({model_metrics[best_mrr]['mrr']*100:.1f}%)\")\n",
    "    \n",
    "    # Efficienza (se disponibile)\n",
    "    models_with_time = [k for k in successfully_loaded if model_metrics[k]['processing_time_mean'] > 0]\n",
    "    if models_with_time:\n",
    "        best_efficiency = min(models_with_time, key=lambda k: model_metrics[k]['processing_time_mean'])\n",
    "        print(f\"Efficienza:     {MODELS[best_efficiency]['name']} ({model_metrics[best_efficiency]['processing_time_mean']:.1f}s/card)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üé® 3. Generazione Grafici Comparativi Principali\n",
    "\n",
    "### 3.1 Grafico a Barre Comparativo Principale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# GRAFICO COMPARATIVO PRINCIPALE - Barre Raggruppate\n",
    "# ============================================================================\n",
    "\n",
    "if len(successfully_loaded) < 2:\n",
    "    print(\"‚è≠Ô∏è  Saltando grafici: servono almeno 2 modelli\")\n",
    "else:\n",
    "    print(\"üé® Creazione grafico comparativo principale...\")\n",
    "    \n",
    "    # Prepara i dati per il grafico\n",
    "    models_list = [MODELS[k]['name'] for k in successfully_loaded]\n",
    "    colors_list = [MODELS[k]['color'] for k in successfully_loaded]\n",
    "    \n",
    "    top1_values = [model_metrics[k]['top1_accuracy'] * 100 for k in successfully_loaded]\n",
    "    top5_values = [model_metrics[k]['top5_hit_rate'] * 100 for k in successfully_loaded]\n",
    "    mrr_values = [model_metrics[k]['mrr'] * 100 for k in successfully_loaded]\n",
    "    \n",
    "    # Configura il grafico\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    fig.suptitle('Performance Comparison Across LLM Models', fontsize=20, fontweight='bold')\n",
    "    \n",
    "    # Top-1 Accuracy\n",
    "    bars1 = ax1.bar(models_list, top1_values, color=colors_list, alpha=0.8, edgecolor='black', linewidth=0.5)\n",
    "    ax1.set_title('Top-1 Accuracy (%)', fontsize=14, fontweight='bold')\n",
    "    ax1.set_ylabel('Accuracy (%)', fontsize=12)\n",
    "    ax1.tick_params(axis='x', rotation=45)\n",
    "    ax1.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Aggiungi valori sopra le barre\n",
    "    for bar, value in zip(bars1, top1_values):\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5, \n",
    "                f'{value:.1f}%', ha='center', va='bottom', fontweight='bold', fontsize=10)\n",
    "    \n",
    "    # Top-5 Hit Rate\n",
    "    bars2 = ax2.bar(models_list, top5_values, color=colors_list, alpha=0.8, edgecolor='black', linewidth=0.5)\n",
    "    ax2.set_title('Top-5 Hit Rate (%)', fontsize=14, fontweight='bold')\n",
    "    ax2.set_ylabel('Hit Rate (%)', fontsize=12)\n",
    "    ax2.tick_params(axis='x', rotation=45)\n",
    "    ax2.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    for bar, value in zip(bars2, top5_values):\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1, \n",
    "                f'{value:.1f}%', ha='center', va='bottom', fontweight='bold', fontsize=10)\n",
    "    \n",
    "    # Mean Reciprocal Rank\n",
    "    bars3 = ax3.bar(models_list, mrr_values, color=colors_list, alpha=0.8, edgecolor='black', linewidth=0.5)\n",
    "    ax3.set_title('Mean Reciprocal Rank (%)', fontsize=14, fontweight='bold')\n",
    "    ax3.set_ylabel('MRR (%)', fontsize=12)\n",
    "    ax3.tick_params(axis='x', rotation=45)\n",
    "    ax3.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    for bar, value in zip(bars3, mrr_values):\n",
    "        ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5, \n",
    "                f'{value:.1f}%', ha='center', va='bottom', fontweight='bold', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Salva il grafico\n",
    "    output_path = Path(OUTPUT_DIRS['comparison']) / 'models_performance_comparison.png'\n",
    "    plt.savefig(output_path, dpi=300, bbox_inches='tight', facecolor='white')\n",
    "    print(f\"üíæ Salvato: {output_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # ============================================================================\n",
    "    # GRAFICO PERFORMANCE SINGOLE METRICHE\n",
    "    # ============================================================================\n",
    "    \n",
    "    print(\"\\nüé® Creazione grafici per singola metrica...\")\n",
    "    \n",
    "    metrics_info = [\n",
    "        ('top1_accuracy', 'Top-1 Accuracy', 'Accuracy (%)', 'top1_accuracy_by_model.png'),\n",
    "        ('top5_hit_rate', 'Top-5 Hit Rate', 'Hit Rate (%)', 'top5_hitrate_by_model.png'),\n",
    "        ('mrr', 'Mean Reciprocal Rank', 'MRR (%)', 'mrr_by_model.png')\n",
    "    ]\n",
    "    \n",
    "    for metric_key, title, ylabel, filename in metrics_info:\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        \n",
    "        values = [model_metrics[k][metric_key] * 100 for k in successfully_loaded]\n",
    "        \n",
    "        bars = ax.bar(models_list, values, color=colors_list, alpha=0.8, \n",
    "                     edgecolor='black', linewidth=0.5)\n",
    "        \n",
    "        ax.set_title(f'{title} - Model Comparison', fontsize=16, fontweight='bold')\n",
    "        ax.set_ylabel(ylabel, fontsize=12)\n",
    "        ax.tick_params(axis='x', rotation=45)\n",
    "        ax.grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        # Aggiungi valori sopra le barre\n",
    "        for bar, value in zip(bars, values):\n",
    "            ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(values)*0.01, \n",
    "                   f'{value:.1f}%', ha='center', va='bottom', fontweight='bold')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Salva\n",
    "        output_path = Path(OUTPUT_DIRS['comparison']) / filename\n",
    "        plt.savefig(output_path, dpi=300, bbox_inches='tight', facecolor='white')\n",
    "        print(f\"üíæ Salvato: {output_path}\")\n",
    "        \n",
    "        plt.show()\n",
    "    \n",
    "    print(\"‚úÖ Grafici comparativi principali completati!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Scatter Plot Performance vs Efficienza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SCATTER PLOT PERFORMANCE VS EFFICIENZA\n",
    "# ============================================================================\n",
    "\n",
    "if len(successfully_loaded) < 2:\n",
    "    print(\"‚è≠Ô∏è  Saltando scatter plot: servono almeno 2 modelli\")\n",
    "else:\n",
    "    # Controlla se abbiamo dati sui tempi di processing\n",
    "    models_with_time = [k for k in successfully_loaded \n",
    "                       if model_metrics[k]['processing_time_mean'] > 0]\n",
    "    \n",
    "    if len(models_with_time) < 2:\n",
    "        print(\"‚ö†Ô∏è  Scatter plot performance-efficienza saltato: serve processing_time per almeno 2 modelli\")\n",
    "        print(\"   üí° Assicurati che i CSV contengano la colonna 'processing_time'\")\n",
    "    else:\n",
    "        print(\"üé® Creazione scatter plot performance vs efficienza...\")\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(10, 8))\n",
    "        \n",
    "        # Prepara i dati\n",
    "        x_values = []  # Cards per secondo (efficienza)\n",
    "        y_values = []  # MRR (performance)\n",
    "        sizes = []     # Numero di parametri (per dimensione punti)\n",
    "        colors = []\n",
    "        labels = []\n",
    "        \n",
    "        # Mappa parametri a dimensioni numeriche per i punti\n",
    "        param_to_size = {\n",
    "            '7B': 100,\n",
    "            '8B': 120,\n",
    "            '8x7B (47B active)': 300,\n",
    "            '27B': 250\n",
    "        }\n",
    "        \n",
    "        for model_key in models_with_time:\n",
    "            model_config = MODELS[model_key]\n",
    "            metrics = model_metrics[model_key]\n",
    "            \n",
    "            # Calcola cards per secondo\n",
    "            cards_per_sec = 1.0 / metrics['processing_time_mean']\n",
    "            mrr_percent = metrics['mrr'] * 100\n",
    "            \n",
    "            x_values.append(cards_per_sec)\n",
    "            y_values.append(mrr_percent)\n",
    "            colors.append(model_config['color'])\n",
    "            labels.append(model_config['name'])\n",
    "            \n",
    "            # Dimensione punto basata sui parametri\n",
    "            size = param_to_size.get(model_config['params'], 150)\n",
    "            sizes.append(size)\n",
    "        \n",
    "        # Crea scatter plot\n",
    "        scatter = ax.scatter(x_values, y_values, s=sizes, c=colors, alpha=0.7, \n",
    "                           edgecolors='black', linewidth=1.5)\n",
    "        \n",
    "        # Aggiungi etichette ai punti\n",
    "        for i, (x, y, label) in enumerate(zip(x_values, y_values, labels)):\n",
    "            ax.annotate(label, (x, y), xytext=(5, 5), textcoords='offset points',\n",
    "                       fontsize=10, fontweight='bold')\n",
    "        \n",
    "        ax.set_xlabel('Processing Efficiency (Cards/Second)', fontsize=14)\n",
    "        ax.set_ylabel('Performance (MRR %)', fontsize=14)\n",
    "        ax.set_title('Performance vs Efficiency Trade-off Analysis', fontsize=16, fontweight='bold')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Aggiungi linee di riferimento (quartili o mediana)\n",
    "        if len(x_values) >= 3:\n",
    "            median_x = np.median(x_values)\n",
    "            median_y = np.median(y_values)\n",
    "            ax.axvline(median_x, color='gray', linestyle='--', alpha=0.5, label='Median Efficiency')\n",
    "            ax.axhline(median_y, color='gray', linestyle='--', alpha=0.5, label='Median Performance')\n",
    "            ax.legend(loc='best')\n",
    "        \n",
    "        # Annotazione sulla dimensione dei punti\n",
    "        ax.text(0.02, 0.98, 'Point size = Model parameters', transform=ax.transAxes, \n",
    "               fontsize=10, verticalalignment='top', bbox=dict(boxstyle='round', \n",
    "               facecolor='white', alpha=0.8))\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Salva il grafico\n",
    "        output_path = Path(OUTPUT_DIRS['comparison']) / 'performance_efficiency_scatter.png'\n",
    "        plt.savefig(output_path, dpi=300, bbox_inches='tight', facecolor='white')\n",
    "        print(f\"üíæ Salvato: {output_path}\")\n",
    "        \n",
    "        plt.show()\n",
    "        \n",
    "        # Analisi dei quadranti\n",
    "        print(\"\\nüìä ANALISI QUADRANTI (Performance vs Efficienza):\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        if len(x_values) >= 2:\n",
    "            median_x = np.median(x_values)\n",
    "            median_y = np.median(y_values)\n",
    "            \n",
    "            for i, (x, y, label) in enumerate(zip(x_values, y_values, labels)):\n",
    "                if x > median_x and y > median_y:\n",
    "                    quadrant = \"üü¢ HIGH Performance, HIGH Efficiency (Optimal)\"\n",
    "                elif x > median_x and y <= median_y:\n",
    "                    quadrant = \"üü° LOW Performance, HIGH Efficiency (Fast but inaccurate)\"\n",
    "                elif x <= median_x and y > median_y:\n",
    "                    quadrant = \"üü† HIGH Performance, LOW Efficiency (Accurate but slow)\"\n",
    "                else:\n",
    "                    quadrant = \"üî¥ LOW Performance, LOW Efficiency (Poor)\"\n",
    "                \n",
    "                print(f\"{label:>15s}: {quadrant}\")\n",
    "        \n",
    "        print(\"\\n‚úÖ Scatter plot performance-efficienza completato!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Heat Map Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# HEAT MAP PERFORMANCE NORMALIZZATA\n",
    "# ============================================================================\n",
    "\n",
    "if len(successfully_loaded) < 2:\n",
    "    print(\"‚è≠Ô∏è  Saltando heat map: servono almeno 2 modelli\")\n",
    "else:\n",
    "    print(\"üé® Creazione heat map performance normalizzata...\")\n",
    "    \n",
    "    # Prepara dati per heat map\n",
    "    metrics_for_heatmap = ['top1_accuracy', 'top5_hit_rate', 'mrr', 'catalogue_coverage']\n",
    "    metric_labels = ['Top-1\\nAccuracy', 'Top-5\\nHit Rate', 'Mean Reciprocal\\nRank', 'Catalogue\\nCoverage']\n",
    "    \n",
    "    # Crea matrice dati\n",
    "    heatmap_data = []\n",
    "    model_labels = []\n",
    "    \n",
    "    for model_key in successfully_loaded:\n",
    "        model_config = MODELS[model_key]\n",
    "        metrics = model_metrics[model_key]\n",
    "        \n",
    "        row_data = []\n",
    "        for metric in metrics_for_heatmap:\n",
    "            if metric == 'catalogue_coverage':\n",
    "                # Coverage pu√≤ essere > 1, quindi normalizziamo diversamente\n",
    "                value = min(metrics[metric] * 100, 100)  # Cap a 100%\n",
    "            else:\n",
    "                value = metrics[metric] * 100\n",
    "            row_data.append(value)\n",
    "        \n",
    "        heatmap_data.append(row_data)\n",
    "        model_labels.append(model_config['name'])\n",
    "    \n",
    "    # Converte a numpy array per facilit√†\n",
    "    heatmap_matrix = np.array(heatmap_data)\n",
    "    \n",
    "    # Crea heat map\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    \n",
    "    # Usa colormap personalizzata\n",
    "    im = ax.imshow(heatmap_matrix, cmap='RdYlGn', aspect='auto', vmin=0, vmax=100)\n",
    "    \n",
    "    # Configura assi\n",
    "    ax.set_xticks(range(len(metric_labels)))\n",
    "    ax.set_xticklabels(metric_labels, rotation=0, ha='center')\n",
    "    ax.set_yticks(range(len(model_labels)))\n",
    "    ax.set_yticklabels(model_labels)\n",
    "    \n",
    "    # Aggiungi valori nelle celle\n",
    "    for i in range(len(model_labels)):\n",
    "        for j in range(len(metric_labels)):\n",
    "            value = heatmap_matrix[i, j]\n",
    "            text_color = 'white' if value < 50 else 'black'\n",
    "            ax.text(j, i, f'{value:.1f}%', ha='center', va='center', \n",
    "                   color=text_color, fontweight='bold', fontsize=10)\n",
    "    \n",
    "    ax.set_title('Performance Heat Map - Model Comparison\\n(Normalized 0-100%)', \n",
    "                fontsize=16, fontweight='bold', pad=20)\n",
    "    \n",
    "    # Colorbar\n",
    "    cbar = plt.colorbar(im, ax=ax, shrink=0.8, aspect=20)\n",
    "    cbar.set_label('Performance (%)', rotation=270, labelpad=20, fontsize=12)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Salva\n",
    "    output_path = Path(OUTPUT_DIRS['comparison']) / 'models_performance_heatmap.png'\n",
    "    plt.savefig(output_path, dpi=300, bbox_inches='tight', facecolor='white')\n",
    "    print(f\"üíæ Salvato: {output_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # Analisi best/worst per categoria\n",
    "    print(\"\\nüìä ANALISI HEAT MAP:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    for j, metric_name in enumerate(['Top-1 Accuracy', 'Top-5 Hit Rate', 'MRR', 'Coverage']):\n",
    "        values_for_metric = heatmap_matrix[:, j]\n",
    "        best_idx = np.argmax(values_for_metric)\n",
    "        worst_idx = np.argmin(values_for_metric)\n",
    "        \n",
    "        print(f\"{metric_name}:\")\n",
    "        print(f\"  ü•á Best:  {model_labels[best_idx]} ({values_for_metric[best_idx]:.1f}%)\")\n",
    "        print(f\"  ü•â Worst: {model_labels[worst_idx]} ({values_for_metric[worst_idx]:.1f}%)\")\n",
    "        print()\n",
    "    \n",
    "    print(\"‚úÖ Heat map performance completata!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìà 4. Analisi Temporale Multi-Modello\n",
    "\n",
    "Confronto delle performance nel tempo per identificare stabilit√† e trend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# ANALISI TEMPORALE MULTI-MODELLO\n",
    "# ============================================================================\n",
    "\n",
    "if len(successfully_loaded) < 2:\n",
    "    print(\"‚è≠Ô∏è  Saltando analisi temporale: servono almeno 2 modelli\")\n",
    "else:\n",
    "    print(\"üìÖ Analisi temporale multi-modello...\")\n",
    "    \n",
    "    # Verifica che abbiamo dati temporali\n",
    "    temporal_data_available = False\n",
    "    for model_key in successfully_loaded:\n",
    "        df = model_data[model_key]\n",
    "        if 'year' in df.columns and not df['year'].isna().all():\n",
    "            temporal_data_available = True\n",
    "            break\n",
    "    \n",
    "    if not temporal_data_available:\n",
    "        print(\"   ‚ö†Ô∏è  Nessun dato temporale disponibile (colonna 'year' mancante o vuota)\")\n",
    "    else:\n",
    "        print(\"   ‚úÖ Dati temporali trovati, creazione grafici...\")\n",
    "        \n",
    "        # Calcola metriche per anno per ogni modello\n",
    "        temporal_results = {}\n",
    "        all_years = set()\n",
    "        \n",
    "        for model_key in successfully_loaded:\n",
    "            df = model_data[model_key]\n",
    "            if 'year' in df.columns:\n",
    "                # Filtra anni validi\n",
    "                df_with_year = df[df['year'].notna()]\n",
    "                \n",
    "                if len(df_with_year) > 0:\n",
    "                    # Calcola metriche per anno\n",
    "                    yearly_metrics = {}\n",
    "                    for year in df_with_year['year'].unique():\n",
    "                        year_data = df_with_year[df_with_year['year'] == year]\n",
    "                        if len(year_data) > 0:\n",
    "                            metrics = calculate_metrics(year_data)\n",
    "                            yearly_metrics[int(year)] = metrics\n",
    "                            all_years.add(int(year))\n",
    "                    \n",
    "                    temporal_results[model_key] = yearly_metrics\n",
    "        \n",
    "        if len(temporal_results) == 0:\n",
    "            print(\"   ‚ö†Ô∏è  Nessun risultato temporale calcolabile\")\n",
    "        else:\n",
    "            # Ordina gli anni\n",
    "            sorted_years = sorted(all_years)\n",
    "            print(f\"   üìä Anni disponibili: {sorted_years}\")\n",
    "            \n",
    "            # Crea grafici temporali\n",
    "            metrics_to_plot = [\n",
    "                ('top1_accuracy', 'Top-1 Accuracy (%)', 'temporal_top1_comparison.png'),\n",
    "                ('top5_hit_rate', 'Top-5 Hit Rate (%)', 'temporal_top5_comparison.png'),\n",
    "                ('mrr', 'Mean Reciprocal Rank (%)', 'temporal_mrr_comparison.png')\n",
    "            ]\n",
    "            \n",
    "            for metric_key, ylabel, filename in metrics_to_plot:\n",
    "                fig, ax = plt.subplots(figsize=(12, 6))\n",
    "                \n",
    "                # Plot linea per ogni modello\n",
    "                for model_key in successfully_loaded:\n",
    "                    if model_key in temporal_results:\n",
    "                        model_config = MODELS[model_key]\n",
    "                        \n",
    "                        # Prepara dati per questo modello\n",
    "                        x_data = []\n",
    "                        y_data = []\n",
    "                        \n",
    "                        for year in sorted_years:\n",
    "                            if year in temporal_results[model_key]:\n",
    "                                x_data.append(year)\n",
    "                                y_data.append(temporal_results[model_key][year][metric_key] * 100)\n",
    "                        \n",
    "                        if len(x_data) > 0:\n",
    "                            ax.plot(x_data, y_data, color=model_config['color'], \n",
    "                                   marker='o', linewidth=2, markersize=6,\n",
    "                                   label=model_config['name'])\n",
    "                \n",
    "                ax.set_xlabel('Year', fontsize=12)\n",
    "                ax.set_ylabel(ylabel, fontsize=12)\n",
    "                ax.set_title(f'{ylabel.replace(\" (%)\", \"\")} - Temporal Comparison', \n",
    "                           fontsize=14, fontweight='bold')\n",
    "                ax.legend(loc='best')\n",
    "                ax.grid(True, alpha=0.3)\n",
    "                \n",
    "                # Evidenzia periodo COVID se presente\n",
    "                covid_years = [2020, 2021]\n",
    "                if any(year in sorted_years for year in covid_years):\n",
    "                    for covid_year in covid_years:\n",
    "                        if covid_year in sorted_years:\n",
    "                            ax.axvline(covid_year, color='red', linestyle='--', \n",
    "                                     alpha=0.5, linewidth=1)\n",
    "                    ax.text(0.02, 0.98, 'Red lines: COVID-19 period', \n",
    "                           transform=ax.transAxes, fontsize=10, \n",
    "                           verticalalignment='top', \n",
    "                           bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                \n",
    "                # Salva\n",
    "                output_path = Path(OUTPUT_DIRS['comparison']) / filename\n",
    "                plt.savefig(output_path, dpi=300, bbox_inches='tight', facecolor='white')\n",
    "                print(f\"üíæ Salvato: {output_path}\")\n",
    "                \n",
    "                plt.show()\n",
    "            \n",
    "            # Analisi di stabilit√† temporale\n",
    "            print(\"\\nüìä ANALISI STABILIT√Ä TEMPORALE:\")\n",
    "            print(\"-\" * 50)\n",
    "            \n",
    "            for model_key in successfully_loaded:\n",
    "                if model_key in temporal_results and len(temporal_results[model_key]) > 1:\n",
    "                    model_name = MODELS[model_key]['name']\n",
    "                    \n",
    "                    # Calcola variabilit√† per MRR\n",
    "                    mrr_values = [temporal_results[model_key][year]['mrr'] * 100 \n",
    "                                 for year in temporal_results[model_key].keys()]\n",
    "                    mrr_std = np.std(mrr_values)\n",
    "                    mrr_mean = np.mean(mrr_values)\n",
    "                    stability_score = mrr_mean / (mrr_std + 1e-6)  # Higher = more stable\n",
    "                    \n",
    "                    print(f\"{model_name:>15s}: MRR avg={mrr_mean:.1f}%, std={mrr_std:.1f}%, stability={stability_score:.1f}\")\n",
    "            \n",
    "            print(\"\\n‚úÖ Analisi temporale completata!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç 5. Analisi Errori Comparativa\n",
    "\n",
    "Confronto dei pattern di errore tra i diversi modelli per identificare bias e differenze qualitative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# ANALISI ERRORI COMPARATIVA\n",
    "# ============================================================================\n",
    "\n",
    "if len(successfully_loaded) < 2:\n",
    "    print(\"‚è≠Ô∏è  Saltando analisi errori: servono almeno 2 modelli\")\n",
    "else:\n",
    "    print(\"üîç Analisi comparativa degli errori...\")\n",
    "    \n",
    "    # Analizza i POI pi√π problematici per ogni modello\n",
    "    print(\"\\nüìä POI PI√ô PROBLEMATICI PER MODELLO:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    model_error_patterns = {}\n",
    "    \n",
    "    for model_key in successfully_loaded:\n",
    "        df = model_data[model_key]\n",
    "        model_name = MODELS[model_key]['name']\n",
    "        \n",
    "        if 'ground_truth_norm' in df.columns and 'prediction_norm' in df.columns:\n",
    "            # Calcola hit@1 se non presente\n",
    "            if 'hit@1' not in df.columns:\n",
    "                df['hit@1'] = df['prediction_norm'].str[0] == df['ground_truth_norm']\n",
    "            \n",
    "            # Analizza errori\n",
    "            errors_df = df[~df['hit@1']].copy()\n",
    "            \n",
    "            if len(errors_df) > 0:\n",
    "                # Top POI problematici (ground truth con pi√π errori)\n",
    "                error_counts = errors_df['ground_truth_norm'].value_counts().head(5)\n",
    "                total_counts = df['ground_truth_norm'].value_counts()\n",
    "                \n",
    "                print(f\"\\nüî¥ {model_name}:\")\n",
    "                print(\"   Top POI con pi√π errori:\")\n",
    "                \n",
    "                error_data = []\n",
    "                for poi, error_count in error_counts.items():\n",
    "                    total_count = total_counts.get(poi, error_count)\n",
    "                    error_rate = error_count / total_count\n",
    "                    print(f\"     {poi[:25]:>25s}: {error_count:>4d} errori ({error_rate:>5.1%})\")\n",
    "                    error_data.append((poi, error_count, error_rate))\n",
    "                \n",
    "                model_error_patterns[model_key] = error_data\n",
    "    \n",
    "    # Confronto bias geografici\n",
    "    print(f\"\\nüìä CONFRONTO BIAS NELLE PREDIZIONI:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for model_key in successfully_loaded:\n",
    "        df = model_data[model_key]\n",
    "        model_name = MODELS[model_key]['name']\n",
    "        \n",
    "        if 'prediction_norm' in df.columns:\n",
    "            # Analizza predizioni pi√π frequenti (possibili bias)\n",
    "            all_predictions = []\n",
    "            for pred_list in df['prediction_norm']:\n",
    "                if isinstance(pred_list, list) and len(pred_list) > 0:\n",
    "                    all_predictions.append(pred_list[0])  # Solo top-1 prediction\n",
    "            \n",
    "            if all_predictions:\n",
    "                pred_freq = Counter(all_predictions)\n",
    "                top_predictions = pred_freq.most_common(5)\n",
    "                \n",
    "                print(f\"\\nüéØ {model_name} - Predizioni pi√π frequenti:\")\n",
    "                total_preds = len(all_predictions)\n",
    "                for poi, count in top_predictions:\n",
    "                    percentage = (count / total_preds) * 100\n",
    "                    print(f\"     {poi[:25]:>25s}: {count:>5d} ({percentage:>4.1f}%)\")\n",
    "    \n",
    "    # Matrice di confusione comparativa (se fattibile)\n",
    "    if len(successfully_loaded) >= 2:\n",
    "        print(f\"\\nüìä Generazione matrici di confusione per confronto...\")\n",
    "        \n",
    "        # Identifica POI comuni pi√π frequenti\n",
    "        all_gt_pois = set()\n",
    "        for model_key in successfully_loaded:\n",
    "            df = model_data[model_key]\n",
    "            if 'ground_truth_norm' in df.columns:\n",
    "                all_gt_pois.update(df['ground_truth_norm'].unique())\n",
    "        \n",
    "        # Prendi top 10 POI pi√π comuni\n",
    "        poi_frequencies = Counter()\n",
    "        for model_key in successfully_loaded:\n",
    "            df = model_data[model_key]\n",
    "            if 'ground_truth_norm' in df.columns:\n",
    "                poi_frequencies.update(df['ground_truth_norm'].value_counts().to_dict())\n",
    "        \n",
    "        top_pois = [poi for poi, _ in poi_frequencies.most_common(10)]\n",
    "        \n",
    "        # Crea confusion matrix per ogni modello\n",
    "        fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "        axes = axes.flatten()\n",
    "        \n",
    "        for i, model_key in enumerate(successfully_loaded):\n",
    "            if i >= len(axes):\n",
    "                break\n",
    "                \n",
    "            df = model_data[model_key]\n",
    "            model_name = MODELS[model_key]['name']\n",
    "            ax = axes[i]\n",
    "            \n",
    "            if 'ground_truth_norm' in df.columns and 'prediction_norm' in df.columns:\n",
    "                # Filtra per top POI\n",
    "                mask = (df['ground_truth_norm'].isin(top_pois) & \n",
    "                       df['prediction_norm'].str[0].isin(top_pois))\n",
    "                df_filtered = df[mask]\n",
    "                \n",
    "                if len(df_filtered) > 0:\n",
    "                    # Crea confusion matrix\n",
    "                    cm = pd.crosstab(df_filtered['ground_truth_norm'],\n",
    "                                   df_filtered['prediction_norm'].str[0],\n",
    "                                   normalize='index')\n",
    "                    \n",
    "                    # Plot heatmap\n",
    "                    sns.heatmap(cm, annot=True, fmt='.2f', cmap='Blues', \n",
    "                              ax=ax, cbar_kws={'shrink': 0.5})\n",
    "                    ax.set_title(f'{model_name}\\nConfusion Matrix', fontweight='bold')\n",
    "                    ax.set_xlabel('Predicted')\n",
    "                    ax.set_ylabel('True')\n",
    "                    \n",
    "                    # Ruota etichette per leggibilit√†\n",
    "                    ax.tick_params(axis='x', rotation=45)\n",
    "                    ax.tick_params(axis='y', rotation=0)\n",
    "            else:\n",
    "                ax.text(0.5, 0.5, f'{model_name}\\nDati non disponibili', \n",
    "                       ha='center', va='center', transform=ax.transAxes)\n",
    "        \n",
    "        # Nascondi assi non utilizzati\n",
    "        for j in range(len(successfully_loaded), len(axes)):\n",
    "            axes[j].set_visible(False)\n",
    "        \n",
    "        plt.suptitle('Confusion Matrices Comparison - Top 10 POI', \n",
    "                    fontsize=16, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Salva\n",
    "        output_path = Path(OUTPUT_DIRS['comparison']) / 'confusion_matrices_comparison.png'\n",
    "        plt.savefig(output_path, dpi=300, bbox_inches='tight', facecolor='white')\n",
    "        print(f\"üíæ Salvato: {output_path}\")\n",
    "        \n",
    "        plt.show()\n",
    "    \n",
    "    print(\"\\n‚úÖ Analisi errori comparativa completata!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã 6. Report Finale e Riepilogo\n",
    "\n",
    "Genera un report completo con tutti i risultati e le raccomandazioni per l'integrazione nella tesi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# REPORT FINALE E RIEPILOGO\n",
    "# ============================================================================\n",
    "\n",
    "print(\"üìã Generazione report finale...\\n\")\n",
    "print(\"=\" * 80)\n",
    "print(\"üéì MULTI-MODEL COMPARISON ANALYSIS - FINAL REPORT\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if len(successfully_loaded) == 0:\n",
    "    print(\"\\n‚ùå NESSUN MODELLO CARICATO - VERIFICA CONFIGURAZIONE\")\n",
    "    print(\"\\nüí° AZIONI RICHIESTE:\")\n",
    "    print(\"   1. Crea la directory results/ nella root del progetto\")\n",
    "    print(\"   2. Crea sottodirectory per ogni modello (es. results/llama3.1_8b/)\")\n",
    "    print(\"   3. Sposta i file CSV in quelle directory\")\n",
    "    print(\"   4. Ri-esegui questo notebook\")\n",
    "else:\n",
    "    # Riepilogo modelli caricati\n",
    "    print(f\"\\nüìä MODELLI ANALIZZATI: {len(successfully_loaded)}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    total_predictions = 0\n",
    "    for model_key in successfully_loaded:\n",
    "        model_config = MODELS[model_key]\n",
    "        metrics = model_metrics[model_key]\n",
    "        total_predictions += metrics['total_predictions']\n",
    "        \n",
    "        print(f\"‚úÖ {model_config['name']:>15s}: {metrics['total_predictions']:>6,} predizioni\")\n",
    "    \n",
    "    print(f\"\\nüìà TOTALE PREDIZIONI ANALIZZATE: {total_predictions:,}\")\n",
    "    \n",
    "    # Best performers\n",
    "    print(f\"\\nüèÜ BEST PERFORMERS PER METRICA:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    metrics_names = [\n",
    "        ('top1_accuracy', 'Top-1 Accuracy'),\n",
    "        ('top5_hit_rate', 'Top-5 Hit Rate'),\n",
    "        ('mrr', 'Mean Reciprocal Rank')\n",
    "    ]\n",
    "    \n",
    "    for metric_key, metric_name in metrics_names:\n",
    "        best_model = max(successfully_loaded, key=lambda k: model_metrics[k][metric_key])\n",
    "        best_value = model_metrics[best_model][metric_key] * 100\n",
    "        print(f\"{metric_name:>18s}: {MODELS[best_model]['name']} ({best_value:.1f}%)\")\n",
    "    \n",
    "    # Efficienza se disponibile\n",
    "    models_with_time = [k for k in successfully_loaded \n",
    "                       if model_metrics[k]['processing_time_mean'] > 0]\n",
    "    \n",
    "    if models_with_time:\n",
    "        fastest_model = min(models_with_time, \n",
    "                           key=lambda k: model_metrics[k]['processing_time_mean'])\n",
    "        fastest_time = model_metrics[fastest_model]['processing_time_mean']\n",
    "        print(f\"{'Efficienza':>18s}: {MODELS[fastest_model]['name']} ({fastest_time:.1f}s/card)\")\n",
    "    \n",
    "    # Files generati\n",
    "    print(f\"\\nüìÅ FILES GENERATI:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    comparison_dir = Path(OUTPUT_DIRS['comparison'])\n",
    "    generated_files = list(comparison_dir.glob('*.png')) + list(comparison_dir.glob('*.csv')) + list(comparison_dir.glob('*.tex'))\n",
    "    \n",
    "    print(f\"Directory: {comparison_dir}\")\n",
    "    for file_path in sorted(generated_files):\n",
    "        file_size = file_path.stat().st_size / 1024  # KB\n",
    "        print(f\"  ‚úÖ {file_path.name} ({file_size:.1f} KB)\")\n",
    "    \n",
    "    # Istruzioni per LaTeX\n",
    "    print(f\"\\nüìÑ INTEGRAZIONE LATEX:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(\"1. I grafici sono salvati in img/multi_model_comparison/\")\n",
    "    print(\"2. La tabella LaTeX √® in performance_table.tex\")\n",
    "    print(\"3. Sostituisci i placeholder --% nella tabella del .tex con i dati reali\")\n",
    "    print(\"4. Decommenta le linee \\\\includegraphics nel file .tex\")\n",
    "    \n",
    "    # Raccomandazioni\n",
    "    print(f\"\\nüí° RACCOMANDAZIONI:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    if len(successfully_loaded) >= 3:\n",
    "        # Identifica modelli complementari\n",
    "        best_accuracy = max(successfully_loaded, key=lambda k: model_metrics[k]['top1_accuracy'])\n",
    "        best_efficiency = min(models_with_time, key=lambda k: model_metrics[k]['processing_time_mean']) if models_with_time else None\n",
    "        \n",
    "        print(f\"üéØ Per accuratezza massima: {MODELS[best_accuracy]['name']}\")\n",
    "        if best_efficiency:\n",
    "            print(f\"‚ö° Per efficienza massima: {MODELS[best_efficiency]['name']}\")\n",
    "        \n",
    "        # Analisi trade-off\n",
    "        if models_with_time and len(models_with_time) >= 2:\n",
    "            print(\"\\nüîÑ TRADE-OFF ANALYSIS:\")\n",
    "            for model_key in models_with_time:\n",
    "                model_name = MODELS[model_key]['name']\n",
    "                accuracy = model_metrics[model_key]['top1_accuracy'] * 100\n",
    "                time_per_card = model_metrics[model_key]['processing_time_mean']\n",
    "                efficiency_score = accuracy / time_per_card  # accuracy per second\n",
    "                print(f\"  {model_name:>15s}: {efficiency_score:.2f} accuracy/second\")\n",
    "    \n",
    "    # Status completamento\n",
    "    print(f\"\\n‚úÖ ANALISI COMPLETATA CON SUCCESSO!\")\n",
    "    print(f\"   üìä {len(successfully_loaded)} modelli analizzati\")\n",
    "    print(f\"   üìà {len(generated_files)} file generati\")\n",
    "    print(f\"   üéØ Pronti per integrazione nella tesi\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üéì MULTI-MODEL ANALYSIS COMPLETED\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìù Note Finali e Prossimi Passi\n",
    "\n",
    "### ‚úÖ Checklist Completamento\n",
    "\n",
    "- [ ] **Dati caricati**: Tutti i modelli hanno dati in `results/`\n",
    "- [ ] **Grafici generati**: Files PNG creati in `img/multi_model_comparison/`\n",
    "- [ ] **Tabella pronta**: File LaTeX generato per integrazione\n",
    "- [ ] **Analisi completata**: Report finale visualizzato\n",
    "\n",
    "### üîÑ Prossimi Passi\n",
    "\n",
    "1. **Verifica grafici**: Controlla che tutti i PNG siano stati generati correttamente\n",
    "2. **Integra LaTeX**: Copia il contenuto di `performance_table.tex` nel tuo file principale\n",
    "3. **Decommenta immagini**: Rimuovi i `%` dalle linee `\\includegraphics` nel .tex\n",
    "4. **Compila tesi**: Verifica che LaTeX compili senza errori\n",
    "5. **Revisiona risultati**: Controlla che i grafici supportino la narrativa della tesi\n",
    "\n",
    "### ‚ö†Ô∏è Troubleshooting\n",
    "\n",
    "- **Nessun modello caricato**: Verifica struttura directory `results/`\n",
    "- **Grafici mancanti**: Controlla che directory `img/` esista e sia scrivibile\n",
    "- **Errori LaTeX**: Verifica path delle immagini (`../../img/...`)\n",
    "- **Performance inattese**: Controlla formato e qualit√† dei dati CSV\n",
    "\n",
    "---\n",
    "\n",
    "**üéâ Congratulazioni!** Hai ora un sistema completo per confrontare le performance dei tuoi modelli LLM sulla predizione di mobilit√† turistica. I grafici e le analisi generate sono pronti per l'inclusione nella tua tesi di laurea."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
