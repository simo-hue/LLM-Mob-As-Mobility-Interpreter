{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "presentation_header",
   "metadata": {},
   "source": [
    "# üìä Export CSV per Presentazione Canva\n",
    "\n",
    "Questo notebook estrae i dati chiave dall'analisi delle metriche e li esporta in formato CSV\n",
    "ottimizzato per l'importazione diretta in Canva per mantenere uno stile coerente nella presentazione.\n",
    "\n",
    "## üìã File CSV generati:\n",
    "1. **`metriche_per_anno.csv`** - Performance per anno (Top-1, Top-5, MRR)\n",
    "2. **`confronto_modelli.csv`** - Confronto tra diversi modelli \n",
    "3. **`metriche_globali.csv`** - Performance aggregate generali\n",
    "4. **`coverage_analysis.csv`** - Analisi della copertura del catalogo\n",
    "5. **`yearly_summary.csv`** - Riassunto annuale con conteggi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importazioni necessarie\n",
    "from pathlib import Path\n",
    "import glob, ast, os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Crea directory output per i CSV\n",
    "output_dir = Path('../csv_for_canva')\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"‚úÖ Directory output creata: {output_dir}\")\n",
    "print(f\"üìÅ I file CSV verranno salvati in: {output_dir.absolute()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data_loading",
   "metadata": {},
   "source": [
    "## üîÑ Caricamento Dati\n",
    "Utilizziamo lo stesso codice del notebook principale per caricare e processare i dati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load_data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funzioni di parsing (copiate dal notebook principale)\n",
    "def safe_parse_prediction(x, debug_mode=False):\n",
    "    \"\"\"\n",
    "    Parsing sicuro delle predizioni da stringa a lista\n",
    "    Gestisce vari formati inclusi dict e stringhe\n",
    "    \"\"\"\n",
    "    if pd.isna(x) or x == \"[]\" or x == \"\" or not isinstance(x, str):\n",
    "        return []\n",
    "    \n",
    "    try:\n",
    "        parsed = ast.literal_eval(x)\n",
    "        \n",
    "        if isinstance(parsed, list):\n",
    "            # Se √® una lista, estrai le stringhe (gestisce sia str che dict)\n",
    "            result = []\n",
    "            for item in parsed:\n",
    "                if isinstance(item, str):\n",
    "                    result.append(item.strip())\n",
    "                elif isinstance(item, dict):\n",
    "                    # Se √® un dict, prova a estrarre un campo chiave\n",
    "                    if 'name' in item:\n",
    "                        result.append(str(item['name']).strip())\n",
    "                    elif 'poi' in item:\n",
    "                        result.append(str(item['poi']).strip())\n",
    "                    else:\n",
    "                        # Prendi il primo valore del dict\n",
    "                        values = list(item.values())\n",
    "                        if values:\n",
    "                            result.append(str(values[0]).strip())\n",
    "                else:\n",
    "                    # Converti altri tipi in stringa\n",
    "                    result.append(str(item).strip())\n",
    "            return result\n",
    "        else:\n",
    "            return []\n",
    "    except Exception as e:\n",
    "        if debug_mode:\n",
    "            print(f\"Errore parsing: {x} -> {e}\")\n",
    "        return []\n",
    "\n",
    "def normalize_poi_name(poi_name):\n",
    "    \"\"\"\n",
    "    Normalizza il nome di un POI per confronti consistenti\n",
    "    \"\"\"\n",
    "    if pd.isna(poi_name) or not isinstance(poi_name, str):\n",
    "        return str(poi_name).strip()\n",
    "    \n",
    "    # Rimuovi spazi extra e normalizza\n",
    "    normalized = poi_name.strip()\n",
    "    \n",
    "    # Potrebbero esserci altre normalizzazioni specifiche\n",
    "    # Es: rimuovere caratteri speciali, convertire a lowercase, etc.\n",
    "    \n",
    "    return normalized\n",
    "\n",
    "def load_and_process_data(model_name, version=\"base_version\"):\n",
    "    \"\"\"\n",
    "    Carica e processa i dati per un modello specifico\n",
    "    \"\"\"\n",
    "    # Path dei file CSV\n",
    "    csv_pattern = f'../results/{model_name}/{version}/*_pred_*.csv'\n",
    "    csv_files = [Path(p) for p in glob.glob(csv_pattern)]\n",
    "    csv_files = sorted(csv_files)\n",
    "    \n",
    "    if not csv_files:\n",
    "        print(f\"‚ùå Nessun file trovato per {model_name}/{version}\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"üìÇ Trovati {len(csv_files)} file per {model_name}/{version}\")\n",
    "    \n",
    "    # Lista per contenere tutti i DataFrame\n",
    "    dfs = []\n",
    "    total_processed = 0\n",
    "    \n",
    "    for fp in csv_files:\n",
    "        print(f\"  Processando {fp.name}...\")\n",
    "        try:\n",
    "            df = pd.read_csv(fp)\n",
    "        except pd.errors.ParserError:\n",
    "            print(f\"‚ö†Ô∏è Errore parsing {fp.name}, usando error handling...\")\n",
    "            df = pd.read_csv(fp, on_bad_lines='skip', engine='python')\n",
    "        \n",
    "        print(f\"    Righe caricate: {len(df)}\")\n",
    "        \n",
    "        # Estrai anno dal nome file\n",
    "        year_token = next((part for part in fp.stem.split('_') \n",
    "                          if part.isdigit() and len(part) == 4), None)\n",
    "        if year_token:\n",
    "            df['year'] = int(year_token)\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è Anno non trovato in {fp.name}\")\n",
    "            continue\n",
    "        \n",
    "        # Parsing predizioni con gestione errori migliorata\n",
    "        df['prediction_list'] = df['prediction'].apply(safe_parse_prediction)\n",
    "        \n",
    "        # Debug: mostra alcuni esempi di parsing\n",
    "        valid_predictions = df[df['prediction_list'].apply(lambda x: len(x) > 0)]\n",
    "        invalid_predictions = df[df['prediction_list'].apply(lambda x: len(x) == 0)]\n",
    "        \n",
    "        print(f\"    Predizioni valide: {len(valid_predictions)}\")\n",
    "        print(f\"    Predizioni invalide: {len(invalid_predictions)}\")\n",
    "        \n",
    "        # Filtra righe valide (con predizioni non vuote)\n",
    "        df = df[df['prediction_list'].apply(lambda x: isinstance(x, list) and len(x) > 0)]\n",
    "        \n",
    "        if len(df) > 0:\n",
    "            dfs.append(df)\n",
    "            total_processed += len(df)\n",
    "            print(f\"    ‚úÖ Righe valide aggiunte: {len(df)}\")\n",
    "        else:\n",
    "            print(f\"    ‚ùå Nessuna riga valida in {fp.name}\")\n",
    "    \n",
    "    if not dfs:\n",
    "        print(f\"‚ùå Nessun dato valido trovato per {model_name}\")\n",
    "        return None\n",
    "    \n",
    "    # Concatena tutti i DataFrame\n",
    "    df_all = pd.concat(dfs, ignore_index=True)\n",
    "    print(f\"‚úÖ Dataset combinato: {len(df_all):,} righe\")\n",
    "    \n",
    "    # Normalizzazione POI (migliorata)\n",
    "    df_all['prediction_norm'] = df_all['prediction_list'].apply(\n",
    "        lambda x: [normalize_poi_name(poi) for poi in x] if isinstance(x, list) else []\n",
    "    )\n",
    "    df_all['ground_truth_norm'] = df_all['ground_truth'].apply(normalize_poi_name)\n",
    "    \n",
    "    return df_all\n",
    "\n",
    "print(\"‚úÖ Funzioni di caricamento dati definite (versione corretta)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "calculate_metrics",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(df):\n",
    "    \"\"\"\n",
    "    Calcola le metriche per un dataset\n",
    "    \"\"\"\n",
    "    def safe_top1_accuracy(row):\n",
    "        pred_norm = row['prediction_norm']\n",
    "        if not pred_norm or len(pred_norm) == 0:\n",
    "            return False\n",
    "        return pred_norm[0] == row['ground_truth_norm']\n",
    "    \n",
    "    def safe_top_k_hit(row, k=5):\n",
    "        pred_norm = row['prediction_norm']\n",
    "        if not pred_norm or len(pred_norm) == 0:\n",
    "            return False\n",
    "        return row['ground_truth_norm'] in pred_norm[:k]\n",
    "    \n",
    "    def safe_reciprocal_rank(row, k=5):\n",
    "        pred_norm = row['prediction_norm']\n",
    "        if not pred_norm or len(pred_norm) == 0:\n",
    "            return 0.0\n",
    "        try:\n",
    "            rank = pred_norm[:k].index(row['ground_truth_norm']) + 1\n",
    "            return 1.0 / rank\n",
    "        except ValueError:\n",
    "            return 0.0\n",
    "    \n",
    "    # Calcola metriche\n",
    "    df['hit@1'] = df.apply(safe_top1_accuracy, axis=1)\n",
    "    df['hit@5'] = df.apply(safe_top_k_hit, axis=1)\n",
    "    df['rr'] = df.apply(safe_reciprocal_rank, axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "print(\"‚úÖ Funzioni di calcolo metriche definite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "process_models",
   "metadata": {},
   "source": [
    "## ü§ñ Processamento Modelli\n",
    "Carichiamo e processiamo i dati per tutti i modelli disponibili"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load_models",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista dei modelli da processare (in ordine di priorit√†)\n",
    "models_to_process = [\n",
    "    'qwen2.5_14b',\n",
    "    'deepseek-coder_33b', \n",
    "    'qwen2.5_7b',\n",
    "    'llama3.1_8b',\n",
    "    'mixtral_8x7b',\n",
    "    'deepseek-r1_32b'  # Aggiungo anche questo se presente\n",
    "]\n",
    "\n",
    "# Dizionario per contenere i dati di tutti i modelli\n",
    "models_data = {}\n",
    "failed_models = []\n",
    "\n",
    "# Carica dati per ogni modello\n",
    "for model in models_to_process:\n",
    "    print(f\"\\nüîÑ Processando {model}...\")\n",
    "    try:\n",
    "        df = load_and_process_data(model, \"base_version\")\n",
    "        if df is not None and len(df) > 0:\n",
    "            df = calculate_metrics(df)\n",
    "            df['model'] = model  # Aggiungi colonna modello\n",
    "            models_data[model] = df\n",
    "            print(f\"‚úÖ {model}: {len(df):,} predizioni caricate\")\n",
    "            \n",
    "            # Mostra alcune statistiche di base\n",
    "            years = sorted(df['year'].unique())\n",
    "            print(f\"   üìÖ Anni coperti: {years[0]} - {years[-1]} ({len(years)} anni)\")\n",
    "            print(f\"   üéØ Top-1 Accuracy: {df['hit@1'].mean():.1%}\")\n",
    "        else:\n",
    "            print(f\"‚ùå {model}: Dataset vuoto o errore nel caricamento\")\n",
    "            failed_models.append(model)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå {model}: Errore - {str(e)}\")\n",
    "        failed_models.append(model)\n",
    "\n",
    "print(f\"\\nüìä RIEPILOGO CARICAMENTO:\")\n",
    "print(f\"‚úÖ Modelli caricati con successo: {len(models_data)}\")\n",
    "print(f\"‚ùå Modelli falliti: {len(failed_models)}\")\n",
    "\n",
    "if failed_models:\n",
    "    print(f\"   Modelli falliti: {', '.join(failed_models)}\")\n",
    "\n",
    "# Crea un dataset combinato per alcune analisi (solo se ci sono dati)\n",
    "if models_data:\n",
    "    try:\n",
    "        combined_df = pd.concat(models_data.values(), ignore_index=True)\n",
    "        print(f\"üìà Dataset combinato: {len(combined_df):,} predizioni totali\")\n",
    "        print(f\"ü§ñ Modelli nel dataset combinato: {', '.join(models_data.keys())}\")\n",
    "        \n",
    "        # Verifica la distribuzione per modello\n",
    "        model_counts = combined_df['model'].value_counts()\n",
    "        print(\"üìä Distribuzione predizioni per modello:\")\n",
    "        for model, count in model_counts.items():\n",
    "            print(f\"   {model}: {count:,}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Errore nella creazione del dataset combinato: {e}\")\n",
    "        combined_df = None\n",
    "else:\n",
    "    combined_df = None\n",
    "    print(\"‚ùå Nessun modello caricato con successo\")\n",
    "    print(\"üîß Suggerimenti:\")\n",
    "    print(\"   - Verifica che i file CSV esistano nella directory ../results/\")\n",
    "    print(\"   - Controlla che i path dei modelli siano corretti\")\n",
    "    print(\"   - Verifica la struttura dei file CSV\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "export_section",
   "metadata": {},
   "source": [
    "## üì§ Esportazione CSV per Canva\n",
    "Generiamo tutti i file CSV ottimizzati per Canva"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "export_yearly_metrics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. METRICHE PER ANNO (usando il modello migliore disponibile)\n",
    "if models_data:\n",
    "    # Scegli il modello principale in ordine di priorit√†\n",
    "    primary_model = None\n",
    "    priority_order = ['qwen2.5_14b', 'deepseek-coder_33b', 'qwen2.5_7b', 'llama3.1_8b', 'mixtral_8x7b']\n",
    "    \n",
    "    for preferred_model in priority_order:\n",
    "        if preferred_model in models_data:\n",
    "            primary_model = preferred_model\n",
    "            break\n",
    "    \n",
    "    # Se nessun modello prioritario, prendi il primo disponibile\n",
    "    if primary_model is None:\n",
    "        primary_model = list(models_data.keys())[0]\n",
    "    \n",
    "    df_primary = models_data[primary_model]\n",
    "    print(f\"üéØ Usando {primary_model} come modello principale per metriche annuali\")\n",
    "    \n",
    "    try:\n",
    "        # Calcola metriche per anno\n",
    "        yearly_metrics = df_primary.groupby('year').agg({\n",
    "            'hit@1': 'mean',\n",
    "            'hit@5': 'mean', \n",
    "            'rr': 'mean',\n",
    "            'card_id': 'count'\n",
    "        }).round(4)\n",
    "        \n",
    "        # Rinomina colonne per Canva\n",
    "        yearly_metrics.columns = ['Top-1_Accuracy', 'Top-5_Hit_Rate', 'MRR', 'Num_Predictions']\n",
    "        \n",
    "        # Converti percentuali\n",
    "        yearly_metrics['Top-1_Accuracy_Percent'] = (yearly_metrics['Top-1_Accuracy'] * 100).round(2)\n",
    "        yearly_metrics['Top-5_Hit_Rate_Percent'] = (yearly_metrics['Top-5_Hit_Rate'] * 100).round(2) \n",
    "        yearly_metrics['MRR_Percent'] = (yearly_metrics['MRR'] * 100).round(2)\n",
    "        \n",
    "        # Aggiungi informazioni sul modello\n",
    "        yearly_metrics['Model_Used'] = primary_model\n",
    "        yearly_metrics['Model_Display'] = primary_model.replace('_', ' ').replace('-', ' ').title()\n",
    "        \n",
    "        # Reset index per avere year come colonna\n",
    "        yearly_metrics = yearly_metrics.reset_index()\n",
    "        \n",
    "        # Aggiungi medie per riferimento\n",
    "        overall_means = {\n",
    "            'year': 'MEDIA',\n",
    "            'Top-1_Accuracy': yearly_metrics['Top-1_Accuracy'].mean(),\n",
    "            'Top-5_Hit_Rate': yearly_metrics['Top-5_Hit_Rate'].mean(),\n",
    "            'MRR': yearly_metrics['MRR'].mean(),\n",
    "            'Num_Predictions': yearly_metrics['Num_Predictions'].sum(),\n",
    "            'Top-1_Accuracy_Percent': yearly_metrics['Top-1_Accuracy_Percent'].mean(),\n",
    "            'Top-5_Hit_Rate_Percent': yearly_metrics['Top-5_Hit_Rate_Percent'].mean(),\n",
    "            'MRR_Percent': yearly_metrics['MRR_Percent'].mean(),\n",
    "            'Model_Used': primary_model,\n",
    "            'Model_Display': yearly_metrics['Model_Display'].iloc[0]\n",
    "        }\n",
    "        \n",
    "        # Aggiungi la riga media\n",
    "        yearly_metrics = pd.concat([yearly_metrics, pd.DataFrame([overall_means])], ignore_index=True)\n",
    "        \n",
    "        # Esporta\n",
    "        yearly_csv = output_dir / 'metriche_per_anno.csv'\n",
    "        yearly_metrics.to_csv(yearly_csv, index=False)\n",
    "        \n",
    "        print(f\"‚úÖ Esportato: {yearly_csv}\")\n",
    "        print(f\"üìä Dati per {len(yearly_metrics)-1} anni + riga media (modello: {primary_model})\")\n",
    "        print(\"üìã Prime righe:\")\n",
    "        display(yearly_metrics.head())\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Errore nell'esportazione metriche annuali: {e}\")\n",
    "        print(\"üîß Controllare che il dataset contenga colonne year, hit@1, hit@5, rr, card_id\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Nessun modello disponibile per le metriche annuali\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "export_model_comparison",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. CONFRONTO TRA MODELLI\n",
    "if len(models_data) > 1:\n",
    "    print(\"üîÑ Generando confronto tra modelli...\")\n",
    "    try:\n",
    "        model_comparison = []\n",
    "        \n",
    "        for model_name, df in models_data.items():\n",
    "            # Verifica che il DataFrame contenga le colonne necessarie\n",
    "            required_cols = ['hit@1', 'hit@5', 'rr', 'year']\n",
    "            missing_cols = [col for col in required_cols if col not in df.columns]\n",
    "            \n",
    "            if missing_cols:\n",
    "                print(f\"‚ö†Ô∏è {model_name}: colonne mancanti {missing_cols}, skip\")\n",
    "                continue\n",
    "            \n",
    "            # Calcola metriche globali per ogni modello\n",
    "            metrics = {\n",
    "                'Model': model_name.replace('_', ' ').replace('-', ' ').title(),\n",
    "                'Model_Code': model_name,\n",
    "                'Top-1_Accuracy': df['hit@1'].mean() if len(df) > 0 else 0,\n",
    "                'Top-5_Hit_Rate': df['hit@5'].mean() if len(df) > 0 else 0,\n",
    "                'MRR': df['rr'].mean() if len(df) > 0 else 0,\n",
    "                'Total_Predictions': len(df),\n",
    "                'Years_Covered': len(df['year'].unique()) if 'year' in df.columns else 0,\n",
    "                'Year_Range': f\"{df['year'].min()}-{df['year'].max()}\" if 'year' in df.columns and len(df) > 0 else \"N/A\"\n",
    "            }\n",
    "            \n",
    "            # Converti in percentuali\n",
    "            metrics['Top-1_Accuracy_Percent'] = round(metrics['Top-1_Accuracy'] * 100, 2)\n",
    "            metrics['Top-5_Hit_Rate_Percent'] = round(metrics['Top-5_Hit_Rate'] * 100, 2)\n",
    "            metrics['MRR_Percent'] = round(metrics['MRR'] * 100, 2)\n",
    "            \n",
    "            # Aggiungi metriche di qualit√†\n",
    "            metrics['Error_Rate_Percent'] = round(100 - metrics['Top-1_Accuracy_Percent'], 2)\n",
    "            metrics['Predictions_Per_Year'] = round(metrics['Total_Predictions'] / max(1, metrics['Years_Covered']), 0)\n",
    "            \n",
    "            model_comparison.append(metrics)\n",
    "        \n",
    "        if model_comparison:\n",
    "            # Crea DataFrame e ordina per Top-1 Accuracy\n",
    "            model_comparison_df = pd.DataFrame(model_comparison)\n",
    "            model_comparison_df = model_comparison_df.sort_values('Top-1_Accuracy_Percent', ascending=False)\n",
    "            \n",
    "            # Aggiungi ranking\n",
    "            model_comparison_df['Rank_Top1'] = range(1, len(model_comparison_df) + 1)\n",
    "            model_comparison_df['Rank_Top5'] = model_comparison_df['Top-5_Hit_Rate_Percent'].rank(ascending=False, method='min').astype(int)\n",
    "            model_comparison_df['Rank_MRR'] = model_comparison_df['MRR_Percent'].rank(ascending=False, method='min').astype(int)\n",
    "            \n",
    "            # Aggiungi classificazioni qualitative\n",
    "            model_comparison_df['Performance_Level'] = pd.cut(\n",
    "                model_comparison_df['Top-1_Accuracy_Percent'],\n",
    "                bins=[0, 3, 5, 7, 100],\n",
    "                labels=['Base', 'Good', 'Very Good', 'Excellent']\n",
    "            )\n",
    "            \n",
    "            # Esporta\n",
    "            comparison_csv = output_dir / 'confronto_modelli.csv'\n",
    "            model_comparison_df.to_csv(comparison_csv, index=False)\n",
    "            \n",
    "            print(f\"‚úÖ Esportato: {comparison_csv}\")\n",
    "            print(f\"ü§ñ Confronto di {len(model_comparison_df)} modelli\")\n",
    "            print(\"üèÜ Top 3 modelli per Top-1 Accuracy:\")\n",
    "            top_models = model_comparison_df.head(3)[['Model', 'Top-1_Accuracy_Percent', 'Rank_Top1']]\n",
    "            for _, row in top_models.iterrows():\n",
    "                print(f\"   {row['Rank_Top1']}. {row['Model']}: {row['Top-1_Accuracy_Percent']}%\")\n",
    "            \n",
    "            display(model_comparison_df[['Model', 'Top-1_Accuracy_Percent', 'Top-5_Hit_Rate_Percent', 'MRR_Percent', 'Performance_Level']].head())\n",
    "        else:\n",
    "            print(\"‚ùå Nessun modello valido per il confronto\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Errore nella generazione confronto modelli: {e}\")\n",
    "        print(\"üîß Verificare che i modelli abbiano le colonne hit@1, hit@5, rr\")\n",
    "\n",
    "elif len(models_data) == 1:\n",
    "    print(\"‚ö†Ô∏è Un solo modello disponibile, creando file di confronto con singolo modello\")\n",
    "    try:\n",
    "        single_model = list(models_data.keys())[0]\n",
    "        df = models_data[single_model]\n",
    "        \n",
    "        single_model_data = [{\n",
    "            'Model': single_model.replace('_', ' ').replace('-', ' ').title(),\n",
    "            'Model_Code': single_model,\n",
    "            'Top-1_Accuracy_Percent': round(df['hit@1'].mean() * 100, 2),\n",
    "            'Top-5_Hit_Rate_Percent': round(df['hit@5'].mean() * 100, 2),\n",
    "            'MRR_Percent': round(df['rr'].mean() * 100, 2),\n",
    "            'Total_Predictions': len(df),\n",
    "            'Years_Covered': len(df['year'].unique()),\n",
    "            'Rank_Top1': 1,\n",
    "            'Performance_Level': 'Single Model',\n",
    "            'Note': 'Only model available'\n",
    "        }]\n",
    "        \n",
    "        single_model_df = pd.DataFrame(single_model_data)\n",
    "        comparison_csv = output_dir / 'confronto_modelli.csv'\n",
    "        single_model_df.to_csv(comparison_csv, index=False)\n",
    "        print(f\"‚úÖ Esportato confronto singolo modello: {comparison_csv}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Errore nell'esportazione singolo modello: {e}\")\n",
    "else:\n",
    "    print(\"‚ùå Nessun modello disponibile per il confronto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "export_global_metrics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. METRICHE GLOBALI (aggregate su tutti i modelli o modello principale)\n",
    "if combined_df is not None:\n",
    "    # Usa il dataset combinato se disponibile\n",
    "    df_for_global = combined_df\n",
    "    scope = \"Tutti i Modelli\"\n",
    "else:\n",
    "    # Usa il modello principale\n",
    "    df_for_global = df_primary\n",
    "    scope = f\"Modello {primary_model}\"\n",
    "\n",
    "# Calcola metriche globali\n",
    "global_metrics = {\n",
    "    'Metric': ['Top-1 Accuracy', 'Top-5 Hit Rate', 'Mean Reciprocal Rank', 'Total Predictions'],\n",
    "    'Value': [\n",
    "        df_for_global['hit@1'].mean(),\n",
    "        df_for_global['hit@5'].mean(), \n",
    "        df_for_global['rr'].mean(),\n",
    "        len(df_for_global)\n",
    "    ],\n",
    "    'Percentage': [\n",
    "        round(df_for_global['hit@1'].mean() * 100, 2),\n",
    "        round(df_for_global['hit@5'].mean() * 100, 2),\n",
    "        round(df_for_global['rr'].mean() * 100, 2),\n",
    "        len(df_for_global)  # Mantieni conteggio come numero\n",
    "    ],\n",
    "    'Scope': [scope] * 4\n",
    "}\n",
    "\n",
    "global_metrics_df = pd.DataFrame(global_metrics)\n",
    "\n",
    "# Esporta\n",
    "global_csv = output_dir / 'metriche_globali.csv'\n",
    "global_metrics_df.to_csv(global_csv, index=False)\n",
    "\n",
    "print(f\"‚úÖ Esportato: {global_csv}\")\n",
    "print(f\"üåç Scope: {scope}\")\n",
    "display(global_metrics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "export_coverage_analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. ANALISI COVERAGE\n",
    "if models_data:\n",
    "    coverage_analysis = []\n",
    "    \n",
    "    for model_name, df in models_data.items():\n",
    "        # Calcola POI unici\n",
    "        all_predicted_pois = set()\n",
    "        for pred_list in df['prediction_norm']:\n",
    "            if pred_list:\n",
    "                all_predicted_pois.update(pred_list)\n",
    "        \n",
    "        ground_truth_pois = set(df['ground_truth_norm'].unique())\n",
    "        \n",
    "        # Calcola metriche di coverage\n",
    "        overlap = all_predicted_pois.intersection(ground_truth_pois)\n",
    "        coverage = len(all_predicted_pois) / len(ground_truth_pois) if len(ground_truth_pois) > 0 else 0\n",
    "        recall_coverage = len(overlap) / len(ground_truth_pois) if len(ground_truth_pois) > 0 else 0\n",
    "        precision_coverage = len(overlap) / len(all_predicted_pois) if len(all_predicted_pois) > 0 else 0\n",
    "        \n",
    "        coverage_data = {\n",
    "            'Model': model_name.replace('_', ' ').replace('-', ' ').title(),\n",
    "            'Model_Code': model_name,\n",
    "            'POI_Predicted_Unique': len(all_predicted_pois),\n",
    "            'POI_GroundTruth_Unique': len(ground_truth_pois),\n",
    "            'POI_Overlap': len(overlap),\n",
    "            'Catalogue_Coverage': round(coverage * 100, 2),\n",
    "            'Coverage_Recall': round(recall_coverage * 100, 2),\n",
    "            'Coverage_Precision': round(precision_coverage * 100, 2)\n",
    "        }\n",
    "        \n",
    "        coverage_analysis.append(coverage_data)\n",
    "    \n",
    "    coverage_df = pd.DataFrame(coverage_analysis)\n",
    "    \n",
    "    # Esporta\n",
    "    coverage_csv = output_dir / 'coverage_analysis.csv'\n",
    "    coverage_df.to_csv(coverage_csv, index=False)\n",
    "    \n",
    "    print(f\"‚úÖ Esportato: {coverage_csv}\")\n",
    "    print(f\"üìã Analisi coverage per {len(coverage_df)} modelli\")\n",
    "    display(coverage_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "export_yearly_summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. RIASSUNTO ANNUALE DETTAGLIATO\n",
    "if models_data:\n",
    "    # Usa il dataset combinato se disponibile, altrimenti il modello principale\n",
    "    df_for_yearly = combined_df if combined_df is not None else df_primary\n",
    "    \n",
    "    # Calcola statistiche dettagliate per anno\n",
    "    yearly_summary = df_for_yearly.groupby('year').agg({\n",
    "        'hit@1': ['count', 'mean', 'std'],\n",
    "        'hit@5': ['mean', 'std'],\n",
    "        'rr': ['mean', 'std'],\n",
    "        'card_id': 'nunique'  # Numero di utenti unici\n",
    "    }).round(4)\n",
    "    \n",
    "    # Flattening delle colonne\n",
    "    yearly_summary.columns = [\n",
    "        'Total_Predictions', 'Top1_Mean', 'Top1_Std',\n",
    "        'Top5_Mean', 'Top5_Std', 'MRR_Mean', 'MRR_Std', 'Unique_Users'\n",
    "    ]\n",
    "    \n",
    "    # Converti in percentuali\n",
    "    for col in ['Top1_Mean', 'Top1_Std', 'Top5_Mean', 'Top5_Std', 'MRR_Mean', 'MRR_Std']:\n",
    "        yearly_summary[f'{col}_Percent'] = (yearly_summary[col] * 100).round(2)\n",
    "    \n",
    "    # Calcola tasso di errore\n",
    "    yearly_summary['Error_Rate_Percent'] = (100 - yearly_summary['Top1_Mean_Percent']).round(2)\n",
    "    \n",
    "    # Reset index\n",
    "    yearly_summary = yearly_summary.reset_index()\n",
    "    \n",
    "    # Aggiungi informazioni aggiuntive\n",
    "    yearly_summary['Data_Source'] = 'Combined Models' if combined_df is not None else primary_model\n",
    "    \n",
    "    # Esporta\n",
    "    summary_csv = output_dir / 'yearly_summary.csv'\n",
    "    yearly_summary.to_csv(summary_csv, index=False)\n",
    "    \n",
    "    print(f\"‚úÖ Esportato: {summary_csv}\")\n",
    "    print(f\"üìÖ Riassunto per {len(yearly_summary)} anni\")\n",
    "    \n",
    "    # Mostra colonne principali\n",
    "    display(yearly_summary[['year', 'Total_Predictions', 'Top1_Mean_Percent', \n",
    "                           'Top5_Mean_Percent', 'MRR_Mean_Percent', 'Error_Rate_Percent']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary_section",
   "metadata": {},
   "source": [
    "## üìã Riepilogo File Esportati\n",
    "Verifica finale dei file CSV generati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final_summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifica file esportati\n",
    "csv_files = list(output_dir.glob('*.csv'))\n",
    "\n",
    "print(\"üìä FILE CSV ESPORTATI PER CANVA:\")\n",
    "print(\"==\" * 25)\n",
    "\n",
    "if csv_files:\n",
    "    total_size = 0\n",
    "    for csv_file in sorted(csv_files):\n",
    "        try:\n",
    "            df_check = pd.read_csv(csv_file)\n",
    "            file_size = csv_file.stat().st_size\n",
    "            total_size += file_size\n",
    "            \n",
    "            print(f\"‚úÖ {csv_file.name}\")\n",
    "            print(f\"   üìè Dimensioni: {df_check.shape[0]} righe √ó {df_check.shape[1]} colonne\")\n",
    "            print(f\"   üíæ Dimensione file: {file_size:,} bytes ({file_size/1024:.1f} KB)\")\n",
    "            print(f\"   üìÇ Path: {csv_file}\")\n",
    "            \n",
    "            # Mostra le prime colonne per verifica\n",
    "            columns_preview = ', '.join(df_check.columns[:5])\n",
    "            if len(df_check.columns) > 5:\n",
    "                columns_preview += f\", ... (+{len(df_check.columns)-5} altre)\"\n",
    "            print(f\"   üìã Colonne: {columns_preview}\")\n",
    "            print()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå {csv_file.name}: Errore nella lettura - {e}\")\n",
    "            print()\n",
    "\n",
    "    print(f\"üéØ TOTALE: {len(csv_files)} file CSV pronti per Canva\")\n",
    "    print(f\"üíæ Dimensione totale: {total_size:,} bytes ({total_size/1024:.1f} KB)\")\n",
    "    print(f\"üìÅ Directory: {output_dir.absolute()}\")\n",
    "    \n",
    "    # Verifica integrit√† dei dati\n",
    "    print(\"\\nüîç VERIFICA INTEGRIT√Ä DATI:\")\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    integrity_issues = []\n",
    "    \n",
    "    for csv_file in csv_files:\n",
    "        try:\n",
    "            df = pd.read_csv(csv_file)\n",
    "            issues = []\n",
    "            \n",
    "            # Controlla valori mancanti\n",
    "            missing_values = df.isnull().sum().sum()\n",
    "            if missing_values > 0:\n",
    "                issues.append(f\"{missing_values} valori mancanti\")\n",
    "            \n",
    "            # Controlla colonne percentuali\n",
    "            pct_cols = [col for col in df.columns if 'Percent' in col]\n",
    "            for col in pct_cols:\n",
    "                if col in df.columns:\n",
    "                    invalid_pcts = ((df[col] < 0) | (df[col] > 100)).sum()\n",
    "                    if invalid_pcts > 0:\n",
    "                        issues.append(f\"{invalid_pcts} percentuali invalide in {col}\")\n",
    "            \n",
    "            if issues:\n",
    "                integrity_issues.append(f\"{csv_file.name}: {', '.join(issues)}\")\n",
    "            else:\n",
    "                print(f\"‚úÖ {csv_file.name}: Dati integri\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            integrity_issues.append(f\"{csv_file.name}: Errore verifica - {e}\")\n",
    "    \n",
    "    if integrity_issues:\n",
    "        print(\"\\n‚ö†Ô∏è PROBLEMI RILEVATI:\")\n",
    "        for issue in integrity_issues:\n",
    "            print(f\"   {issue}\")\n",
    "    else:\n",
    "        print(\"‚úÖ Tutti i file hanno superato la verifica di integrit√†\")\n",
    "\n",
    "    # Suggerimenti per Canva\n",
    "    print(\"\\nüí° SUGGERIMENTI PER CANVA:\")\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    suggestions = {\n",
    "        'metriche_per_anno.csv': [\n",
    "            \"üìà Grafici temporali (line chart con anni sull'asse X)\",\n",
    "            \"üìä Bar chart per confronti annuali\",\n",
    "            \"üéØ Usa colonne *_Percent per percentuali gi√† formattate\"\n",
    "        ],\n",
    "        'confronto_modelli.csv': [\n",
    "            \"üìä Horizontal bar chart per ranking modelli\",\n",
    "            \"üèÜ Usa Rank_Top1 per ordinamento\",\n",
    "            \"üìã Tabelle comparative con Performance_Level\"\n",
    "        ],\n",
    "        'metriche_globali.csv': [\n",
    "            \"üéØ KPI cards per dashboard\",\n",
    "            \"üìä Gauge charts con colonna Percentage\",\n",
    "            \"üìà Metriche principali per overview\"\n",
    "        ],\n",
    "        'coverage_analysis.csv': [\n",
    "            \"üìä Grafici a barre per copertura\",\n",
    "            \"üéØ Scatter plot Precision vs Recall\",\n",
    "            \"üìã Tabelle dettagliate per appendice\"\n",
    "        ],\n",
    "        'yearly_summary.csv': [\n",
    "            \"üìä Tabelle complete con statistiche\",\n",
    "            \"üìà Include deviazioni standard\",\n",
    "            \"üîç Dati dettagliati per analisi profonde\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    for csv_file in csv_files:\n",
    "        file_name = csv_file.name\n",
    "        if file_name in suggestions:\n",
    "            print(f\"\\nüìÑ {file_name}:\")\n",
    "            for suggestion in suggestions[file_name]:\n",
    "                print(f\"   {suggestion}\")\n",
    "    \n",
    "    print(f\"\\nüé® FORMATO DATI OTTIMIZZATO:\")\n",
    "    print(\"   ‚Ä¢ Percentuali gi√† convertite (es: 4.32 invece di 0.0432)\")\n",
    "    print(\"   ‚Ä¢ Nomi modelli formattati per presentazione\")\n",
    "    print(\"   ‚Ä¢ Encoding UTF-8 compatibile con Canva\")\n",
    "    print(\"   ‚Ä¢ Struttura pulita senza indici complessi\")\n",
    "    print(\"   ‚Ä¢ Colonne separate per valori assoluti e percentuali\")\n",
    "    \n",
    "    print(f\"\\nüöÄ PRONTO PER CANVA! Importa i file e crea visualizzazioni fantastiche!\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå Nessun file CSV trovato!\")\n",
    "    print(\"\\nüîß RISOLUZIONE PROBLEMI:\")\n",
    "    print(\"   1. Verifica che almeno un modello sia stato caricato correttamente\")\n",
    "    print(\"   2. Controlla gli errori nelle sezioni di esportazione precedenti\")\n",
    "    print(\"   3. Verifica che la directory di output sia accessibile\")\n",
    "    print(\"   4. Re-esegui le celle di esportazione\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
