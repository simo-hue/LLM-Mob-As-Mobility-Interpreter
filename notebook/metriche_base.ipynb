{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3428bdf",
   "metadata": {},
   "source": [
    "# VeronaCard Next‑POI Prediction – Metrics & Exploratory Analysis\n",
    "\n",
    "*Generated automatically on 2025-06-25 13:01 UTC*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b32bbc",
   "metadata": {},
   "source": [
    "# 🎯 Obiettivi dell'Analisi\n",
    "\n",
    "Questo notebook fornisce una **valutazione completa** delle performance del modello di predizione next-POI implementato in `veronacard_mob.py`, attraverso un framework di analisi strutturato e riproducibile.\n",
    "\n",
    "## 📋 Pipeline di Analisi\n",
    "\n",
    "### 🔄 **1. Data Loading & Preprocessing**\n",
    "- **Caricamento automatico** di tutti i file CSV di predizione (`*_pred_*.csv`)\n",
    "- **Parsing robusto** con gestione errori per file malformati\n",
    "- **Estrazione automatica** dell'anno dai nomi file (es. `dati_2016_pred_model.csv` → 2016)\n",
    "- **Validazione** delle predizioni (filtro per liste con esattamente 5 elementi)\n",
    "\n",
    "### 📊 **2. Evaluation Metrics**\n",
    "Calcolo di **quattro metriche standard** per sistemi di raccomandazione:\n",
    "- **🎯 Top-1 Accuracy**: Precisione della predizione principale\n",
    "- **📈 Top-5 Hit Rate**: Presenza del POI corretto nella top-5\n",
    "- **⚡ Mean Reciprocal Rank (MRR)**: Qualità del ranking delle predizioni\n",
    "- **🗂️ Catalogue Coverage**: Diversità delle raccomandazioni\n",
    "\n",
    "### 📈 **3. Multi-level Visualization**\n",
    "- **📊 Global metrics**: Performance aggregate su tutto il dataset\n",
    "- **📅 Temporal analysis**: Trend evolutivo per anno (2016→2020)\n",
    "- **🎨 Interactive charts**: Grafici comparativi e heat-maps\n",
    "\n",
    "### 🔍 **4. Advanced Error Analysis Framework**\n",
    "Sistema modulare ed estendibile per analisi approfondita:\n",
    "- **🚨 Worst-case analysis**: Identificazione delle coppie POI più problematiche\n",
    "- **🔥 Confusion matrices**: Visualizzazione degli errori sui POI più frequenti\n",
    "- **🧠 Explainability**: Analisi dei pattern testuali che causano errori (LIME/SHAP-ready)\n",
    "- **📊 Temporal patterns**: Analisi stagionale e trend temporali degli errori\n",
    "\n",
    "## 🏗️ Caratteristiche Architetturali\n",
    "\n",
    "### **✅ Robustezza**\n",
    "- **Error handling** completo per file CSV corrotti\n",
    "- **Graceful degradation** per dipendenze mancanti (LIME, etc.)\n",
    "- **Validation** automatica dei dati di input\n",
    "\n",
    "### **🔧 Flessibilità**\n",
    "- **Path-agnostic**: Funziona con CSV nella cartella corrente o tramite `DATA_DIR`\n",
    "- **Modular design**: Sezioni indipendenti eseguibili singolarmente\n",
    "- **Extensible framework**: Facile aggiungere nuove metriche o analisi\n",
    "\n",
    "### **📊 Riproducibilità**\n",
    "- **Timestamp automatico** nel titolo\n",
    "- **Seed fisso** per operazioni random\n",
    "- **Versioning implicito** tramite nomi file con data\n",
    "\n",
    "---\n",
    "\n",
    "## 🚀 Quick Start\n",
    "\n",
    "```python\n",
    "# Esecuzione standard - CSV nella cartella 'results/'\n",
    "# Nessuna configurazione richiesta\n",
    "\n",
    "# Esecuzione personalizzata\n",
    "DATA_DIR = \"/path/to/your/csv/files\"  # opzionale\n",
    "```\n",
    "\n",
    "## 📁 Struttura File Attesa\n",
    "\n",
    "```\n",
    "project_root/\n",
    "├── notebook.ipynb           # questo notebook\n",
    "├── results/                 # directory predefinita\n",
    "│   ├── dati_2016_pred_model1.csv\n",
    "│   ├── dati_2017_pred_model1.csv\n",
    "│   └── ...\n",
    "└── veronacard_mob.py       # script di generazione predizioni\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "> **💡 Pro Tip**: Il notebook è progettato per **fail-safe operation** — anche se alcuni file sono corrotti o alcune librerie mancano, l'analisi procede con le parti disponibili e fornisce sempre risultati utili."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07dfa4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import ast, json, re, csv\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.rcParams['figure.dpi'] = 120\n",
    "plt.rcParams['axes.grid'] = True\n",
    "\n",
    "# Configuration - easily switch between different result directories\n",
    "#DATA_DIR = \"../results_mixtral_8x7b_with_geom/\" \n",
    "#DATA_DIR = \"../results/results_mixtral_8x7b_with_geom/\" \n",
    "DATA_DIR = \"../results/qwen2.5_14b//with_geom/\" \n",
    "#DATA_DIR = \"../results/results_mixtral_8x7b_base_version\"\n",
    "#DATA_DIR = \"../results_parziali_sequenziali_ministral/\" \n",
    "#DATA_DIR = \"../result_run_completa_with_geom_srv_univr/\" #llama 3.1:8b\n",
    "#DATA_DIR = \"../results\"\n",
    "\n",
    "print(f\"📁 Loading data from: {DATA_DIR}\")\n",
    "print(f\"🛡️ Robust CSV parsing enabled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a744cbc",
   "metadata": {},
   "source": [
    "## 1. 🛡️ Robust Data Loading\n",
    "\n",
    "Caricamento robusto dei dati con gestione automatica di:\n",
    "- **Escaped quotes**: `\"\"Basilica di Sant'Anastasia\"\"` → `'Basilica di Sant\\'Anastasia'`\n",
    "- **CSV malformati**: Parsing con multiple strategie di fallback\n",
    "- **Predizioni incomplete**: Filtro automatico per dati validi\n",
    "- **Error tracking**: Monitoraggio della qualità dei dati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac05b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def robust_parse_prediction(x, debug=False):\n",
    "    \"\"\"\n",
    "    Ultra-robust prediction parsing with multiple fallback strategies\n",
    "    \n",
    "    Handles the specific issue in CSV files where POI names contain escaped quotes\n",
    "    like: \"\"Basilica di Sant'Anastasia\"\"\n",
    "    \"\"\"\n",
    "    if not isinstance(x, str) or not x.strip():\n",
    "        return []\n",
    "    \n",
    "    x = x.strip()\n",
    "    \n",
    "    # Strategy 1: Try ast.literal_eval first (fastest for well-formed data)\n",
    "    try:\n",
    "        result = ast.literal_eval(x)\n",
    "        if isinstance(result, list):\n",
    "            return [str(item) for item in result]\n",
    "        else:\n",
    "            return [str(result)]\n",
    "    except (ValueError, SyntaxError):\n",
    "        pass\n",
    "    \n",
    "    # Strategy 2: Handle escaped quotes (main issue in CSV files)\n",
    "    try:\n",
    "        # Replace double escaped quotes with single quotes\n",
    "        # \"\"Basilica di Sant'Anastasia\"\" -> 'Basilica di Sant\\'Anastasia'\n",
    "        fixed = re.sub(r'\"\"([^\"]*?)\"\"', r\"'\\1'\", x)\n",
    "        \n",
    "        result = ast.literal_eval(fixed)\n",
    "        if isinstance(result, list):\n",
    "            return [str(item) for item in result]\n",
    "        else:\n",
    "            return [str(result)]\n",
    "    except (ValueError, SyntaxError):\n",
    "        pass\n",
    "    \n",
    "    # Strategy 3: Manual parsing with regex\n",
    "    try:\n",
    "        if x.startswith('[') and x.endswith(']'):\n",
    "            # Extract content between various quote types\n",
    "            patterns = [\n",
    "                r\"'([^']*?)'(?:,|\\\\])\",  # Single quotes\n",
    "                r'\"([^\"]*?)\"(?:,|\\\\])',   # Double quotes  \n",
    "                r'\"\"([^\"]*?)\"\"(?:,|\\\\])'  # Escaped double quotes\n",
    "            ]\n",
    "            \n",
    "            result = []\n",
    "            for pattern in patterns:\n",
    "                matches = re.findall(pattern, x + ',')\n",
    "                result.extend(matches)\n",
    "            \n",
    "            # Remove duplicates while preserving order\n",
    "            seen = set()\n",
    "            unique_result = []\n",
    "            for item in result:\n",
    "                if item not in seen:\n",
    "                    seen.add(item)\n",
    "                    unique_result.append(item)\n",
    "            \n",
    "            if unique_result:\n",
    "                return unique_result\n",
    "    except Exception:\n",
    "        pass\n",
    "    \n",
    "    # Strategy 4: JSON-like parsing\n",
    "    try:\n",
    "        json_like = x.replace(\"'\", '\"')  # Convert single quotes to double\n",
    "        result = json.loads(json_like)\n",
    "        if isinstance(result, list):\n",
    "            return [str(item) for item in result]\n",
    "        else:\n",
    "            return [str(result)]\n",
    "    except (json.JSONDecodeError, ValueError):\n",
    "        pass\n",
    "    \n",
    "    # Strategy 5: Simple comma splitting (last resort)\n",
    "    try:\n",
    "        if ',' in x:\n",
    "            clean = x.strip('[]').split(',')\n",
    "            result = []\n",
    "            for item in clean:\n",
    "                item = item.strip().strip('\"\\'')\n",
    "                if item:\n",
    "                    result.append(item)\n",
    "            if result:\n",
    "                return result\n",
    "    except Exception:\n",
    "        pass\n",
    "    \n",
    "    # Final fallback: empty list with optional warning\n",
    "    if debug:\n",
    "        print(f\"⚠️  Could not parse: '{x[:50]}{'...' if len(x) > 50 else ''}'\")\n",
    "    \n",
    "    return []\n",
    "\n",
    "\n",
    "def robust_csv_reader(file_path, debug=False):\n",
    "    \"\"\"\n",
    "    Robust CSV reader with multiple parsing strategies.\n",
    "    Handles various CSV formats including those with additional columns\n",
    "    like processing_time, status, etc.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Try standard pandas CSV reader first\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Check if we have the minimum required columns\n",
    "        required_columns = ['card_id', 'prediction', 'ground_truth']\n",
    "        if all(col in df.columns for col in required_columns):\n",
    "            if debug:\n",
    "                print(f\"✅ Standard CSV read: {len(df)} rows, columns: {list(df.columns)}\")\n",
    "            return df\n",
    "        else:\n",
    "            missing_cols = [col for col in required_columns if col not in df.columns]\n",
    "            if debug:\n",
    "                print(f\"⚠️  Missing required columns: {missing_cols}\")\n",
    "            \n",
    "    except pd.errors.ParserError as e:\n",
    "        if debug:\n",
    "            print(f\"⚠️  Standard read failed: {e}\")\n",
    "    \n",
    "    # Fallback: try with different parsing options\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, on_bad_lines='skip', engine='python')\n",
    "        \n",
    "        # Check for required columns again\n",
    "        required_columns = ['card_id', 'prediction', 'ground_truth']\n",
    "        if all(col in df.columns for col in required_columns):\n",
    "            if debug:\n",
    "                print(f\"✅ Fallback CSV read: {len(df)} rows, columns: {list(df.columns)}\")\n",
    "            return df\n",
    "        else:\n",
    "            if debug:\n",
    "                print(f\"⚠️  Fallback read missing columns\")\n",
    "                \n",
    "    except Exception as e2:\n",
    "        if debug:\n",
    "            print(f\"❌ All CSV reads failed: {e2}\")\n",
    "    \n",
    "    # Final fallback: return empty DataFrame\n",
    "    return pd.DataFrame()\n",
    "\n",
    "\n",
    "# Find and load all CSV files\n",
    "csv_files = [Path(p) for p in glob.glob(os.path.join(DATA_DIR, '*_pred_*.csv'))]\n",
    "csv_files = sorted(csv_files)\n",
    "\n",
    "if not csv_files:\n",
    "    raise FileNotFoundError(f\"No CSV files matching *_pred_*.csv found in {DATA_DIR}\")\n",
    "\n",
    "print(f\"📂 Found {len(csv_files)} CSV files to process\")\n",
    "\n",
    "# Process files with robust parsing and error tracking\n",
    "dfs = []\n",
    "parse_errors = 0\n",
    "total_rows_processed = 0\n",
    "skipped_rows = 0\n",
    "files_with_issues = []\n",
    "\n",
    "for i, fp in enumerate(csv_files, 1):\n",
    "    print(f\"\\\\n📄 Processing file {i}/{len(csv_files)}: {fp.name}\")\n",
    "    \n",
    "    # Load CSV with robust reader\n",
    "    df = robust_csv_reader(fp, debug=True)\n",
    "    \n",
    "    if df.empty:\n",
    "        print(f\"   ⚠️  Skipped empty/corrupted file\")\n",
    "        files_with_issues.append(fp.name)\n",
    "        continue\n",
    "    \n",
    "    # Handle missing columns gracefully\n",
    "    if 'ground_truth' not in df.columns:\n",
    "        print(f\"   ⚠️  No ground_truth column found, skipping file\")\n",
    "        files_with_issues.append(fp.name)\n",
    "        continue\n",
    "        \n",
    "    if 'prediction' not in df.columns:\n",
    "        print(f\"   ⚠️  No prediction column found, skipping file\")\n",
    "        files_with_issues.append(fp.name)\n",
    "        continue\n",
    "    \n",
    "    # Handle optional columns with defaults\n",
    "    if 'hit' not in df.columns:\n",
    "        df['hit'] = False  # Default to False if not present\n",
    "        print(f\"   📝 Added default 'hit' column\")\n",
    "    \n",
    "    if 'cluster' not in df.columns:\n",
    "        df['cluster'] = 0  # Default cluster\n",
    "        print(f\"   📝 Added default 'cluster' column\")\n",
    "    \n",
    "    if 'history' not in df.columns:\n",
    "        df['history'] = \"[]\"  # Empty history\n",
    "        print(f\"   📝 Added default 'history' column\")\n",
    "    \n",
    "    if 'current_poi' not in df.columns:\n",
    "        df['current_poi'] = \"\"  # Empty current POI\n",
    "        print(f\"   📝 Added default 'current_poi' column\")\n",
    "    \n",
    "    # Extract year from filename (e.g., dati_2016_pred_...)\n",
    "    year_token = next((part for part in fp.stem.split('_')\n",
    "                       if part.isdigit() and len(part) == 4), None)\n",
    "    df['year'] = int(year_token) if year_token else np.nan\n",
    "    \n",
    "    # Filter out rows with empty ground_truth\n",
    "    original_count = len(df)\n",
    "    df = df[df['ground_truth'].notna() & (df['ground_truth'].astype(str).str.strip() != \"\")].copy()\n",
    "    empty_gt_rows = original_count - len(df)\n",
    "    \n",
    "    if empty_gt_rows > 0:\n",
    "        print(f\"   📉 Filtered out {empty_gt_rows} rows with empty ground_truth\")\n",
    "        skipped_rows += empty_gt_rows\n",
    "    \n",
    "    if df.empty:\n",
    "        print(f\"   ⚠️  No valid rows remaining after filtering\")\n",
    "        files_with_issues.append(fp.name)\n",
    "        continue\n",
    "    \n",
    "    # Robust prediction parsing\n",
    "    print(f\"   🔧 Parsing {len(df)} prediction entries...\")\n",
    "    \n",
    "    df['prediction_list'] = df['prediction'].apply(robust_parse_prediction)\n",
    "    \n",
    "    # Count parsing issues\n",
    "    empty_predictions = (df['prediction_list'].apply(len) == 0).sum()\n",
    "    if empty_predictions > 0:\n",
    "        parse_errors += empty_predictions\n",
    "        print(f\"   ⚠️  {empty_predictions} entries could not be parsed\")\n",
    "    \n",
    "    # Filter for valid predictions (exactly 5 elements)\n",
    "    df_valid = df[df['prediction_list'].apply(len) == 5].copy()\n",
    "    rows_kept = len(df_valid)\n",
    "    rows_dropped = len(df) - rows_kept\n",
    "    \n",
    "    if rows_dropped > 0:\n",
    "        skipped_rows += rows_dropped\n",
    "        print(f\"   📉 Filtered out {rows_dropped} rows (wrong prediction count)\")\n",
    "    \n",
    "    print(f\"   ✅ Kept {rows_kept} valid rows\")\n",
    "    \n",
    "    if not df_valid.empty:\n",
    "        dfs.append(df_valid)\n",
    "        total_rows_processed += rows_kept\n",
    "\n",
    "# Combine all DataFrames\n",
    "if dfs:\n",
    "    df_all = pd.concat(dfs, ignore_index=True)\n",
    "    print(f\"\\\\n🎉 Successfully processed {len(csv_files)} files:\")\n",
    "    print(f\"   📊 Total valid rows: {total_rows_processed:,}\")\n",
    "    print(f\"   ⚠️  Parse errors: {parse_errors:,}\")\n",
    "    print(f\"   📉 Skipped rows: {skipped_rows:,}\")\n",
    "    \n",
    "    if files_with_issues:\n",
    "        print(f\"   ⚠️  Files with issues: {len(files_with_issues)}\")\n",
    "        for fname in files_with_issues[:5]:  # Show first 5\n",
    "            print(f\"      • {fname}\")\n",
    "        if len(files_with_issues) > 5:\n",
    "            print(f\"      • ... and {len(files_with_issues) - 5} more\")\n",
    "    \n",
    "    success_rate = (total_rows_processed/(total_rows_processed+skipped_rows+parse_errors))*100\n",
    "    print(f\"   ✅ Success rate: {success_rate:.1f}%\")\n",
    "    \n",
    "    # Data quality summary\n",
    "    print(f\"\\\\n📈 Data Quality Summary:\")\n",
    "    years_available = sorted(df_all['year'].dropna().unique().astype(int).tolist())\n",
    "    print(f\"   • Years covered: {years_available}\")\n",
    "    print(f\"   • Unique POI in ground truth: {df_all['ground_truth'].nunique()}\")\n",
    "    print(f\"   • Average predictions per POI: {len(df_all) / df_all['ground_truth'].nunique():.1f}\")\n",
    "    \n",
    "    # Check for additional columns\n",
    "    extra_cols = [col for col in df_all.columns if col not in \n",
    "                  ['card_id', 'cluster', 'history', 'current_poi', 'prediction', \n",
    "                   'ground_truth', 'reason', 'hit', 'year', 'prediction_list']]\n",
    "    if extra_cols:\n",
    "        print(f\"   • Additional columns found: {extra_cols}\")\n",
    "    \n",
    "    # Display sample data\n",
    "    print(f\"\\\\n📋 Sample data:\")\n",
    "    display_cols = ['card_id', 'year', 'current_poi', 'ground_truth', 'prediction_list', 'hit']\n",
    "    if 'processing_time' in df_all.columns:\n",
    "        display_cols.append('processing_time')\n",
    "    if 'status' in df_all.columns:\n",
    "        display_cols.append('status')\n",
    "    \n",
    "    display(df_all[display_cols].head())\n",
    "else:\n",
    "    raise ValueError(\"No valid data could be loaded from any CSV file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8882065",
   "metadata": {},
   "source": [
    "# 2. Definizione delle Metriche di Valutazione\n",
    "\n",
    "Per valutare le performance del modello di predizione next-POI, utilizziamo quattro metriche standard nel campo dei sistemi di raccomandazione e della predizione sequenziale.\n",
    "\n",
    "## Notazione\n",
    "\n",
    "Sia $y_i$ il vero next-POI (ground truth) per la sequenza $i$-esima, e sia $\\hat{y}_i^{(1)}, \\hat{y}_i^{(2)}, \\ldots, \\hat{y}_i^{(k)}$ la lista **ordinata** delle $k$ raccomandazioni prodotte dal modello per quella sequenza, dove $\\hat{y}_i^{(1)}$ è la predizione con confidence più alta.\n",
    "\n",
    "---\n",
    "\n",
    "## 📊 Top-1 Accuracy\n",
    "\n",
    "Misura la **precisione della predizione principale** del modello.\n",
    "\n",
    "$$\\text{Acc}_{@1} = \\frac{1}{N}\\sum_{i=1}^{N}\\mathbf{1}\\!\\left\\{y_i = \\hat{y}_i^{(1)}\\right\\}$$\n",
    "\n",
    "**Interpretazione**: Percentuale di casi in cui la prima predizione del modello coincide esattamente con il POI realmente visitato.\n",
    "\n",
    "**Range**: [0, 1], dove 1 = predizione perfetta\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 Top-k Hit Rate\n",
    "\n",
    "Misura se il POI corretto è presente **tra le prime $k$ predizioni**.\n",
    "\n",
    "$$\\text{HR}_{@k} = \\frac{1}{N}\\sum_{i=1}^{N}\\mathbf{1}\\!\\left\\{y_i \\in \\{\\hat{y}_i^{(1)}, \\hat{y}_i^{(2)}, \\ldots, \\hat{y}_i^{(k)}\\}\\right\\}$$\n",
    "\n",
    "**Interpretazione**: Percentuale di casi in cui il POI corretto è presente nella lista delle prime $k$ raccomandazioni. Per $k=5$: \"Il modello include il POI corretto nella sua top-5?\"\n",
    "\n",
    "**Range**: [0, 1], sempre $\\text{HR}_{@k} \\geq \\text{Acc}_{@1}$\n",
    "\n",
    "---\n",
    "\n",
    "## ⚡ Mean Reciprocal Rank (MRR)\n",
    "\n",
    "Considera sia la **presenza** del POI corretto nella lista che la sua **posizione**.\n",
    "\n",
    "$$\\text{MRR} = \\frac{1}{N}\\sum_{i=1}^{N}\\frac{1}{\\text{rank}_i}$$\n",
    "\n",
    "dove $\\text{rank}_i = \\min\\{r \\mid y_i = \\hat{y}_i^{(r)}\\}$ è la posizione del POI corretto nella lista ordinata.\n",
    "\n",
    "**Interpretazione**: \n",
    "- Se il POI corretto è al 1° posto → contributo = 1.0\n",
    "- Se il POI corretto è al 2° posto → contributo = 0.5  \n",
    "- Se il POI corretto è al 5° posto → contributo = 0.2\n",
    "- Se il POI corretto non è nella top-k → contributo = 0.0\n",
    "\n",
    "**Range**: [0, 1], dove valori più alti indicano che i POI corretti appaiono nelle prime posizioni\n",
    "\n",
    "---\n",
    "\n",
    "## 🗂️ Catalogue Coverage  \n",
    "\n",
    "Misura la **diversità** delle raccomandazioni prodotte dal modello.\n",
    "\n",
    "$$\\text{Coverage} = \\frac{\\left|\\bigcup_{i=1}^{N}\\{\\hat{y}_i^{(1)}, \\hat{y}_i^{(2)}, \\ldots, \\hat{y}_i^{(k)}\\}\\right|}{|\\mathcal{P}|}$$\n",
    "\n",
    "dove $\\mathcal{P}$ è l'insieme completo dei POI presenti nel dataset (ground truth).\n",
    "\n",
    "**Interpretazione**: Frazione dei POI disponibili che il modello è in grado di raccomandare. \n",
    "- Coverage = 1.0: il modello raccomanda tutti i POI del catalogo\n",
    "- Coverage = 0.1: il modello raccomanda solo il 10% dei POI disponibili\n",
    "\n",
    "**Importanza**: Previene il bias verso POI molto popolari e garantisce diversità nelle raccomandazioni.\n",
    "\n",
    "---\n",
    "\n",
    "## 💡 Note Metodologiche\n",
    "\n",
    "- **$N$**: Numero totale di predizioni nel dataset di test\n",
    "- **$k = 5$**: Utilizziamo consistently una finestra di 5 raccomandazioni\n",
    "- **Ordinamento**: Le predizioni sono ordinate per confidence/probabilità decrescente\n",
    "- **Handling missing**: Se $y_i \\notin \\{\\hat{y}_i^{(1)}, \\ldots, \\hat{y}_i^{(k)}\\}$, contributo = 0 per tutte le metriche\n",
    "\n",
    "Queste metriche forniscono una **valutazione completa** del modello: precision (Acc@1), recall (HR@k), ranking quality (MRR), e diversity (Coverage)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04234505",
   "metadata": {},
   "source": [
    "### 2.1 Helper functions & per‑row computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc35347",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def poi_id(x):\n",
    "    \"\"\"\n",
    "    Converte 'x' in un identificatore hashable comparabile con ground-truth.\n",
    "    • Se x è un dict          → prova a usare 'poi', 'poi_id', 'name', 'id'\n",
    "    • Se x è list/tuple       → ritorna tupla ricorsiva\n",
    "    • Altrimenti (str/int)    → cast a str\n",
    "    \"\"\"\n",
    "    if isinstance(x, dict):\n",
    "        for key in ('poi', 'poi_id', 'name', 'id'):\n",
    "            if key in x:\n",
    "                return str(x[key])\n",
    "        # fallback: serializza in JSON ordinato\n",
    "        return json.dumps(x, sort_keys=True)\n",
    "    elif isinstance(x, (list, tuple)):\n",
    "        return tuple(map(poi_id, x))\n",
    "    else:\n",
    "        return str(x)\n",
    "\n",
    "# Normalizza prediction_list e ground_truth\n",
    "df_all['prediction_norm']    = df_all['prediction_list'].apply(lambda lst: [poi_id(e) for e in lst])\n",
    "df_all['ground_truth_norm']  = df_all['ground_truth'].apply(poi_id)\n",
    "\n",
    "# -- metriche element-wise --------------------------------------------\n",
    "df_all['hit@1'] = df_all['prediction_norm'].str[0] == df_all['ground_truth_norm']\n",
    "\n",
    "def top_k_hit(row, k=5):\n",
    "    return row['ground_truth_norm'] in row['prediction_norm'][:k]\n",
    "\n",
    "def reciprocal_rank(row, k=5):\n",
    "    try:\n",
    "        rank = row['prediction_norm'][:k].index(row['ground_truth_norm']) + 1\n",
    "        return 1.0 / rank\n",
    "    except ValueError:\n",
    "        return 0.0\n",
    "\n",
    "df_all['hit@5'] = df_all.apply(top_k_hit, axis=1)\n",
    "df_all['rr']    = df_all.apply(reciprocal_rank, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4b31a1",
   "metadata": {},
   "source": [
    "### 2.2 Global metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87936f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_global = {\n",
    "    \"Top-1 Accuracy\": df_all[\"hit@1\"].mean(),\n",
    "    \"Top-5 Hit Rate\": df_all[\"hit@5\"].mean(),\n",
    "    \"MRR\": df_all[\"rr\"].mean(),\n",
    "}\n",
    "\n",
    "# Catalogue Coverage\n",
    "coverage_set = {poi for preds in df_all[\"prediction_norm\"] for poi in preds}\n",
    "metrics_global[\"Catalogue Coverage\"] = (\n",
    "    len(coverage_set) / df_all[\"ground_truth_norm\"].nunique()\n",
    ")\n",
    "\n",
    "# Visualizza in percentuale\n",
    "metrics_df = (\n",
    "    pd.DataFrame(metrics_global, index=[\"Value\"])\n",
    "      .T\n",
    "      .style.format(\"{:.2%}\")\n",
    ")\n",
    "display(metrics_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1f7e3c",
   "metadata": {},
   "source": [
    "### 2.3 Metrics by year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b12f0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "by_year = (\n",
    "    df_all\n",
    "    .groupby('year')\n",
    "    .agg(\n",
    "        top1=('hit@1', 'mean'),\n",
    "        hit5=('hit@5', 'mean'),\n",
    "        mrr=('rr', 'mean'),\n",
    "        n=('card_id', 'size')\n",
    "    )\n",
    "    .reset_index()\n",
    "    .sort_values('year')\n",
    ")\n",
    "display(by_year)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ac7adc",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3f4d2744",
   "metadata": {},
   "source": [
    "## 3. Visualisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cbabe05",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(figsize=(6,4))\n",
    "ax.bar(by_year['year'].astype(str), by_year['top1']*100)\n",
    "ax.set_ylabel('Top‑1 Accuracy (%)')\n",
    "ax.set_title('Top‑1 Accuracy (%) by Year')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58242dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(figsize=(6,4))\n",
    "ax.bar(by_year['year'].astype(str), by_year['hit5']*100)\n",
    "ax.set_ylabel('Top‑5 Hit Rate (%)')\n",
    "ax.set_title('Top‑5 Hit Rate (%) by Year')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12cf7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(figsize=(6,4))\n",
    "ax.bar(by_year['year'].astype(str), by_year['mrr']*100)\n",
    "ax.set_ylabel('MRR (%)')\n",
    "ax.set_title('MRR (%) by Year')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38d992e",
   "metadata": {},
   "source": [
    "### 3.1 Coverage breakdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dab3c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_pred_poi = (\n",
    "    pd.Series([poi for preds in df_all[\"prediction_norm\"] for poi in preds])\n",
    "      .nunique()\n",
    ")\n",
    "unique_gt_poi = df_all[\"ground_truth_norm\"].nunique()\n",
    "\n",
    "print(f\"POI distinct in predictions: {unique_pred_poi}\")\n",
    "print(f\"POI distinct in ground-truth: {unique_gt_poi}\")\n",
    "print(f\"Catalogue Coverage: {unique_pred_poi/unique_gt_poi:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2aee9a",
   "metadata": {},
   "source": [
    "## 4. Error analysis – overview\n",
    "\n",
    "In questa sezione valutiamo **dove** e **perché** il modello sbaglia, seguendo tre filoni:\n",
    "\n",
    "1. **Worst-performing POI pairs**  \n",
    "   Scopriamo quali coppie `ground-truth → first_pred` generano il maggior numero di errori.\n",
    "\n",
    "2. **Confusion matrix per subset**  \n",
    "   Heat-map delle frequenze (o tasso d’errore) su un sotto-insieme di POI di interesse\n",
    "   (ad es. i 20 più visitati o solo l’anno 2020).\n",
    "\n",
    "3. **Explainability (SHAP / LIME)**  \n",
    "   Analisi dei fattori testuali nella colonna `history` che portano a una predizione\n",
    "   sbagliata. Qui mostriamo un prototipo basato su LIME; lo script è modulare\n",
    "   e può essere sostituito da SHAP se usi modelli compatibili.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f8e9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------\n",
    "# 4.1 Worst-performing POI pairs (Top-N) – FIX\n",
    "# ---------------------------------------------------------------------\n",
    "# filtro righe con errore Top-1\n",
    "ERR = df_all.assign(first_pred=df_all[\"prediction_norm\"].str[0])\n",
    "ERR = ERR[~ERR[\"hit@1\"]]           # equivalente a query(\"`hit@1` == False\")\n",
    "\n",
    "# conteggia coppie ground-truth → first_pred\n",
    "pairs = (\n",
    "    ERR.groupby([\"ground_truth_norm\", \"first_pred\"])\n",
    "        .size()\n",
    "        .reset_index(name=\"errors\")\n",
    "        .sort_values(\"errors\", ascending=False)\n",
    "        .head(15)          # Top-15 coppie\n",
    ")\n",
    "\n",
    "display(pairs)\n",
    "\n",
    "# barplot (opzionale)\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "ax.barh(\n",
    "    pairs.apply(lambda r: f\"{r['ground_truth_norm']} → {r['first_pred']}\", axis=1),\n",
    "    pairs[\"errors\"]\n",
    ")\n",
    "ax.set_xlabel(\"Errori (count)\")\n",
    "ax.set_title(\"Worst-performing POI pairs – Top 15\")\n",
    "ax.invert_yaxis()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a501fa2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------\n",
    "# 4.2 Confusion matrix – seleziona subset\n",
    "# ---------------------------------------------------------------------\n",
    "TOP_K = 20          # limitiamoci ai 20 POI più frequenti (ground-truth)\n",
    "YEAR  = None        # imposta un anno (es. 2020) oppure None per all\n",
    "\n",
    "subset = df_all.copy()\n",
    "if YEAR is not None:\n",
    "    subset = subset.query(\"year == @YEAR\")\n",
    "\n",
    "# filtra i POI più frequenti\n",
    "top_poi = (\n",
    "    subset[\"ground_truth_norm\"]\n",
    "           .value_counts()\n",
    "           .head(TOP_K)\n",
    "           .index\n",
    ")\n",
    "mask = subset[\"ground_truth_norm\"].isin(top_poi) & subset[\"prediction_norm\"].str[0].isin(top_poi)\n",
    "cm_df = pd.crosstab(\n",
    "    subset.loc[mask, \"ground_truth_norm\"],\n",
    "    subset.loc[mask, \"prediction_norm\"].str[0],\n",
    "    rownames=[\"True\"],\n",
    "    colnames=[\"Pred\"],\n",
    "    dropna=False\n",
    ")\n",
    "\n",
    "# Normalizza su righe per avere tassi d’errore\n",
    "cm_norm = cm_df.div(cm_df.sum(axis=1), axis=0)\n",
    "\n",
    "# Heat-map (matplotlib only)\n",
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "im = ax.imshow(cm_norm.values, aspect=\"auto\")\n",
    "ax.set_xticks(range(len(cm_norm.columns)))\n",
    "ax.set_xticklabels(cm_norm.columns, rotation=90)\n",
    "ax.set_yticks(range(len(cm_norm.index)))\n",
    "ax.set_yticklabels(cm_norm.index)\n",
    "ax.set_xlabel(\"Predicted POI\")\n",
    "ax.set_ylabel(\"True POI\")\n",
    "ax.set_title(f\"Confusion matrix – top {TOP_K} POI\" + (f\", year {YEAR}\" if YEAR else \"\"))\n",
    "fig.colorbar(im, ax=ax, fraction=0.02, pad=0.04, label=\"Error rate\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0439af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------\n",
    "# 4.3 Explainability \n",
    "# ---------------------------------------------------------------------\n",
    "import warnings, json, numpy as np, pandas as pd\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ERROR ANALYSIS - Simplified Version (No LIME required)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Analisi dei pattern testuali negli errori\n",
    "# ---------------------------------------------------------------------\n",
    "err_rows = df_all[~df_all[\"hit@1\"]].copy()\n",
    "correct_rows = df_all[df_all[\"hit@1\"]].copy()\n",
    "\n",
    "if err_rows.empty:\n",
    "    print(\"🎉 Nessun errore Top-1 da analizzare!\")\n",
    "else:\n",
    "    print(f\"📊 Analyzing {len(err_rows):,} prediction errors vs {len(correct_rows):,} correct predictions\")\n",
    "    \n",
    "    # Aggiungi colonne per l'analisi\n",
    "    err_rows['first_pred'] = err_rows[\"prediction_norm\"].str[0]\n",
    "    correct_rows['first_pred'] = correct_rows[\"prediction_norm\"].str[0]\n",
    "    \n",
    "    # ---------------------------------------------------------------------\n",
    "    # 1. Analisi lunghezza history\n",
    "    # ---------------------------------------------------------------------\n",
    "    print(\"\\n1️⃣ HISTORY LENGTH ANALYSIS\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    err_rows['history_length'] = err_rows['history'].astype(str).str.len()\n",
    "    correct_rows['history_length'] = correct_rows['history'].astype(str).str.len()\n",
    "    \n",
    "    print(f\"Average history length in ERRORS: {err_rows['history_length'].mean():.0f} chars\")\n",
    "    print(f\"Average history length in CORRECT: {correct_rows['history_length'].mean():.0f} chars\")\n",
    "    \n",
    "    # ---------------------------------------------------------------------\n",
    "    # 2. Analisi parole chiave più frequenti negli errori\n",
    "    # ---------------------------------------------------------------------\n",
    "    print(\"\\n2️⃣ KEYWORD ANALYSIS IN ERROR CASES\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    def extract_keywords(text, min_length=3):\n",
    "        \"\"\"Estrae parole significative dal testo\"\"\"\n",
    "        if pd.isna(text):\n",
    "            return []\n",
    "        # Rimuovi caratteri speciali e converti in lowercase\n",
    "        words = re.findall(r'\\b[a-zA-ZÀ-ÿ]+\\b', str(text).lower())\n",
    "        # Filtra parole troppo corte e comuni\n",
    "        stop_words = {'and', 'the', 'poi', 'via', 'del', 'dei', 'per', 'con', 'una', 'uno', 'che', 'nel', 'sul'}\n",
    "        return [w for w in words if len(w) >= min_length and w not in stop_words]\n",
    "    \n",
    "    # Estrai keywords dalle history degli errori\n",
    "    error_keywords = []\n",
    "    for history in err_rows['history'].head(1000):  # Limita per performance\n",
    "        error_keywords.extend(extract_keywords(history))\n",
    "    \n",
    "    # Estrai keywords dalle history corrette\n",
    "    correct_keywords = []\n",
    "    for history in correct_rows['history'].head(1000):  # Limita per performance\n",
    "        correct_keywords.extend(extract_keywords(history))\n",
    "    \n",
    "    error_word_freq = Counter(error_keywords).most_common(15)\n",
    "    correct_word_freq = Counter(correct_keywords).most_common(15)\n",
    "    \n",
    "    print(\"Most frequent words in ERROR cases:\")\n",
    "    for word, freq in error_word_freq:\n",
    "        print(f\"  {word:>15s}: {freq:>4d}\")\n",
    "    \n",
    "    print(\"\\nMost frequent words in CORRECT cases:\")\n",
    "    for word, freq in correct_word_freq:\n",
    "        print(f\"  {word:>15s}: {freq:>4d}\")\n",
    "    \n",
    "    # ---------------------------------------------------------------------\n",
    "    # 3. Analisi dei POI più problematici\n",
    "    # ---------------------------------------------------------------------\n",
    "    print(\"\\n3️⃣ MOST PROBLEMATIC POI (Ground Truth)\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # POI con più errori\n",
    "    error_by_true_poi = (\n",
    "        err_rows['ground_truth_norm']\n",
    "        .value_counts()\n",
    "        .head(10)\n",
    "        .reset_index()\n",
    "    )\n",
    "    error_by_true_poi.columns = ['POI', 'Error_Count']\n",
    "    \n",
    "    # Calcola anche il tasso di errore\n",
    "    total_by_poi = df_all['ground_truth_norm'].value_counts()\n",
    "    error_by_true_poi['Total_Count'] = error_by_true_poi['POI'].map(total_by_poi)\n",
    "    error_by_true_poi['Error_Rate'] = error_by_true_poi['Error_Count'] / error_by_true_poi['Total_Count']\n",
    "    \n",
    "    print(\"POI with highest error counts:\")\n",
    "    for _, row in error_by_true_poi.iterrows():\n",
    "        print(f\"  {row['POI']:>25s}: {row['Error_Count']:>3d} errors ({row['Error_Rate']:>5.1%} rate)\")\n",
    "    \n",
    "    # ---------------------------------------------------------------------\n",
    "    # 4. Analisi delle predizioni più frequenti negli errori\n",
    "    # ---------------------------------------------------------------------\n",
    "    print(\"\\n4️⃣ MOST FREQUENT WRONG PREDICTIONS\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    wrong_pred_freq = err_rows['first_pred'].value_counts().head(10)\n",
    "    print(\"Most frequently predicted (wrong) POI:\")\n",
    "    for poi, freq in wrong_pred_freq.items():\n",
    "        print(f\"  {poi:>25s}: {freq:>3d} times\")\n",
    "    \n",
    "    # ---------------------------------------------------------------------\n",
    "    # 5. Esempio dettagliato di errore\n",
    "    # ---------------------------------------------------------------------\n",
    "    print(\"\\n5️⃣ DETAILED ERROR EXAMPLE\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Prendi il primo errore disponibile\n",
    "    sample_idx = err_rows.index[0]\n",
    "    sample = df_all.loc[sample_idx]\n",
    "    \n",
    "    print(f\"Card ID: {sample.get('card_id', 'N/A')}\")\n",
    "    print(f\"Year: {sample.get('year', 'N/A')}\")\n",
    "    print(f\"True POI: {sample['ground_truth_norm']}\")\n",
    "    print(f\"Predicted POI: {sample['prediction_norm'][0]}\")\n",
    "    print(f\"Top-5 Predictions: {sample['prediction_norm']}\")\n",
    "    print(f\"History (first 200 chars): {str(sample['history'])[:200]}...\")\n",
    "    \n",
    "    # ---------------------------------------------------------------------\n",
    "    # 6. Matrice di confusione semplificata (top POI)\n",
    "    # ---------------------------------------------------------------------\n",
    "    print(\"\\n6️⃣ CONFUSION MATRIX - TOP 10 POI\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Prendi i 10 POI più frequenti\n",
    "    top_poi = df_all['ground_truth_norm'].value_counts().head(10).index.tolist()\n",
    "    \n",
    "    # Filtra solo le predizioni che coinvolgono questi POI\n",
    "    confusion_data = df_all[\n",
    "        (df_all['ground_truth_norm'].isin(top_poi)) & \n",
    "        (df_all['prediction_norm'].str[0].isin(top_poi))\n",
    "    ].copy()\n",
    "    \n",
    "    confusion_matrix = pd.crosstab(\n",
    "        confusion_data['ground_truth_norm'],\n",
    "        confusion_data['prediction_norm'].str[0],\n",
    "        normalize='index'  # Normalizza per righe (% di errore per POI)\n",
    "    )\n",
    "    \n",
    "    print(\"Confusion matrix (% of predictions for each true POI):\")\n",
    "    print(\"Rows = True POI, Columns = Predicted POI\")\n",
    "    print(confusion_matrix.round(3))\n",
    "    \n",
    "    # ---------------------------------------------------------------------\n",
    "    # 7. Summary statistiche\n",
    "    # ---------------------------------------------------------------------\n",
    "    print(f\"\\n7️⃣ SUMMARY STATISTICS\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"Total predictions analyzed: {len(df_all):,}\")\n",
    "    print(f\"Total errors (Top-1): {len(err_rows):,} ({len(err_rows)/len(df_all):.2%})\")\n",
    "    print(f\"Unique POI in dataset: {df_all['ground_truth_norm'].nunique():,}\")\n",
    "    print(f\"Unique POI in predictions: {len({poi for preds in df_all['prediction_norm'] for poi in preds}):,}\")\n",
    "    \n",
    "    if len(csv_files) > 1:\n",
    "        error_by_year = df_all.groupby('year')['hit@1'].agg(['count', 'mean']).reset_index()\n",
    "        error_by_year['error_rate'] = 1 - error_by_year['mean']\n",
    "        print(f\"\\nError rate by year:\")\n",
    "        for _, row in error_by_year.iterrows():\n",
    "            if not pd.isna(row['year']):\n",
    "                print(f\"  {int(row['year'])}: {row['error_rate']:.2%} ({int(row['count'])} predictions)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ANALYSIS COMPLETE\")\n",
    "print(\"This analysis doesn't require LIME but provides valuable insights into prediction errors.\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
