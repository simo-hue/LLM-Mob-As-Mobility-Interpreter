{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üîç Comprehensive Model & Strategy Comparison Analysis\n",
    "\n",
    "This notebook provides a comprehensive analysis comparing:\n",
    "- **Inter-model performance**: Different LLMs (qwen2.5_14b, deepseek-coder_33b, llama3.1_8b, etc.)\n",
    "- **Strategy effectiveness**: Different prompt strategies (base_version, with_geom, with_geom_time)\n",
    "\n",
    "## üìÅ Data Structure\n",
    "- `results/middle/`: Contains prediction results from all models\n",
    "- `results/penultimate/`: Additional results dataset\n",
    "\n",
    "Each model has three strategy subdirectories:\n",
    "1. **base_version**: Basic tourist mobility prediction\n",
    "2. **with_geom**: Enhanced with geospatial features\n",
    "3. **with_geom_time**: Enhanced with both geospatial and temporal features\n",
    "\n",
    "## üìä Outputs\n",
    "- Interactive visualizations for model and strategy comparison\n",
    "- Canva-ready CSV exports for presentation graphics\n",
    "- Statistical analysis and performance insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import glob\n",
    "import ast\n",
    "import os\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = [12, 8]\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "# Create output directories\n",
    "output_dir = Path('../csv_for_canva_comprehensive')\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "plots_dir = Path('../plots_comprehensive')\n",
    "plots_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"‚úÖ Setup complete\")\n",
    "print(f\"üìÅ CSV output directory: {output_dir.absolute()}\")\n",
    "print(f\"üìä Plots directory: {plots_dir.absolute()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Data Loading and Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_parse_prediction(x, debug_mode=False):\n",
    "    \"\"\"\n",
    "    Safely parse predictions from string to list\n",
    "    Handles various formats including dicts and strings\n",
    "    \"\"\"\n",
    "    if pd.isna(x) or x == \"[]\" or x == \"\" or not isinstance(x, str):\n",
    "        return []\n",
    "    \n",
    "    try:\n",
    "        parsed = ast.literal_eval(x)\n",
    "        \n",
    "        if isinstance(parsed, list):\n",
    "            # Extract strings from list (handles both str and dict)\n",
    "            result = []\n",
    "            for item in parsed:\n",
    "                if isinstance(item, str):\n",
    "                    result.append(item.strip())\n",
    "                elif isinstance(item, dict):\n",
    "                    # Try to extract a key field\n",
    "                    if 'name' in item:\n",
    "                        result.append(str(item['name']).strip())\n",
    "                    elif 'poi' in item:\n",
    "                        result.append(str(item['poi']).strip())\n",
    "                    else:\n",
    "                        # Take first dict value\n",
    "                        values = list(item.values())\n",
    "                        if values:\n",
    "                            result.append(str(values[0]).strip())\n",
    "                else:\n",
    "                    result.append(str(item).strip())\n",
    "            return result\n",
    "        else:\n",
    "            return []\n",
    "    except Exception as e:\n",
    "        if debug_mode:\n",
    "            print(f\"Parse error: {x} -> {e}\")\n",
    "        return []\n",
    "\n",
    "def normalize_poi_name(poi_name):\n",
    "    \"\"\"\n",
    "    Normalize POI name for consistent comparisons\n",
    "    \"\"\"\n",
    "    if pd.isna(poi_name) or not isinstance(poi_name, str):\n",
    "        return str(poi_name).strip()\n",
    "    return poi_name.strip()\n",
    "\n",
    "def calculate_metrics(df):\n",
    "    \"\"\"\n",
    "    Calculate performance metrics for a dataset\n",
    "    \"\"\"\n",
    "    def safe_top1_accuracy(row):\n",
    "        pred_norm = row['prediction_norm']\n",
    "        if not pred_norm or len(pred_norm) == 0:\n",
    "            return False\n",
    "        return pred_norm[0] == row['ground_truth_norm']\n",
    "    \n",
    "    def safe_top_k_hit(row, k=5):\n",
    "        pred_norm = row['prediction_norm']\n",
    "        if not pred_norm or len(pred_norm) == 0:\n",
    "            return False\n",
    "        return row['ground_truth_norm'] in pred_norm[:k]\n",
    "    \n",
    "    def safe_reciprocal_rank(row, k=5):\n",
    "        pred_norm = row['prediction_norm']\n",
    "        if not pred_norm or len(pred_norm) == 0:\n",
    "            return 0.0\n",
    "        try:\n",
    "            rank = pred_norm[:k].index(row['ground_truth_norm']) + 1\n",
    "            return 1.0 / rank\n",
    "        except ValueError:\n",
    "            return 0.0\n",
    "    \n",
    "    # Calculate metrics\n",
    "    df['hit@1'] = df.apply(safe_top1_accuracy, axis=1)\n",
    "    df['hit@5'] = df.apply(safe_top_k_hit, axis=1)\n",
    "    df['rr'] = df.apply(safe_reciprocal_rank, axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "print(\"‚úÖ Data processing functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_strategy_data(results_base_path='../results'):\n",
    "    \"\"\"\n",
    "    Load data for all models and strategies from results directory\n",
    "    Returns nested dictionary: {dataset_type: {model: {strategy: dataframe}}}\n",
    "    \"\"\"\n",
    "    results_path = Path(results_base_path)\n",
    "    \n",
    "    # Available dataset types (excluding DEV)\n",
    "    dataset_types = ['middle', 'penultimate']\n",
    "    \n",
    "    # Strategy types\n",
    "    strategies = ['base_version', 'with_geom', 'with_geom_time']\n",
    "    \n",
    "    all_data = {}\n",
    "    \n",
    "    for dataset_type in dataset_types:\n",
    "        dataset_path = results_path / dataset_type\n",
    "        if not dataset_path.exists():\n",
    "            print(f\"‚ö†Ô∏è Dataset {dataset_type} not found, skipping\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"\\nüîÑ Loading {dataset_type} dataset...\")\n",
    "        all_data[dataset_type] = {}\n",
    "        \n",
    "        # Find all model directories\n",
    "        model_dirs = [d for d in dataset_path.iterdir() if d.is_dir()]\n",
    "        \n",
    "        for model_dir in sorted(model_dirs):\n",
    "            model_name = model_dir.name\n",
    "            print(f\"  üìÇ Processing model: {model_name}\")\n",
    "            all_data[dataset_type][model_name] = {}\n",
    "            \n",
    "            for strategy in strategies:\n",
    "                strategy_path = model_dir / strategy\n",
    "                if not strategy_path.exists():\n",
    "                    print(f\"    ‚ö†Ô∏è Strategy {strategy} not found for {model_name}\")\n",
    "                    continue\n",
    "                    \n",
    "                # Find CSV files in strategy directory\n",
    "                csv_files = list(strategy_path.glob('*_pred_*.csv'))\n",
    "                \n",
    "                if not csv_files:\n",
    "                    print(f\"    ‚ö†Ô∏è No prediction CSV files found in {strategy}\")\n",
    "                    continue\n",
    "                    \n",
    "                print(f\"    üìä Loading {strategy}: {len(csv_files)} files\")\n",
    "                \n",
    "                # Load and concatenate all CSV files for this strategy\n",
    "                strategy_dfs = []\n",
    "                total_rows = 0\n",
    "                \n",
    "                for csv_file in sorted(csv_files):\n",
    "                    try:\n",
    "                        df = pd.read_csv(csv_file)\n",
    "                        \n",
    "                        # Extract year from filename\n",
    "                        year_parts = [part for part in csv_file.stem.split('_') \n",
    "                                    if part.isdigit() and len(part) == 4]\n",
    "                        if year_parts:\n",
    "                            df['year'] = int(year_parts[0])\n",
    "                        else:\n",
    "                            print(f\"      ‚ö†Ô∏è No year found in {csv_file.name}\")\n",
    "                            continue\n",
    "                        \n",
    "                        # Process predictions\n",
    "                        df['prediction_list'] = df['prediction'].apply(safe_parse_prediction)\n",
    "                        df['prediction_norm'] = df['prediction_list'].apply(\n",
    "                            lambda x: [normalize_poi_name(poi) for poi in x] if isinstance(x, list) else []\n",
    "                        )\n",
    "                        df['ground_truth_norm'] = df['ground_truth'].apply(normalize_poi_name)\n",
    "                        \n",
    "                        # Filter valid predictions\n",
    "                        df = df[df['prediction_list'].apply(lambda x: isinstance(x, list) and len(x) > 0)]\n",
    "                        \n",
    "                        if len(df) > 0:\n",
    "                            strategy_dfs.append(df)\n",
    "                            total_rows += len(df)\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        print(f\"      ‚ùå Error loading {csv_file.name}: {e}\")\n",
    "                        continue\n",
    "                \n",
    "                if strategy_dfs:\n",
    "                    # Concatenate all dataframes for this strategy\n",
    "                    strategy_df = pd.concat(strategy_dfs, ignore_index=True)\n",
    "                    \n",
    "                    # Add metadata\n",
    "                    strategy_df['model'] = model_name\n",
    "                    strategy_df['strategy'] = strategy\n",
    "                    strategy_df['dataset'] = dataset_type\n",
    "                    \n",
    "                    # Calculate metrics\n",
    "                    strategy_df = calculate_metrics(strategy_df)\n",
    "                    \n",
    "                    all_data[dataset_type][model_name][strategy] = strategy_df\n",
    "                    print(f\"      ‚úÖ {strategy}: {len(strategy_df):,} valid predictions\")\n",
    "                else:\n",
    "                    print(f\"      ‚ùå {strategy}: No valid data loaded\")\n",
    "    \n",
    "    return all_data\n",
    "\n",
    "# Load all data\n",
    "print(\"üöÄ Starting comprehensive data loading...\")\n",
    "all_model_data = load_model_strategy_data()\n",
    "\n",
    "print(\"\\nüìä LOADING SUMMARY:\")\n",
    "print(\"=\" * 40)\n",
    "for dataset_type, models in all_model_data.items():\n",
    "    print(f\"üìÅ {dataset_type.upper()}: {len(models)} models\")\n",
    "    for model_name, strategies in models.items():\n",
    "        print(f\"  ü§ñ {model_name}: {len(strategies)} strategies\")\n",
    "        for strategy_name, df in strategies.items():\n",
    "            print(f\"    üìä {strategy_name}: {len(df):,} predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Comprehensive Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_comprehensive_comparison_df(all_data):\n",
    "    \"\"\"\n",
    "    Create a comprehensive comparison DataFrame with all model-strategy combinations\n",
    "    \"\"\"\n",
    "    comparison_data = []\n",
    "    \n",
    "    for dataset_type, models in all_data.items():\n",
    "        for model_name, strategies in models.items():\n",
    "            for strategy_name, df in strategies.items():\n",
    "                if len(df) == 0:\n",
    "                    continue\n",
    "                \n",
    "                # Calculate aggregate metrics\n",
    "                metrics = {\n",
    "                    'Dataset': dataset_type,\n",
    "                    'Model': model_name,\n",
    "                    'Strategy': strategy_name,\n",
    "                    'Model_Display': model_name.replace('_', ' ').replace('-', ' ').title(),\n",
    "                    'Strategy_Display': strategy_name.replace('_', ' ').title(),\n",
    "                    'Combination': f\"{model_name}_{strategy_name}\",\n",
    "                    'Total_Predictions': len(df),\n",
    "                    'Top1_Accuracy': df['hit@1'].mean(),\n",
    "                    'Top5_Hit_Rate': df['hit@5'].mean(),\n",
    "                    'MRR': df['rr'].mean(),\n",
    "                    'Years_Covered': len(df['year'].unique()),\n",
    "                    'Year_Range': f\"{df['year'].min()}-{df['year'].max()}\",\n",
    "                    'Unique_Users': df['card_id'].nunique() if 'card_id' in df.columns else len(df),\n",
    "                }\n",
    "                \n",
    "                # Convert to percentages\n",
    "                metrics['Top1_Accuracy_Percent'] = round(metrics['Top1_Accuracy'] * 100, 2)\n",
    "                metrics['Top5_Hit_Rate_Percent'] = round(metrics['Top5_Hit_Rate'] * 100, 2)\n",
    "                metrics['MRR_Percent'] = round(metrics['MRR'] * 100, 2)\n",
    "                metrics['Error_Rate_Percent'] = round(100 - metrics['Top1_Accuracy_Percent'], 2)\n",
    "                \n",
    "                # Calculate standard deviations for confidence intervals\n",
    "                metrics['Top1_Std'] = df['hit@1'].std()\n",
    "                metrics['Top5_Std'] = df['hit@5'].std()\n",
    "                metrics['MRR_Std'] = df['rr'].std()\n",
    "                \n",
    "                # Strategy encoding for analysis\n",
    "                strategy_map = {\n",
    "                    'base_version': 'Base',\n",
    "                    'with_geom': 'Geospatial',\n",
    "                    'with_geom_time': 'Geo+Temporal'\n",
    "                }\n",
    "                metrics['Strategy_Category'] = strategy_map.get(strategy_name, strategy_name)\n",
    "                \n",
    "                # Model size estimation (for analysis)\n",
    "                if '7b' in model_name.lower():\n",
    "                    metrics['Model_Size'] = '7B'\n",
    "                elif '8b' in model_name.lower():\n",
    "                    metrics['Model_Size'] = '8B'\n",
    "                elif '14b' in model_name.lower():\n",
    "                    metrics['Model_Size'] = '14B'\n",
    "                elif '33b' in model_name.lower():\n",
    "                    metrics['Model_Size'] = '33B'\n",
    "                elif '8x7b' in model_name.lower():\n",
    "                    metrics['Model_Size'] = '8x7B'\n",
    "                else:\n",
    "                    metrics['Model_Size'] = 'Unknown'\n",
    "                \n",
    "                comparison_data.append(metrics)\n",
    "    \n",
    "    return pd.DataFrame(comparison_data)\n",
    "\n",
    "# Create comprehensive comparison DataFrame\n",
    "comparison_df = create_comprehensive_comparison_df(all_model_data)\n",
    "\n",
    "print(f\"üìä Created comprehensive comparison with {len(comparison_df)} model-strategy combinations\")\n",
    "print(f\"ü§ñ Models: {comparison_df['Model'].nunique()}\")\n",
    "print(f\"üéØ Strategies: {comparison_df['Strategy'].nunique()}\")\n",
    "print(f\"üìÅ Datasets: {comparison_df['Dataset'].nunique()}\")\n",
    "\n",
    "# Display top performers\n",
    "print(\"\\nüèÜ TOP 10 PERFORMERS (by Top-1 Accuracy):\")\n",
    "top_performers = comparison_df.nlargest(10, 'Top1_Accuracy_Percent')\n",
    "display(top_performers[['Model_Display', 'Strategy_Display', 'Dataset', 'Top1_Accuracy_Percent', 'Top5_Hit_Rate_Percent', 'Total_Predictions']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìà Visualization Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Model Performance Comparison (grouped by strategy)\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Filter to middle dataset for cleaner visualization\n",
    "middle_df = comparison_df[comparison_df['Dataset'] == 'middle']\n",
    "\n",
    "if len(middle_df) > 0:\n",
    "    # Create pivot table for heatmap\n",
    "    pivot_data = middle_df.pivot_table(\n",
    "        index='Model_Display', \n",
    "        columns='Strategy_Category', \n",
    "        values='Top1_Accuracy_Percent',\n",
    "        fill_value=0\n",
    "    )\n",
    "    \n",
    "    # Create heatmap\n",
    "    plt.subplot(2, 2, 1)\n",
    "    sns.heatmap(pivot_data, annot=True, cmap='RdYlGn', fmt='.2f', \n",
    "                cbar_kws={'label': 'Top-1 Accuracy (%)'}, square=True)\n",
    "    plt.title('Model Performance by Strategy\\n(Top-1 Accuracy %)', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Strategy', fontweight='bold')\n",
    "    plt.ylabel('Model', fontweight='bold')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    # Bar plot comparing strategies\n",
    "    plt.subplot(2, 2, 2)\n",
    "    strategy_means = middle_df.groupby('Strategy_Category')['Top1_Accuracy_Percent'].mean().sort_values(ascending=False)\n",
    "    bars = plt.bar(strategy_means.index, strategy_means.values, \n",
    "                   color=['#ff7f0e', '#2ca02c', '#d62728'][:len(strategy_means)])\n",
    "    plt.title('Average Strategy Performance\\n(Top-1 Accuracy)', fontsize=14, fontweight='bold')\n",
    "    plt.ylabel('Top-1 Accuracy (%)', fontweight='bold')\n",
    "    plt.xlabel('Strategy', fontweight='bold')\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, value in zip(bars, strategy_means.values):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1, \n",
    "                f'{value:.2f}%', ha='center', fontweight='bold')\n",
    "    \n",
    "    # Model comparison (best strategy per model)\n",
    "    plt.subplot(2, 2, 3)\n",
    "    best_per_model = middle_df.loc[middle_df.groupby('Model')['Top1_Accuracy_Percent'].idxmax()]\n",
    "    best_per_model = best_per_model.sort_values('Top1_Accuracy_Percent', ascending=True)\n",
    "    \n",
    "    bars = plt.barh(best_per_model['Model_Display'], best_per_model['Top1_Accuracy_Percent'])\n",
    "    plt.title('Best Model Performance\\n(Best Strategy per Model)', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Top-1 Accuracy (%)', fontweight='bold')\n",
    "    \n",
    "    # Color bars by strategy\n",
    "    strategy_colors = {'Base': '#ff7f0e', 'Geospatial': '#2ca02c', 'Geo+Temporal': '#d62728'}\n",
    "    for i, (_, row) in enumerate(best_per_model.iterrows()):\n",
    "        bars[i].set_color(strategy_colors.get(row['Strategy_Category'], '#1f77b4'))\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, (_, row) in enumerate(best_per_model.iterrows()):\n",
    "        plt.text(row['Top1_Accuracy_Percent'] + 0.1, i, \n",
    "                f\"{row['Top1_Accuracy_Percent']:.2f}%\\n({row['Strategy_Category']})\", \n",
    "                va='center', fontsize=9)\n",
    "    \n",
    "    # Strategy improvement analysis\n",
    "    plt.subplot(2, 2, 4)\n",
    "    \n",
    "    # Calculate improvement from base to advanced strategies\n",
    "    improvement_data = []\n",
    "    for model in middle_df['Model'].unique():\n",
    "        model_data = middle_df[middle_df['Model'] == model]\n",
    "        base_acc = model_data[model_data['Strategy'] == 'base_version']['Top1_Accuracy_Percent']\n",
    "        geom_acc = model_data[model_data['Strategy'] == 'with_geom']['Top1_Accuracy_Percent']\n",
    "        geom_time_acc = model_data[model_data['Strategy'] == 'with_geom_time']['Top1_Accuracy_Percent']\n",
    "        \n",
    "        if len(base_acc) > 0:\n",
    "            base_val = base_acc.iloc[0]\n",
    "            if len(geom_acc) > 0:\n",
    "                improvement_data.append({\n",
    "                    'Model': model.replace('_', ' ').title(),\n",
    "                    'Strategy': 'Geospatial vs Base',\n",
    "                    'Improvement': geom_acc.iloc[0] - base_val\n",
    "                })\n",
    "            if len(geom_time_acc) > 0:\n",
    "                improvement_data.append({\n",
    "                    'Model': model.replace('_', ' ').title(),\n",
    "                    'Strategy': 'Geo+Temporal vs Base',\n",
    "                    'Improvement': geom_time_acc.iloc[0] - base_val\n",
    "                })\n",
    "    \n",
    "    if improvement_data:\n",
    "        imp_df = pd.DataFrame(improvement_data)\n",
    "        \n",
    "        # Create grouped bar chart\n",
    "        models = imp_df['Model'].unique()\n",
    "        x = np.arange(len(models))\n",
    "        width = 0.35\n",
    "        \n",
    "        geom_improvements = []\n",
    "        geom_time_improvements = []\n",
    "        \n",
    "        for model in models:\n",
    "            geom_imp = imp_df[(imp_df['Model'] == model) & (imp_df['Strategy'] == 'Geospatial vs Base')]['Improvement']\n",
    "            geom_time_imp = imp_df[(imp_df['Model'] == model) & (imp_df['Strategy'] == 'Geo+Temporal vs Base')]['Improvement']\n",
    "            \n",
    "            geom_improvements.append(geom_imp.iloc[0] if len(geom_imp) > 0 else 0)\n",
    "            geom_time_improvements.append(geom_time_imp.iloc[0] if len(geom_time_imp) > 0 else 0)\n",
    "        \n",
    "        plt.bar(x - width/2, geom_improvements, width, label='Geospatial vs Base', color='#2ca02c', alpha=0.8)\n",
    "        plt.bar(x + width/2, geom_time_improvements, width, label='Geo+Temporal vs Base', color='#d62728', alpha=0.8)\n",
    "        \n",
    "        plt.axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "        plt.title('Strategy Improvement Analysis\\n(Percentage Points vs Base)', fontsize=14, fontweight='bold')\n",
    "        plt.xlabel('Model', fontweight='bold')\n",
    "        plt.ylabel('Accuracy Improvement (pp)', fontweight='bold')\n",
    "        plt.xticks(x, [m.replace(' ', '\\n') for m in models], rotation=0, fontsize=9)\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save plot\n",
    "    plot_path = plots_dir / 'comprehensive_model_strategy_comparison.png'\n",
    "    plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"üìä Saved comprehensive comparison plot: {plot_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No middle dataset found for visualization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Detailed Performance Distribution Analysis\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "if len(middle_df) > 0:\n",
    "    # Box plot: Performance distribution by strategy\n",
    "    sns.boxplot(data=middle_df, x='Strategy_Category', y='Top1_Accuracy_Percent', ax=axes[0,0])\n",
    "    axes[0,0].set_title('Performance Distribution by Strategy', fontsize=14, fontweight='bold')\n",
    "    axes[0,0].set_ylabel('Top-1 Accuracy (%)', fontweight='bold')\n",
    "    axes[0,0].set_xlabel('Strategy', fontweight='bold')\n",
    "    \n",
    "    # Violin plot: Performance distribution by model size\n",
    "    sns.violinplot(data=middle_df, x='Model_Size', y='Top1_Accuracy_Percent', ax=axes[0,1])\n",
    "    axes[0,1].set_title('Performance Distribution by Model Size', fontsize=14, fontweight='bold')\n",
    "    axes[0,1].set_ylabel('Top-1 Accuracy (%)', fontweight='bold')\n",
    "    axes[0,1].set_xlabel('Model Size', fontweight='bold')\n",
    "    \n",
    "    # Scatter plot: Model Size vs Performance\n",
    "    # Convert model size to numeric for scatter plot\n",
    "    size_mapping = {'7B': 7, '8B': 8, '14B': 14, '33B': 33, '8x7B': 56, 'Unknown': 0}\n",
    "    middle_df_scatter = middle_df.copy()\n",
    "    middle_df_scatter['Model_Size_Numeric'] = middle_df_scatter['Model_Size'].map(size_mapping)\n",
    "    middle_df_scatter = middle_df_scatter[middle_df_scatter['Model_Size_Numeric'] > 0]\n",
    "    \n",
    "    scatter = axes[1,0].scatter(middle_df_scatter['Model_Size_Numeric'], \n",
    "                              middle_df_scatter['Top1_Accuracy_Percent'],\n",
    "                              c=middle_df_scatter['Strategy_Category'].astype('category').cat.codes,\n",
    "                              s=middle_df_scatter['Total_Predictions'] / 1000,  # Size by number of predictions\n",
    "                              alpha=0.7, cmap='Set1')\n",
    "    \n",
    "    axes[1,0].set_xlabel('Model Size (Billions of Parameters)', fontweight='bold')\n",
    "    axes[1,0].set_ylabel('Top-1 Accuracy (%)', fontweight='bold')\n",
    "    axes[1,0].set_title('Model Size vs Performance\\n(Bubble size = # Predictions)', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # Add trend line\n",
    "    z = np.polyfit(middle_df_scatter['Model_Size_Numeric'], middle_df_scatter['Top1_Accuracy_Percent'], 1)\n",
    "    p = np.poly1d(z)\n",
    "    axes[1,0].plot(middle_df_scatter['Model_Size_Numeric'], p(middle_df_scatter['Model_Size_Numeric']), \n",
    "                   \"r--\", alpha=0.8, linewidth=2)\n",
    "    \n",
    "    # Performance vs Data Volume\n",
    "    axes[1,1].scatter(middle_df['Total_Predictions'], middle_df['Top1_Accuracy_Percent'], \n",
    "                     c=middle_df['Strategy_Category'].astype('category').cat.codes,\n",
    "                     alpha=0.7, s=60, cmap='Set1')\n",
    "    axes[1,1].set_xlabel('Total Predictions', fontweight='bold')\n",
    "    axes[1,1].set_ylabel('Top-1 Accuracy (%)', fontweight='bold')\n",
    "    axes[1,1].set_title('Performance vs Dataset Size', fontsize=14, fontweight='bold')\n",
    "    axes[1,1].set_xscale('log')\n",
    "    \n",
    "    # Add strategy legend\n",
    "    strategy_categories = middle_df['Strategy_Category'].unique()\n",
    "    colors = plt.cm.Set1(np.linspace(0, 1, len(strategy_categories)))\n",
    "    legend_elements = [plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=colors[i], \n",
    "                                 markersize=10, label=cat) for i, cat in enumerate(strategy_categories)]\n",
    "    axes[1,1].legend(handles=legend_elements, title='Strategy', loc='upper right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save plot\n",
    "    plot_path = plots_dir / 'detailed_performance_analysis.png'\n",
    "    plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"üìä Saved detailed performance analysis: {plot_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No data available for detailed analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì§ Export Canva-Ready CSV Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export comprehensive comparison data\n",
    "print(\"üîÑ Generating Canva-ready CSV files...\\n\")\n",
    "\n",
    "# 1. Model-Strategy Comparison (Main comparison table)\n",
    "model_strategy_csv = comparison_df[[\n",
    "    'Dataset', 'Model_Display', 'Strategy_Display', 'Strategy_Category',\n",
    "    'Top1_Accuracy_Percent', 'Top5_Hit_Rate_Percent', 'MRR_Percent', \n",
    "    'Error_Rate_Percent', 'Total_Predictions', 'Years_Covered',\n",
    "    'Model_Size', 'Year_Range'\n",
    "]].copy()\n",
    "\n",
    "# Add rankings\n",
    "model_strategy_csv['Rank_Overall'] = model_strategy_csv['Top1_Accuracy_Percent'].rank(ascending=False, method='min')\n",
    "model_strategy_csv['Rank_Within_Strategy'] = model_strategy_csv.groupby('Strategy_Category')['Top1_Accuracy_Percent'].rank(ascending=False, method='min')\n",
    "model_strategy_csv['Rank_Within_Model'] = model_strategy_csv.groupby('Model_Display')['Top1_Accuracy_Percent'].rank(ascending=False, method='min')\n",
    "\n",
    "# Performance categories\n",
    "model_strategy_csv['Performance_Tier'] = pd.cut(\n",
    "    model_strategy_csv['Top1_Accuracy_Percent'],\n",
    "    bins=[0, 2, 4, 6, 8, 100],\n",
    "    labels=['Basic', 'Fair', 'Good', 'Very Good', 'Excellent']\n",
    ")\n",
    "\n",
    "output_file = output_dir / 'comprehensive_model_strategy_comparison.csv'\n",
    "model_strategy_csv.to_csv(output_file, index=False)\n",
    "print(f\"‚úÖ Exported comprehensive comparison: {output_file}\")\n",
    "print(f\"   üìä {len(model_strategy_csv)} model-strategy combinations\")\n",
    "\n",
    "# 2. Strategy Effectiveness Summary\n",
    "strategy_summary = comparison_df.groupby(['Strategy_Category', 'Dataset']).agg({\n",
    "    'Top1_Accuracy_Percent': ['mean', 'std', 'count'],\n",
    "    'Top5_Hit_Rate_Percent': ['mean', 'std'],\n",
    "    'MRR_Percent': ['mean', 'std'],\n",
    "    'Total_Predictions': 'sum'\n",
    "}).round(2)\n",
    "\n",
    "# Flatten column names\n",
    "strategy_summary.columns = [\n",
    "    'Top1_Mean', 'Top1_Std', 'Models_Count', \n",
    "    'Top5_Mean', 'Top5_Std',\n",
    "    'MRR_Mean', 'MRR_Std', \n",
    "    'Total_Predictions'\n",
    "]\n",
    "\n",
    "strategy_summary = strategy_summary.reset_index()\n",
    "strategy_summary['Error_Rate_Mean'] = 100 - strategy_summary['Top1_Mean']\n",
    "\n",
    "output_file = output_dir / 'strategy_effectiveness_summary.csv'\n",
    "strategy_summary.to_csv(output_file, index=False)\n",
    "print(f\"‚úÖ Exported strategy summary: {output_file}\")\n",
    "print(f\"   üìä {len(strategy_summary)} strategy-dataset combinations\")\n",
    "\n",
    "# 3. Model Performance Summary  \n",
    "model_summary = comparison_df.groupby(['Model_Display', 'Model_Size', 'Dataset']).agg({\n",
    "    'Top1_Accuracy_Percent': ['mean', 'std', 'max'],\n",
    "    'Top5_Hit_Rate_Percent': ['mean', 'max'],\n",
    "    'MRR_Percent': ['mean', 'max'],\n",
    "    'Strategy_Category': lambda x: ', '.join(x.unique()),  # Available strategies\n",
    "    'Total_Predictions': 'sum'\n",
    "}).round(2)\n",
    "\n",
    "# Flatten column names\n",
    "model_summary.columns = [\n",
    "    'Top1_Mean', 'Top1_Std', 'Top1_Best',\n",
    "    'Top5_Mean', 'Top5_Best',\n",
    "    'MRR_Mean', 'MRR_Best',\n",
    "    'Available_Strategies', 'Total_Predictions'\n",
    "]\n",
    "\n",
    "model_summary = model_summary.reset_index()\n",
    "model_summary['Strategy_Count'] = model_summary['Available_Strategies'].str.count(',') + 1\n",
    "\n",
    "# Add best strategy for each model\n",
    "best_strategies = comparison_df.loc[comparison_df.groupby(['Model_Display', 'Dataset'])['Top1_Accuracy_Percent'].idxmax()]\n",
    "best_strategy_map = dict(zip(best_strategies['Model_Display'] + '_' + best_strategies['Dataset'], \n",
    "                            best_strategies['Strategy_Category']))\n",
    "model_summary['Best_Strategy'] = (model_summary['Model_Display'] + '_' + model_summary['Dataset']).map(best_strategy_map)\n",
    "\n",
    "output_file = output_dir / 'model_performance_summary.csv'\n",
    "model_summary.to_csv(output_file, index=False)\n",
    "print(f\"‚úÖ Exported model summary: {output_file}\")\n",
    "print(f\"   üìä {len(model_summary)} model-dataset combinations\")\n",
    "\n",
    "# 4. Strategy Improvement Analysis\n",
    "improvement_analysis = []\n",
    "\n",
    "for dataset in comparison_df['Dataset'].unique():\n",
    "    dataset_df = comparison_df[comparison_df['Dataset'] == dataset]\n",
    "    \n",
    "    for model in dataset_df['Model'].unique():\n",
    "        model_data = dataset_df[dataset_df['Model'] == model]\n",
    "        \n",
    "        base_row = model_data[model_data['Strategy'] == 'base_version']\n",
    "        geom_row = model_data[model_data['Strategy'] == 'with_geom']\n",
    "        geom_time_row = model_data[model_data['Strategy'] == 'with_geom_time']\n",
    "        \n",
    "        if len(base_row) > 0:\n",
    "            base_acc = base_row['Top1_Accuracy_Percent'].iloc[0]\n",
    "            model_display = base_row['Model_Display'].iloc[0]\n",
    "            model_size = base_row['Model_Size'].iloc[0]\n",
    "            \n",
    "            # Geospatial improvement\n",
    "            if len(geom_row) > 0:\n",
    "                geom_acc = geom_row['Top1_Accuracy_Percent'].iloc[0]\n",
    "                improvement_analysis.append({\n",
    "                    'Dataset': dataset,\n",
    "                    'Model': model_display,\n",
    "                    'Model_Size': model_size,\n",
    "                    'Comparison': 'Geospatial vs Base',\n",
    "                    'Base_Accuracy': base_acc,\n",
    "                    'Enhanced_Accuracy': geom_acc,\n",
    "                    'Improvement_Points': round(geom_acc - base_acc, 2),\n",
    "                    'Improvement_Percent': round(((geom_acc - base_acc) / base_acc) * 100, 2)\n",
    "                })\n",
    "            \n",
    "            # Temporal improvement\n",
    "            if len(geom_time_row) > 0:\n",
    "                geom_time_acc = geom_time_row['Top1_Accuracy_Percent'].iloc[0]\n",
    "                improvement_analysis.append({\n",
    "                    'Dataset': dataset,\n",
    "                    'Model': model_display,\n",
    "                    'Model_Size': model_size,\n",
    "                    'Comparison': 'Geo+Temporal vs Base',\n",
    "                    'Base_Accuracy': base_acc,\n",
    "                    'Enhanced_Accuracy': geom_time_acc,\n",
    "                    'Improvement_Points': round(geom_time_acc - base_acc, 2),\n",
    "                    'Improvement_Percent': round(((geom_time_acc - base_acc) / base_acc) * 100, 2)\n",
    "                })\n",
    "            \n",
    "            # Geom+Time vs Geom\n",
    "            if len(geom_row) > 0 and len(geom_time_row) > 0:\n",
    "                geom_acc = geom_row['Top1_Accuracy_Percent'].iloc[0]\n",
    "                geom_time_acc = geom_time_row['Top1_Accuracy_Percent'].iloc[0]\n",
    "                improvement_analysis.append({\n",
    "                    'Dataset': dataset,\n",
    "                    'Model': model_display,\n",
    "                    'Model_Size': model_size,\n",
    "                    'Comparison': 'Geo+Temporal vs Geospatial',\n",
    "                    'Base_Accuracy': geom_acc,\n",
    "                    'Enhanced_Accuracy': geom_time_acc,\n",
    "                    'Improvement_Points': round(geom_time_acc - geom_acc, 2),\n",
    "                    'Improvement_Percent': round(((geom_time_acc - geom_acc) / geom_acc) * 100, 2)\n",
    "                })\n",
    "\n",
    "improvement_df = pd.DataFrame(improvement_analysis)\n",
    "\n",
    "if len(improvement_df) > 0:\n",
    "    # Add improvement categories\n",
    "    improvement_df['Improvement_Category'] = pd.cut(\n",
    "        improvement_df['Improvement_Points'],\n",
    "        bins=[-100, -1, -0.1, 0.1, 1, 100],\n",
    "        labels=['Large Decrease', 'Small Decrease', 'No Change', 'Small Increase', 'Large Increase']\n",
    "    )\n",
    "    \n",
    "    output_file = output_dir / 'strategy_improvement_analysis.csv'\n",
    "    improvement_df.to_csv(output_file, index=False)\n",
    "    print(f\"‚úÖ Exported improvement analysis: {output_file}\")\n",
    "    print(f\"   üìä {len(improvement_df)} strategy comparisons\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No improvement analysis data available\")\n",
    "\n",
    "# 5. Top Performers Summary (for presentation highlights)\n",
    "top_performers_summary = []\n",
    "\n",
    "# Overall top performer\n",
    "overall_best = comparison_df.loc[comparison_df['Top1_Accuracy_Percent'].idxmax()]\n",
    "top_performers_summary.append({\n",
    "    'Category': 'Overall Best',\n",
    "    'Model': overall_best['Model_Display'],\n",
    "    'Strategy': overall_best['Strategy_Display'],\n",
    "    'Dataset': overall_best['Dataset'],\n",
    "    'Top1_Accuracy_Percent': overall_best['Top1_Accuracy_Percent'],\n",
    "    'Total_Predictions': overall_best['Total_Predictions']\n",
    "})\n",
    "\n",
    "# Best per strategy\n",
    "for strategy in comparison_df['Strategy_Category'].unique():\n",
    "    strategy_best = comparison_df[comparison_df['Strategy_Category'] == strategy].loc[\n",
    "        comparison_df[comparison_df['Strategy_Category'] == strategy]['Top1_Accuracy_Percent'].idxmax()\n",
    "    ]\n",
    "    \n",
    "    top_performers_summary.append({\n",
    "        'Category': f'Best {strategy}',\n",
    "        'Model': strategy_best['Model_Display'],\n",
    "        'Strategy': strategy_best['Strategy_Display'],\n",
    "        'Dataset': strategy_best['Dataset'],\n",
    "        'Top1_Accuracy_Percent': strategy_best['Top1_Accuracy_Percent'],\n",
    "        'Total_Predictions': strategy_best['Total_Predictions']\n",
    "    })\n",
    "\n",
    "# Best per model size\n",
    "for size in comparison_df['Model_Size'].unique():\n",
    "    if size != 'Unknown':\n",
    "        size_best = comparison_df[comparison_df['Model_Size'] == size].loc[\n",
    "            comparison_df[comparison_df['Model_Size'] == size]['Top1_Accuracy_Percent'].idxmax()\n",
    "        ]\n",
    "        \n",
    "        top_performers_summary.append({\n",
    "            'Category': f'Best {size} Model',\n",
    "            'Model': size_best['Model_Display'],\n",
    "            'Strategy': size_best['Strategy_Display'],\n",
    "            'Dataset': size_best['Dataset'],\n",
    "            'Top1_Accuracy_Percent': size_best['Top1_Accuracy_Percent'],\n",
    "            'Total_Predictions': size_best['Total_Predictions']\n",
    "        })\n",
    "\n",
    "top_performers_df = pd.DataFrame(top_performers_summary)\n",
    "\n",
    "output_file = output_dir / 'top_performers_highlights.csv'\n",
    "top_performers_df.to_csv(output_file, index=False)\n",
    "print(f\"‚úÖ Exported top performers: {output_file}\")\n",
    "print(f\"   üèÜ {len(top_performers_df)} highlight categories\")\n",
    "\n",
    "print(f\"\\nüéØ ALL CSV FILES EXPORTED TO: {output_dir.absolute()}\")\n",
    "print(f\"üìä Ready for import into Canva for professional visualizations!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Summary Statistics & Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìà COMPREHENSIVE ANALYSIS SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Overall statistics\n",
    "total_combinations = len(comparison_df)\n",
    "unique_models = comparison_df['Model'].nunique()\n",
    "unique_strategies = comparison_df['Strategy'].nunique()\n",
    "total_predictions = comparison_df['Total_Predictions'].sum()\n",
    "\n",
    "print(f\"üîç Analyzed {total_combinations} model-strategy combinations\")\n",
    "print(f\"ü§ñ {unique_models} unique models tested\")\n",
    "print(f\"üéØ {unique_strategies} different strategies\")\n",
    "print(f\"üìä {total_predictions:,} total predictions processed\")\n",
    "\n",
    "print(f\"\\nüèÜ PERFORMANCE INSIGHTS:\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Best overall performance\n",
    "best_overall = comparison_df.loc[comparison_df['Top1_Accuracy_Percent'].idxmax()]\n",
    "print(f\"ü•á Best Overall: {best_overall['Model_Display']} with {best_overall['Strategy_Display']}\")\n",
    "print(f\"   üìä Top-1 Accuracy: {best_overall['Top1_Accuracy_Percent']:.2f}%\")\n",
    "print(f\"   üìà Top-5 Hit Rate: {best_overall['Top5_Hit_Rate_Percent']:.2f}%\")\n",
    "\n",
    "# Strategy effectiveness\n",
    "print(f\"\\nüéØ STRATEGY EFFECTIVENESS:\")\n",
    "strategy_performance = comparison_df.groupby('Strategy_Category')['Top1_Accuracy_Percent'].agg(['mean', 'std', 'count'])\n",
    "strategy_performance = strategy_performance.sort_values('mean', ascending=False)\n",
    "\n",
    "for strategy, row in strategy_performance.iterrows():\n",
    "    print(f\"   {strategy}: {row['mean']:.2f}% ¬± {row['std']:.2f}% ({int(row['count'])} models)\")\n",
    "\n",
    "# Model size analysis\n",
    "print(f\"\\nüíæ MODEL SIZE ANALYSIS:\")\n",
    "size_performance = comparison_df[comparison_df['Model_Size'] != 'Unknown'].groupby('Model_Size')['Top1_Accuracy_Percent'].agg(['mean', 'std', 'count'])\n",
    "size_performance = size_performance.sort_values('mean', ascending=False)\n",
    "\n",
    "for size, row in size_performance.iterrows():\n",
    "    print(f\"   {size}: {row['mean']:.2f}% ¬± {row['std']:.2f}% ({int(row['count'])} combinations)\")\n",
    "\n",
    "# Improvement analysis\n",
    "if len(improvement_df) > 0:\n",
    "    print(f\"\\nüìà STRATEGY IMPROVEMENTS:\")\n",
    "    print(\"-\" * 25)\n",
    "    \n",
    "    improvement_summary = improvement_df.groupby('Comparison')['Improvement_Points'].agg(['mean', 'std', 'count'])\n",
    "    improvement_summary = improvement_summary.sort_values('mean', ascending=False)\n",
    "    \n",
    "    for comparison, row in improvement_summary.iterrows():\n",
    "        direction = \"üìà\" if row['mean'] > 0 else \"üìâ\" if row['mean'] < 0 else \"‚û°Ô∏è\"\n",
    "        print(f\"   {direction} {comparison}: {row['mean']:+.2f} pp ¬± {row['std']:.2f} ({int(row['count'])} models)\")\n",
    "    \n",
    "    # Best improvements\n",
    "    best_improvements = improvement_df.nlargest(3, 'Improvement_Points')\n",
    "    print(f\"\\nüöÄ TOP 3 IMPROVEMENTS:\")\n",
    "    for _, row in best_improvements.iterrows():\n",
    "        print(f\"   {row['Model']} ({row['Comparison']}): +{row['Improvement_Points']:.2f} pp\")\n",
    "\n",
    "print(f\"\\nüí° KEY INSIGHTS:\")\n",
    "print(\"-\" * 15)\n",
    "\n",
    "# Generate automatic insights\n",
    "insights = []\n",
    "\n",
    "# Strategy insight\n",
    "best_strategy = strategy_performance.index[0]\n",
    "worst_strategy = strategy_performance.index[-1]\n",
    "strategy_gap = strategy_performance.loc[best_strategy, 'mean'] - strategy_performance.loc[worst_strategy, 'mean']\n",
    "insights.append(f\"üéØ {best_strategy} strategy outperforms {worst_strategy} by {strategy_gap:.1f} percentage points\")\n",
    "\n",
    "# Model size insight\n",
    "if len(size_performance) > 1:\n",
    "    largest_model = size_performance.index[0]\n",
    "    insights.append(f\"üíæ {largest_model} models show the best average performance ({size_performance.loc[largest_model, 'mean']:.1f}%)\")\n",
    "\n",
    "# Consistency insight\n",
    "most_consistent_strategy = strategy_performance.loc[strategy_performance['std'].idxmin()].name\n",
    "insights.append(f\"üéØ {most_consistent_strategy} strategy shows most consistent results across models\")\n",
    "\n",
    "# Improvement insight\n",
    "if len(improvement_df) > 0:\n",
    "    avg_geom_improvement = improvement_df[improvement_df['Comparison'] == 'Geospatial vs Base']['Improvement_Points'].mean()\n",
    "    avg_temporal_improvement = improvement_df[improvement_df['Comparison'] == 'Geo+Temporal vs Base']['Improvement_Points'].mean()\n",
    "    \n",
    "    if avg_geom_improvement > 0:\n",
    "        insights.append(f\"üìà Adding geospatial features improves performance by {avg_geom_improvement:.1f} pp on average\")\n",
    "    if avg_temporal_improvement > 0:\n",
    "        insights.append(f\"‚è∞ Full geo+temporal enhancement provides {avg_temporal_improvement:.1f} pp average improvement\")\n",
    "\n",
    "for i, insight in enumerate(insights, 1):\n",
    "    print(f\"{i}. {insight}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Analysis complete! All data exported and ready for presentation.\")\n",
    "print(f\"üìÅ Find your files in: {output_dir.absolute()}\")\n",
    "print(f\"üìä Find your plots in: {plots_dir.absolute()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final verification and file listing\n",
    "print(\"üìã FINAL FILE VERIFICATION\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# List all generated files\n",
    "csv_files = list(output_dir.glob('*.csv'))\n",
    "plot_files = list(plots_dir.glob('*.png'))\n",
    "\n",
    "print(f\"üìä CSV FILES ({len(csv_files)}):\")\n",
    "for csv_file in sorted(csv_files):\n",
    "    size = csv_file.stat().st_size\n",
    "    try:\n",
    "        df_check = pd.read_csv(csv_file)\n",
    "        print(f\"  ‚úÖ {csv_file.name} ({df_check.shape[0]}√ó{df_check.shape[1]}, {size:,} bytes)\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ùå {csv_file.name} (Error: {e})\")\n",
    "\n",
    "print(f\"\\nüñºÔ∏è PLOT FILES ({len(plot_files)}):\")\n",
    "for plot_file in sorted(plot_files):\n",
    "    size = plot_file.stat().st_size\n",
    "    print(f\"  ‚úÖ {plot_file.name} ({size:,} bytes)\")\n",
    "\n",
    "total_size = sum(f.stat().st_size for f in csv_files + plot_files)\n",
    "print(f\"\\nüíæ Total output size: {total_size:,} bytes ({total_size/1024/1024:.1f} MB)\")\n",
    "\n",
    "print(f\"\\nüé® CANVA IMPORT TIPS:\")\n",
    "print(\"- Use comprehensive_model_strategy_comparison.csv for main comparison charts\")\n",
    "print(\"- Use strategy_effectiveness_summary.csv for strategy-focused visualizations\")\n",
    "print(\"- Use top_performers_highlights.csv for presentation highlights\")\n",
    "print(\"- All percentages are pre-formatted (e.g., 4.32 not 0.0432)\")\n",
    "print(\"- Performance tiers are already categorized for easy color coding\")\n",
    "print(\"- Rankings are provided for easy sorting and highlighting\")\n",
    "print(\"\")\n",
    "print(\"üöÄ Ready for professional presentation creation!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADVANCED VISUALIZATION 1: Radar Chart for Model Comparison\n",
    "import math\n",
    "\n",
    "def create_radar_chart_data():\n",
    "    \"\"\"\n",
    "    Create radar chart data for multi-dimensional model comparison\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Calculate multiple performance dimensions for each model\n",
    "        radar_data = []\n",
    "        \n",
    "        for model in comparison_df['Model'].unique():\n",
    "            model_data = comparison_df[comparison_df['Model'] == model]\n",
    "            \n",
    "            # Calculate average performance across strategies\n",
    "            avg_top1 = model_data['Top1_Accuracy_Percent'].mean()\n",
    "            avg_top5 = model_data['Top5_Hit_Rate_Percent'].mean()\n",
    "            avg_mrr = model_data['MRR_Percent'].mean()\n",
    "            \n",
    "            # Calculate consistency (inverse of standard deviation)\n",
    "            consistency = 100 - (model_data['Top1_Accuracy_Percent'].std() * 10)  # Scale for radar\n",
    "            consistency = max(0, min(100, consistency))  # Clamp between 0-100\n",
    "            \n",
    "            # Calculate data coverage (based on total predictions)\n",
    "            max_predictions = comparison_df['Total_Predictions'].max()\n",
    "            coverage = (model_data['Total_Predictions'].sum() / max_predictions) * 100\n",
    "            coverage = min(100, coverage)  # Cap at 100%\n",
    "            \n",
    "            # Calculate strategy versatility (number of strategies available)\n",
    "            strategy_count = len(model_data)\n",
    "            versatility = (strategy_count / 3) * 100  # Assuming max 3 strategies\n",
    "            \n",
    "            radar_data.append({\n",
    "                'Model': model.replace('_', ' ').title(),\n",
    "                'Model_Code': model,\n",
    "                'Top1_Accuracy': avg_top1,\n",
    "                'Top5_Hit_Rate': avg_top5,\n",
    "                'MRR': avg_mrr,\n",
    "                'Consistency': consistency,\n",
    "                'Data_Coverage': coverage,\n",
    "                'Strategy_Versatility': versatility\n",
    "            })\n",
    "        \n",
    "        return pd.DataFrame(radar_data)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error creating radar chart data: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Create and export radar chart data\n",
    "radar_df = create_radar_chart_data()\n",
    "\n",
    "if not radar_df.empty:\n",
    "    # Export for Canva\n",
    "    output_file = output_dir / 'model_radar_comparison.csv'\n",
    "    radar_df.to_csv(output_file, index=False)\n",
    "    print(f\"‚úÖ Exported radar chart data: {output_file}\")\n",
    "    print(f\"   üìä {len(radar_df)} models with 6 performance dimensions\")\n",
    "    \n",
    "    # Create visualization\n",
    "    fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(projection='polar'))\n",
    "    \n",
    "    # Categories for radar chart\n",
    "    categories = ['Top1_Accuracy', 'Top5_Hit_Rate', 'MRR', 'Consistency', 'Data_Coverage', 'Strategy_Versatility']\n",
    "    category_labels = ['Top-1\\nAccuracy', 'Top-5\\nHit Rate', 'Mean\\nReciprocal\\nRank', 'Consistency', 'Data\\nCoverage', 'Strategy\\nVersatility']\n",
    "    \n",
    "    # Number of variables\n",
    "    N = len(categories)\n",
    "    \n",
    "    # Compute angles for each category\n",
    "    angles = [n / float(N) * 2 * math.pi for n in range(N)]\n",
    "    angles += angles[:1]  # Complete the circle\n",
    "    \n",
    "    # Plot data for each model\n",
    "    colors = plt.cm.Set3(np.linspace(0, 1, len(radar_df)))\n",
    "    \n",
    "    for idx, (_, row) in enumerate(radar_df.iterrows()):\n",
    "        values = [row[cat] for cat in categories]\n",
    "        values += values[:1]  # Complete the circle\n",
    "        \n",
    "        ax.plot(angles, values, 'o-', linewidth=2, label=row['Model'], color=colors[idx], alpha=0.8)\n",
    "        ax.fill(angles, values, alpha=0.1, color=colors[idx])\n",
    "    \n",
    "    # Customize the chart\n",
    "    ax.set_xticks(angles[:-1])\n",
    "    ax.set_xticklabels(category_labels, fontsize=10)\n",
    "    ax.set_ylim(0, 100)\n",
    "    ax.set_yticks([20, 40, 60, 80, 100])\n",
    "    ax.set_yticklabels(['20%', '40%', '60%', '80%', '100%'], fontsize=8)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add legend\n",
    "    ax.legend(loc='upper right', bbox_to_anchor=(1.2, 1.1), fontsize=9)\n",
    "    \n",
    "    plt.title('Multi-Dimensional Model Performance Comparison\\n(Radar Chart)', \n",
    "              fontsize=14, fontweight='bold', y=1.08)\n",
    "    \n",
    "    # Save plot\n",
    "    plot_path = plots_dir / 'model_radar_comparison.png'\n",
    "    plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"üìä Saved radar chart: {plot_path}\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Display the data table\n",
    "    print(\"\\nüìã RADAR CHART DATA:\")\n",
    "    display(radar_df.round(2))\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Could not create radar chart data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADVANCED VISUALIZATION 2: Performance Evolution & Temporal Analysis\n",
    "def create_temporal_analysis():\n",
    "    \"\"\"\n",
    "    Analyze performance trends across years and create temporal visualizations\n",
    "    \"\"\"\n",
    "    try:\n",
    "        temporal_data = []\n",
    "        \n",
    "        # Collect all individual prediction data\n",
    "        all_predictions = []\n",
    "        for dataset_type, models in all_model_data.items():\n",
    "            for model_name, strategies in models.items():\n",
    "                for strategy_name, df in strategies.items():\n",
    "                    if len(df) > 0:\n",
    "                        sample_df = df.copy()\n",
    "                        sample_df['model'] = model_name\n",
    "                        sample_df['strategy'] = strategy_name\n",
    "                        sample_df['dataset'] = dataset_type\n",
    "                        all_predictions.append(sample_df)\n",
    "        \n",
    "        if all_predictions:\n",
    "            combined_predictions = pd.concat(all_predictions, ignore_index=True)\n",
    "            \n",
    "            # Calculate yearly performance for each model-strategy combination\n",
    "            yearly_performance = combined_predictions.groupby(['model', 'strategy', 'dataset', 'year']).agg({\n",
    "                'hit@1': ['mean', 'count'],\n",
    "                'hit@5': 'mean',\n",
    "                'rr': 'mean'\n",
    "            }).round(4)\n",
    "            \n",
    "            # Flatten column names\n",
    "            yearly_performance.columns = ['Top1_Accuracy', 'Predictions_Count', 'Top5_Hit_Rate', 'MRR']\n",
    "            yearly_performance = yearly_performance.reset_index()\n",
    "            \n",
    "            # Convert to percentages\n",
    "            for col in ['Top1_Accuracy', 'Top5_Hit_Rate', 'MRR']:\n",
    "                yearly_performance[f'{col}_Percent'] = (yearly_performance[col] * 100).round(2)\n",
    "            \n",
    "            # Add display names\n",
    "            yearly_performance['Model_Display'] = yearly_performance['model'].str.replace('_', ' ').str.replace('-', ' ').str.title()\n",
    "            yearly_performance['Strategy_Display'] = yearly_performance['strategy'].str.replace('_', ' ').str.title()\n",
    "            \n",
    "            return yearly_performance\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è No temporal data available\")\n",
    "            return pd.DataFrame()\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error in temporal analysis: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Create temporal analysis\n",
    "temporal_df = create_temporal_analysis()\n",
    "\n",
    "if not temporal_df.empty:\n",
    "    # Export temporal data\n",
    "    output_file = output_dir / 'temporal_performance_analysis.csv'\n",
    "    temporal_df.to_csv(output_file, index=False)\n",
    "    print(f\"‚úÖ Exported temporal analysis: {output_file}\")\n",
    "    print(f\"   üìä {len(temporal_df)} year-model-strategy combinations\")\n",
    "    \n",
    "    # Create temporal visualization\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # Filter for better visualization (focus on models with multiple years)\n",
    "    models_with_data = temporal_df.groupby(['model', 'strategy']).size()\n",
    "    multi_year_combinations = models_with_data[models_with_data > 3].index.tolist()\n",
    "    \n",
    "    if multi_year_combinations:\n",
    "        # Plot 1: Performance trends over years for top models\n",
    "        top_combinations = []\n",
    "        for model, strategy in multi_year_combinations[:4]:  # Top 4 combinations\n",
    "            combo_data = temporal_df[(temporal_df['model'] == model) & (temporal_df['strategy'] == strategy)]\n",
    "            if len(combo_data) > 0:\n",
    "                top_combinations.append((model, strategy, combo_data))\n",
    "        \n",
    "        for idx, (model, strategy, data) in enumerate(top_combinations):\n",
    "            label = f\"{model.replace('_', ' ').title()} ({strategy.replace('_', ' ').title()})\"\n",
    "            axes[0,0].plot(data['year'], data['Top1_Accuracy_Percent'], \n",
    "                          marker='o', label=label, linewidth=2, alpha=0.8)\n",
    "        \n",
    "        axes[0,0].set_title('Performance Trends Over Years\\n(Top Model-Strategy Combinations)', \n",
    "                           fontsize=14, fontweight='bold')\n",
    "        axes[0,0].set_xlabel('Year', fontweight='bold')\n",
    "        axes[0,0].set_ylabel('Top-1 Accuracy (%)', fontweight='bold')\n",
    "        axes[0,0].legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=8)\n",
    "        axes[0,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Year-over-year performance distribution\n",
    "    if 'year' in temporal_df.columns and len(temporal_df['year'].unique()) > 1:\n",
    "        sns.boxplot(data=temporal_df, x='year', y='Top1_Accuracy_Percent', ax=axes[0,1])\n",
    "        axes[0,1].set_title('Performance Distribution by Year', fontsize=14, fontweight='bold')\n",
    "        axes[0,1].set_xlabel('Year', fontweight='bold')\n",
    "        axes[0,1].set_ylabel('Top-1 Accuracy (%)', fontweight='bold')\n",
    "        axes[0,1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Plot 3: Dataset volume vs performance over years\n",
    "    if len(temporal_df) > 10:\n",
    "        scatter = axes[1,0].scatter(temporal_df['Predictions_Count'], \n",
    "                                  temporal_df['Top1_Accuracy_Percent'],\n",
    "                                  c=temporal_df['year'], cmap='viridis', \n",
    "                                  alpha=0.6, s=50)\n",
    "        axes[1,0].set_xlabel('Predictions Count (per year)', fontweight='bold')\n",
    "        axes[1,0].set_ylabel('Top-1 Accuracy (%)', fontweight='bold')\n",
    "        axes[1,0].set_title('Dataset Volume vs Performance\\n(Color = Year)', fontsize=14, fontweight='bold')\n",
    "        axes[1,0].set_xscale('log')\n",
    "        plt.colorbar(scatter, ax=axes[1,0], label='Year')\n",
    "    \n",
    "    # Plot 4: Performance consistency across years\n",
    "    if len(temporal_df) > 5:\n",
    "        # Calculate coefficient of variation for each model-strategy\n",
    "        consistency_data = []\n",
    "        for (model, strategy), group in temporal_df.groupby(['model', 'strategy']):\n",
    "            if len(group) > 2:  # Need at least 3 years for meaningful CV\n",
    "                cv = (group['Top1_Accuracy_Percent'].std() / group['Top1_Accuracy_Percent'].mean()) * 100\n",
    "                consistency_data.append({\n",
    "                    'Model_Strategy': f\"{model.replace('_', ' ')[:10]}\\n({strategy.replace('_', ' ')[:8]})\",\n",
    "                    'Consistency_Score': max(0, 100 - cv),  # Higher score = more consistent\n",
    "                    'Mean_Performance': group['Top1_Accuracy_Percent'].mean(),\n",
    "                    'Years_Count': len(group)\n",
    "                })\n",
    "        \n",
    "        if consistency_data:\n",
    "            consistency_df = pd.DataFrame(consistency_data)\n",
    "            consistency_df = consistency_df.sort_values('Mean_Performance', ascending=True).tail(8)  # Top 8\n",
    "            \n",
    "            bars = axes[1,1].barh(consistency_df['Model_Strategy'], consistency_df['Consistency_Score'])\n",
    "            axes[1,1].set_xlabel('Consistency Score (0-100)', fontweight='bold')\n",
    "            axes[1,1].set_title('Performance Consistency Across Years\\n(Higher = More Stable)', \n",
    "                               fontsize=14, fontweight='bold')\n",
    "            \n",
    "            # Color bars by mean performance\n",
    "            norm = plt.Normalize(consistency_df['Mean_Performance'].min(), consistency_df['Mean_Performance'].max())\n",
    "            for bar, perf in zip(bars, consistency_df['Mean_Performance']):\n",
    "                bar.set_color(plt.cm.RdYlGn(norm(perf)))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save plot\n",
    "    plot_path = plots_dir / 'temporal_performance_analysis.png'\n",
    "    plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"üìä Saved temporal analysis plot: {plot_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # Show summary statistics\n",
    "    print(\"\\nüìà TEMPORAL PERFORMANCE SUMMARY:\")\n",
    "    print(\"=\" * 35)\n",
    "    \n",
    "    if len(temporal_df['year'].unique()) > 1:\n",
    "        yearly_avg = temporal_df.groupby('year')['Top1_Accuracy_Percent'].mean().sort_index()\n",
    "        print(\"üìÖ Average performance by year:\")\n",
    "        for year, perf in yearly_avg.items():\n",
    "            print(f\"   {year}: {perf:.2f}%\")\n",
    "        \n",
    "        # Calculate trend\n",
    "        years = yearly_avg.index.values\n",
    "        perfs = yearly_avg.values\n",
    "        if len(years) > 1:\n",
    "            trend = np.polyfit(years, perfs, 1)[0]  # Linear trend coefficient\n",
    "            direction = \"üìà Improving\" if trend > 0 else \"üìâ Declining\" if trend < 0 else \"‚û°Ô∏è Stable\"\n",
    "            print(f\"\\nüéØ Overall trend: {direction} ({trend:+.3f}% per year)\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Could not create temporal analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENHANCED ERROR HANDLING & DIAGNOSTIC FUNCTIONS\n",
    "\n",
    "def enhanced_error_handling():\n",
    "    \"\"\"\n",
    "    Comprehensive error handling and diagnostic system\n",
    "    \"\"\"\n",
    "    \n",
    "    class DataProcessingError(Exception):\n",
    "        \"\"\"Custom exception for data processing errors\"\"\"\n",
    "        pass\n",
    "    \n",
    "    class FileValidationError(Exception):\n",
    "        \"\"\"Custom exception for file validation errors\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def validate_csv_file(filepath, required_columns=None, min_rows=1):\n",
    "        \"\"\"\n",
    "        Validate CSV file structure and content\n",
    "        \n",
    "        Args:\n",
    "            filepath (Path): Path to CSV file\n",
    "            required_columns (list): List of required column names\n",
    "            min_rows (int): Minimum number of rows required\n",
    "            \n",
    "        Returns:\n",
    "            dict: Validation result with status and details\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if not filepath.exists():\n",
    "                return {\n",
    "                    'status': 'error',\n",
    "                    'message': f'File does not exist: {filepath}',\n",
    "                    'suggestions': ['Check file path', 'Ensure file was created successfully']\n",
    "                }\n",
    "            \n",
    "            # Try to read the file\n",
    "            try:\n",
    "                df = pd.read_csv(filepath)\n",
    "            except pd.errors.EmptyDataError:\n",
    "                return {\n",
    "                    'status': 'error',\n",
    "                    'message': 'File is empty',\n",
    "                    'suggestions': ['Check data generation process', 'Verify data sources']\n",
    "                }\n",
    "            except pd.errors.ParserError as e:\n",
    "                return {\n",
    "                    'status': 'error',\n",
    "                    'message': f'CSV parsing error: {e}',\n",
    "                    'suggestions': [\n",
    "                        'Check CSV formatting',\n",
    "                        'Look for inconsistent delimiters',\n",
    "                        'Check for embedded quotes or newlines'\n",
    "                    ]\n",
    "                }\n",
    "            \n",
    "            # Validate row count\n",
    "            if len(df) < min_rows:\n",
    "                return {\n",
    "                    'status': 'warning',\n",
    "                    'message': f'File has only {len(df)} rows (minimum: {min_rows})',\n",
    "                    'suggestions': ['Check data filtering logic', 'Verify data source completeness']\n",
    "                }\n",
    "            \n",
    "            # Validate required columns\n",
    "            if required_columns:\n",
    "                missing_cols = [col for col in required_columns if col not in df.columns]\n",
    "                if missing_cols:\n",
    "                    return {\n",
    "                        'status': 'error',\n",
    "                        'message': f'Missing required columns: {missing_cols}',\n",
    "                        'available_columns': list(df.columns),\n",
    "                        'suggestions': [\n",
    "                            'Check column name spelling',\n",
    "                            'Verify data processing pipeline',\n",
    "                            'Check for column name changes'\n",
    "                        ]\n",
    "                    }\n",
    "            \n",
    "            # Check for data quality issues\n",
    "            quality_issues = []\n",
    "            \n",
    "            # Check for missing values\n",
    "            missing_counts = df.isnull().sum()\n",
    "            critical_missing = missing_counts[missing_counts > len(df) * 0.5]\n",
    "            if len(critical_missing) > 0:\n",
    "                quality_issues.append(f'High missing values in: {list(critical_missing.index)}')\n",
    "            \n",
    "            # Check for duplicate rows\n",
    "            if df.duplicated().sum() > 0:\n",
    "                quality_issues.append(f'{df.duplicated().sum()} duplicate rows found')\n",
    "            \n",
    "            # Check for numeric columns with invalid values\n",
    "            numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "            for col in numeric_cols:\n",
    "                if df[col].isna().sum() == len(df):\n",
    "                    quality_issues.append(f'Column {col} is entirely empty')\n",
    "                elif (df[col] < 0).any() and 'Percent' in col:\n",
    "                    quality_issues.append(f'Negative percentages in {col}')\n",
    "                elif (df[col] > 100).any() and 'Percent' in col:\n",
    "                    quality_issues.append(f'Percentages > 100% in {col}')\n",
    "            \n",
    "            return {\n",
    "                'status': 'success' if not quality_issues else 'warning',\n",
    "                'message': f'File validated successfully ({len(df)} rows, {len(df.columns)} columns)',\n",
    "                'quality_issues': quality_issues,\n",
    "                'file_info': {\n",
    "                    'rows': len(df),\n",
    "                    'columns': len(df.columns),\n",
    "                    'size_mb': filepath.stat().st_size / 1024 / 1024,\n",
    "                    'column_names': list(df.columns)\n",
    "                }\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                'status': 'error',\n",
    "                'message': f'Unexpected validation error: {str(e)}',\n",
    "                'suggestions': [\n",
    "                    'Check file permissions',\n",
    "                    'Verify file is not corrupted',\n",
    "                    'Try opening file manually'\n",
    "                ]\n",
    "            }\n",
    "    \n",
    "    def diagnose_data_loading_issues(all_model_data):\n",
    "        \"\"\"\n",
    "        Diagnose common data loading issues and provide solutions\n",
    "        \"\"\"\n",
    "        print(\"üîç DATA LOADING DIAGNOSTIC REPORT\")\n",
    "        print(\"=\" * 40)\n",
    "        \n",
    "        total_models = 0\n",
    "        total_strategies = 0\n",
    "        total_predictions = 0\n",
    "        issues_found = []\n",
    "        \n",
    "        for dataset_type, models in all_model_data.items():\n",
    "            print(f\"\\nüìÅ Dataset: {dataset_type.upper()}\")\n",
    "            \n",
    "            if not models:\n",
    "                issues_found.append(f\"No models found in {dataset_type} dataset\")\n",
    "                print(\"   ‚ùå No models loaded\")\n",
    "                continue\n",
    "                \n",
    "            dataset_predictions = 0\n",
    "            \n",
    "            for model_name, strategies in models.items():\n",
    "                total_models += 1\n",
    "                model_predictions = 0\n",
    "                \n",
    "                print(f\"   ü§ñ {model_name}:\")\n",
    "                \n",
    "                if not strategies:\n",
    "                    issues_found.append(f\"No strategies found for {model_name} in {dataset_type}\")\n",
    "                    print(\"      ‚ùå No strategies loaded\")\n",
    "                    continue\n",
    "                \n",
    "                for strategy_name, df in strategies.items():\n",
    "                    total_strategies += 1\n",
    "                    strategy_predictions = len(df) if df is not None else 0\n",
    "                    model_predictions += strategy_predictions\n",
    "                    \n",
    "                    if strategy_predictions == 0:\n",
    "                        issues_found.append(f\"Empty dataset: {model_name}/{strategy_name} in {dataset_type}\")\n",
    "                        print(f\"      ‚ö†Ô∏è {strategy_name}: No data\")\n",
    "                    elif strategy_predictions < 1000:\n",
    "                        issues_found.append(f\"Small dataset: {model_name}/{strategy_name} has only {strategy_predictions} predictions\")\n",
    "                        print(f\"      ‚ö†Ô∏è {strategy_name}: {strategy_predictions:,} predictions (small)\")\n",
    "                    else:\n",
    "                        print(f\"      ‚úÖ {strategy_name}: {strategy_predictions:,} predictions\")\n",
    "                \n",
    "                dataset_predictions += model_predictions\n",
    "                if model_predictions == 0:\n",
    "                    issues_found.append(f\"Model {model_name} has no valid data in {dataset_type}\")\n",
    "            \n",
    "            total_predictions += dataset_predictions\n",
    "            print(f\"   üìä Dataset total: {dataset_predictions:,} predictions\")\n",
    "        \n",
    "        print(f\"\\nüìà OVERALL SUMMARY:\")\n",
    "        print(f\"   ü§ñ Total models: {total_models}\")\n",
    "        print(f\"   üéØ Total strategies: {total_strategies}\")\n",
    "        print(f\"   üìä Total predictions: {total_predictions:,}\")\n",
    "        \n",
    "        if issues_found:\n",
    "            print(f\"\\n‚ö†Ô∏è ISSUES IDENTIFIED ({len(issues_found)}):\")\n",
    "            for i, issue in enumerate(issues_found, 1):\n",
    "                print(f\"   {i}. {issue}\")\n",
    "            \n",
    "            print(f\"\\nüîß TROUBLESHOOTING SUGGESTIONS:\")\n",
    "            print(\"   ‚Ä¢ Check that CSV files exist in the results directories\")\n",
    "            print(\"   ‚Ä¢ Verify CSV files are not corrupted (try opening manually)\")\n",
    "            print(\"   ‚Ä¢ Check file permissions and paths\")\n",
    "            print(\"   ‚Ä¢ Ensure prediction columns contain valid data\")\n",
    "            print(\"   ‚Ä¢ Look for missing or incomplete model runs\")\n",
    "            print(\"   ‚Ä¢ Check for consistent CSV formatting across files\")\n",
    "        else:\n",
    "            print(\"\\n‚úÖ No major issues detected!\")\n",
    "        \n",
    "        return {\n",
    "            'total_models': total_models,\n",
    "            'total_strategies': total_strategies,\n",
    "            'total_predictions': total_predictions,\n",
    "            'issues': issues_found\n",
    "        }\n",
    "    \n",
    "    return validate_csv_file, diagnose_data_loading_issues, DataProcessingError, FileValidationError\n",
    "\n",
    "# Initialize error handling functions\n",
    "validate_csv_file, diagnose_data_loading_issues, DataProcessingError, FileValidationError = enhanced_error_handling()\n",
    "\n",
    "# Run diagnostic on loaded data\n",
    "diagnostic_result = diagnose_data_loading_issues(all_model_data)\n",
    "\n",
    "print(f\"\\nüîç VALIDATION OF EXPORTED CSV FILES:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Validate all exported CSV files\n",
    "csv_files = list(output_dir.glob('*.csv'))\n",
    "validation_results = {}\n",
    "\n",
    "for csv_file in csv_files:\n",
    "    print(f\"\\nüìÑ Validating {csv_file.name}...\")\n",
    "    \n",
    "    # Define expected columns for each file type\n",
    "    expected_columns = {\n",
    "        'comprehensive_model_strategy_comparison.csv': [\n",
    "            'Model_Display', 'Strategy_Display', 'Top1_Accuracy_Percent', 'Dataset'\n",
    "        ],\n",
    "        'strategy_effectiveness_summary.csv': [\n",
    "            'Strategy_Category', 'Dataset', 'Top1_Mean', 'Models_Count'\n",
    "        ],\n",
    "        'model_performance_summary.csv': [\n",
    "            'Model_Display', 'Model_Size', 'Top1_Best', 'Best_Strategy'\n",
    "        ],\n",
    "        'top_performers_highlights.csv': [\n",
    "            'Category', 'Model', 'Strategy', 'Top1_Accuracy_Percent'\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    required_cols = expected_columns.get(csv_file.name, [])\n",
    "    result = validate_csv_file(csv_file, required_cols, min_rows=1)\n",
    "    validation_results[csv_file.name] = result\n",
    "    \n",
    "    if result['status'] == 'success':\n",
    "        print(f\"   ‚úÖ {result['message']}\")\n",
    "        if 'file_info' in result:\n",
    "            info = result['file_info']\n",
    "            print(f\"      üìä {info['rows']} rows √ó {info['columns']} columns ({info['size_mb']:.2f} MB)\")\n",
    "    elif result['status'] == 'warning':\n",
    "        print(f\"   ‚ö†Ô∏è {result['message']}\")\n",
    "        if 'quality_issues' in result and result['quality_issues']:\n",
    "            for issue in result['quality_issues']:\n",
    "                print(f\"      ‚Ä¢ {issue}\")\n",
    "    else:\n",
    "        print(f\"   ‚ùå {result['message']}\")\n",
    "        if 'suggestions' in result:\n",
    "            print(\"      Suggestions:\")\n",
    "            for suggestion in result['suggestions']:\n",
    "                print(f\"        ‚Ä¢ {suggestion}\")\n",
    "\n",
    "print(f\"\\nüìä VALIDATION SUMMARY:\")\n",
    "success_count = sum(1 for r in validation_results.values() if r['status'] == 'success')\n",
    "warning_count = sum(1 for r in validation_results.values() if r['status'] == 'warning')\n",
    "error_count = sum(1 for r in validation_results.values() if r['status'] == 'error')\n",
    "\n",
    "print(f\"   ‚úÖ Successful: {success_count}/{len(validation_results)}\")\n",
    "print(f\"   ‚ö†Ô∏è Warnings: {warning_count}/{len(validation_results)}\")\n",
    "print(f\"   ‚ùå Errors: {error_count}/{len(validation_results)}\")\n",
    "\n",
    "if error_count == 0:\n",
    "    print(f\"\\nüéâ All CSV files are valid and ready for Canva import!\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è Some files have issues that need attention before use.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPREHENSIVE CSV DOCUMENTATION & ERROR HANDLING\n",
    "\n",
    "def create_csv_documentation():\n",
    "    \"\"\"\n",
    "    Create comprehensive documentation for all exported CSV files\n",
    "    with detailed explanations, use cases, and data schemas\n",
    "    \"\"\"\n",
    "    csv_docs = {\n",
    "        'comprehensive_model_strategy_comparison.csv': {\n",
    "            'title': 'üéØ Comprehensive Model-Strategy Comparison',\n",
    "            'description': 'Main comparison dataset with all model-strategy combinations and performance metrics',\n",
    "            'use_cases': [\n",
    "                'Main comparison charts in Canva (bar charts, heatmaps)',\n",
    "                'Ranking visualizations and performance tables',\n",
    "                'Model selection decision support',\n",
    "                'Strategy effectiveness comparison'\n",
    "            ],\n",
    "            'key_columns': {\n",
    "                'Dataset': 'middle or penultimate - data source',\n",
    "                'Model_Display': 'Human-readable model names (e.g., \"Qwen2.5 14B\")',\n",
    "                'Strategy_Display': 'Human-readable strategy names (e.g., \"With Geom\")',\n",
    "                'Top1_Accuracy_Percent': 'Top-1 accuracy as percentage (0-100)',\n",
    "                'Top5_Hit_Rate_Percent': 'Top-5 hit rate as percentage (0-100)',\n",
    "                'MRR_Percent': 'Mean Reciprocal Rank as percentage (0-100)',\n",
    "                'Error_Rate_Percent': '100 - Top1_Accuracy_Percent',\n",
    "                'Total_Predictions': 'Number of predictions made',\n",
    "                'Rank_Overall': 'Global ranking by Top-1 accuracy (1=best)',\n",
    "                'Performance_Tier': 'Categorical tier (Basic/Fair/Good/Very Good/Excellent)',\n",
    "                'Model_Size': 'Parameter count category (7B, 8B, 14B, 33B, 8x7B)'\n",
    "            },\n",
    "            'canva_tips': [\n",
    "                'Use Rank_Overall for sorting charts',\n",
    "                'Color-code by Performance_Tier',\n",
    "                'Filter by Dataset for cleaner comparisons',\n",
    "                'Use Model_Size for grouping visualizations'\n",
    "            ]\n",
    "        },\n",
    "        \n",
    "        'strategy_effectiveness_summary.csv': {\n",
    "            'title': 'üéØ Strategy Effectiveness Summary',\n",
    "            'description': 'Aggregated performance statistics for each strategy across all models',\n",
    "            'use_cases': [\n",
    "                'Strategy comparison bar charts',\n",
    "                'Statistical confidence intervals',\n",
    "                'Strategy recommendation support',\n",
    "                'Performance distribution analysis'\n",
    "            ],\n",
    "            'key_columns': {\n",
    "                'Strategy_Category': 'Base, Geospatial, or Geo+Temporal',\n",
    "                'Dataset': 'middle or penultimate',\n",
    "                'Top1_Mean': 'Average Top-1 accuracy across models (percentage)',\n",
    "                'Top1_Std': 'Standard deviation of Top-1 accuracy',\n",
    "                'Models_Count': 'Number of models tested with this strategy',\n",
    "                'Error_Rate_Mean': '100 - Top1_Mean'\n",
    "            },\n",
    "            'canva_tips': [\n",
    "                'Use Top1_Mean for main bar heights',\n",
    "                'Add error bars using Top1_Std',\n",
    "                'Show Models_Count as data labels',\n",
    "                'Use Error_Rate_Mean for failure analysis'\n",
    "            ]\n",
    "        },\n",
    "        \n",
    "        'model_performance_summary.csv': {\n",
    "            'title': 'ü§ñ Model Performance Summary',\n",
    "            'description': 'Comprehensive performance statistics for each model across all strategies',\n",
    "            'use_cases': [\n",
    "                'Model comparison charts',\n",
    "                'Best strategy identification per model',\n",
    "                'Model capability assessment',\n",
    "                'Resource allocation decisions'\n",
    "            ],\n",
    "            'key_columns': {\n",
    "                'Model_Display': 'Human-readable model name',\n",
    "                'Model_Size': 'Parameter count category',\n",
    "                'Top1_Best': 'Best Top-1 accuracy achieved by this model',\n",
    "                'Available_Strategies': 'Comma-separated list of tested strategies',\n",
    "                'Best_Strategy': 'Strategy that achieved Top1_Best performance',\n",
    "                'Strategy_Count': 'Number of strategies tested for this model'\n",
    "            },\n",
    "            'canva_tips': [\n",
    "                'Sort by Top1_Best for ranking charts',\n",
    "                'Use Model_Size for grouping',\n",
    "                'Highlight Best_Strategy in annotations',\n",
    "                'Size bubbles by Strategy_Count'\n",
    "            ]\n",
    "        },\n",
    "        \n",
    "        'strategy_improvement_analysis.csv': {\n",
    "            'title': 'üìà Strategy Improvement Analysis',\n",
    "            'description': 'Detailed analysis of performance improvements when upgrading strategies',\n",
    "            'use_cases': [\n",
    "                'Improvement waterfall charts',\n",
    "                'Strategy ROI visualization',\n",
    "                'Decision support for strategy upgrades',\n",
    "                'Cost-benefit presentations'\n",
    "            ],\n",
    "            'key_columns': {\n",
    "                'Comparison': 'Type of comparison (e.g., \"Geospatial vs Base\")',\n",
    "                'Base_Accuracy': 'Performance of baseline strategy',\n",
    "                'Enhanced_Accuracy': 'Performance of enhanced strategy',\n",
    "                'Improvement_Points': 'Absolute improvement in percentage points',\n",
    "                'Improvement_Percent': 'Relative improvement as percentage',\n",
    "                'Improvement_Category': 'Categorical improvement level'\n",
    "            },\n",
    "            'canva_tips': [\n",
    "                'Use Improvement_Points for bar charts',\n",
    "                'Color by Improvement_Category',\n",
    "                'Show both baseline and enhanced values',\n",
    "                'Filter positive improvements for success stories'\n",
    "            ]\n",
    "        },\n",
    "        \n",
    "        'top_performers_highlights.csv': {\n",
    "            'title': 'üèÜ Top Performers Highlights',\n",
    "            'description': 'Key highlights and top performers for presentation summaries',\n",
    "            'use_cases': [\n",
    "                'Executive summary slides',\n",
    "                'Key findings highlights',\n",
    "                'Award-style presentations',\n",
    "                'Quick fact sheets'\n",
    "            ],\n",
    "            'key_columns': {\n",
    "                'Category': 'Type of achievement (e.g., \"Overall Best\", \"Best 14B Model\")',\n",
    "                'Model': 'Winning model name',\n",
    "                'Strategy': 'Winning strategy name',\n",
    "                'Top1_Accuracy_Percent': 'Achievement value'\n",
    "            },\n",
    "            'canva_tips': [\n",
    "                'Create award cards for each category',\n",
    "                'Use as callout boxes in presentations',\n",
    "                'Perfect for infographic highlights',\n",
    "                'Great for social media snippets'\n",
    "            ]\n",
    "        },\n",
    "        \n",
    "        'model_radar_comparison.csv': {\n",
    "            'title': 'üì° Multi-Dimensional Model Radar Data',\n",
    "            'description': 'Multi-dimensional performance data optimized for radar/spider charts',\n",
    "            'use_cases': [\n",
    "                'Radar/spider charts showing model strengths',\n",
    "                'Multi-dimensional comparisons',\n",
    "                'Holistic model assessment',\n",
    "                'Complex performance visualization'\n",
    "            ],\n",
    "            'key_columns': {\n",
    "                'Model': 'Model display name',\n",
    "                'Top1_Accuracy': 'Average accuracy performance (0-100)',\n",
    "                'Top5_Hit_Rate': 'Average hit rate performance (0-100)',\n",
    "                'Consistency': 'Performance consistency score (0-100)',\n",
    "                'Data_Coverage': 'Data volume coverage score (0-100)',\n",
    "                'Strategy_Versatility': 'Strategy availability score (0-100)'\n",
    "            },\n",
    "            'canva_tips': [\n",
    "                'Perfect for radar charts in Canva',\n",
    "                'All values scaled 0-100 for consistency',\n",
    "                'Compare models across multiple dimensions',\n",
    "                'Great for capability assessment'\n",
    "            ]\n",
    "        },\n",
    "        \n",
    "        'temporal_performance_analysis.csv': {\n",
    "            'title': '‚è∞ Temporal Performance Analysis',\n",
    "            'description': 'Year-by-year performance data for trend analysis',\n",
    "            'use_cases': [\n",
    "                'Time series line charts',\n",
    "                'Performance trend analysis',\n",
    "                'Yearly comparison tables',\n",
    "                'Temporal stability assessment'\n",
    "            ],\n",
    "            'key_columns': {\n",
    "                'model': 'Model code name',\n",
    "                'strategy': 'Strategy code name',\n",
    "                'year': 'Data year (2014-2023)',\n",
    "                'Top1_Accuracy_Percent': 'Yearly Top-1 accuracy',\n",
    "                'Predictions_Count': 'Number of predictions that year'\n",
    "            },\n",
    "            'canva_tips': [\n",
    "                'Use year as X-axis for line charts',\n",
    "                'Filter by model/strategy for focused trends',\n",
    "                'Size points by Predictions_Count',\n",
    "                'Show confidence through data volume'\n",
    "            ]\n",
    "        },\n",
    "        \n",
    "        'cost_benefit_analysis.csv': {\n",
    "            'title': 'üí∞ Cost-Benefit Analysis',\n",
    "            'description': 'Comprehensive cost-benefit analysis with ROI calculations',\n",
    "            'use_cases': [\n",
    "                'ROI comparison charts',\n",
    "                'Cost vs performance scatter plots',\n",
    "                'Budget allocation decisions',\n",
    "                'Resource optimization visualization'\n",
    "            ],\n",
    "            'key_columns': {\n",
    "                'Computational_Cost': 'Estimated relative computational cost',\n",
    "                'Performance': 'Top-1 accuracy performance',\n",
    "                'ROI_Score': 'Return on investment score',\n",
    "                'Efficiency_Score': 'Performance per cost unit',\n",
    "                'Cost_Category': 'Low/Medium/High cost classification',\n",
    "                'Benefit_Category': 'Low/Medium/High benefit classification'\n",
    "            },\n",
    "            'canva_tips': [\n",
    "                'Use ROI_Score for ranking charts',\n",
    "                'Create cost vs performance scatter plots',\n",
    "                'Color by Cost_Category or Benefit_Category',\n",
    "                'Size bubbles by Efficiency_Score'\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return csv_docs\n",
    "\n",
    "# Create documentation\n",
    "csv_documentation = create_csv_documentation()\n",
    "\n",
    "print(\"üìö COMPREHENSIVE CSV DOCUMENTATION\")\n",
    "print(\"=\" * 50)\n",
    "print(\"\\nüéØ This documentation covers all exported CSV files with detailed\")\n",
    "print(\"   explanations, use cases, and Canva integration tips.\\n\")\n",
    "\n",
    "for filename, doc in csv_documentation.items():\n",
    "    print(f\"üìÑ {doc['title']}\")\n",
    "    print(f\"   File: {filename}\")\n",
    "    print(f\"   {doc['description']}\\n\")\n",
    "    \n",
    "    print(\"   üìä USE CASES:\")\n",
    "    for use_case in doc['use_cases']:\n",
    "        print(f\"      ‚Ä¢ {use_case}\")\n",
    "    \n",
    "    if 'key_columns' in doc:\n",
    "        print(\"\\n   üè∑Ô∏è KEY COLUMNS:\")\n",
    "        for col, desc in doc['key_columns'].items():\n",
    "            print(f\"      ‚Ä¢ {col}: {desc}\")\n",
    "    \n",
    "    print(\"\\n   üé® CANVA TIPS:\")\n",
    "    for tip in doc['canva_tips']:\n",
    "        print(f\"      ‚Ä¢ {tip}\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\" * 60 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADVANCED VISUALIZATION 2: Performance Evolution & Temporal Analysis\n",
    "def create_temporal_analysis():\n",
    "    \"\"\"\n",
    "    Analyze performance trends across years and create temporal visualizations\n",
    "    \"\"\"\n",
    "    try:\n",
    "        temporal_data = []\n",
    "        \n",
    "        # Collect all individual prediction data\n",
    "        all_predictions = []\n",
    "        for dataset_type, models in all_model_data.items():\n",
    "            for model_name, strategies in models.items():\n",
    "                for strategy_name, df in strategies.items():\n",
    "                    if len(df) > 0:\n",
    "                        sample_df = df.copy()\n",
    "                        sample_df['model'] = model_name\n",
    "                        sample_df['strategy'] = strategy_name\n",
    "                        sample_df['dataset'] = dataset_type\n",
    "                        all_predictions.append(sample_df)\n",
    "        \n",
    "        if all_predictions:\n",
    "            combined_predictions = pd.concat(all_predictions, ignore_index=True)\n",
    "            \n",
    "            # Calculate yearly performance for each model-strategy combination\n",
    "            yearly_performance = combined_predictions.groupby(['model', 'strategy', 'dataset', 'year']).agg({\n",
    "                'hit@1': ['mean', 'count'],\n",
    "                'hit@5': 'mean',\n",
    "                'rr': 'mean'\n",
    "            }).round(4)\n",
    "            \n",
    "            # Flatten column names\n",
    "            yearly_performance.columns = ['Top1_Accuracy', 'Predictions_Count', 'Top5_Hit_Rate', 'MRR']\n",
    "            yearly_performance = yearly_performance.reset_index()\n",
    "            \n",
    "            # Convert to percentages\n",
    "            for col in ['Top1_Accuracy', 'Top5_Hit_Rate', 'MRR']:\n",
    "                yearly_performance[f'{col}_Percent'] = (yearly_performance[col] * 100).round(2)\n",
    "            \n",
    "            # Add display names\n",
    "            yearly_performance['Model_Display'] = yearly_performance['model'].str.replace('_', ' ').str.replace('-', ' ').str.title()\n",
    "            yearly_performance['Strategy_Display'] = yearly_performance['strategy'].str.replace('_', ' ').str.title()\n",
    "            \n",
    "            return yearly_performance\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è No temporal data available\")\n",
    "            return pd.DataFrame()\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error in temporal analysis: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Create temporal analysis\n",
    "temporal_df = create_temporal_analysis()\n",
    "\n",
    "if not temporal_df.empty:\n",
    "    # Export temporal data\n",
    "    output_file = output_dir / 'temporal_performance_analysis.csv'\n",
    "    temporal_df.to_csv(output_file, index=False)\n",
    "    print(f\"‚úÖ Exported temporal analysis: {output_file}\")\n",
    "    print(f\"   üìä {len(temporal_df)} year-model-strategy combinations\")\n",
    "    \n",
    "    # Create temporal visualization\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # Filter for better visualization (focus on models with multiple years)\n",
    "    models_with_data = temporal_df.groupby(['model', 'strategy']).size()\n",
    "    multi_year_combinations = models_with_data[models_with_data > 3].index.tolist()\n",
    "    \n",
    "    if multi_year_combinations:\n",
    "        # Plot 1: Performance trends over years for top models\n",
    "        top_combinations = []\n",
    "        for model, strategy in multi_year_combinations[:4]:  # Top 4 combinations\n",
    "            combo_data = temporal_df[(temporal_df['model'] == model) & (temporal_df['strategy'] == strategy)]\n",
    "            if len(combo_data) > 0:\n",
    "                top_combinations.append((model, strategy, combo_data))\n",
    "        \n",
    "        for idx, (model, strategy, data) in enumerate(top_combinations):\n",
    "            label = f\"{model.replace('_', ' ').title()} ({strategy.replace('_', ' ').title()})\"\n",
    "            axes[0,0].plot(data['year'], data['Top1_Accuracy_Percent'], \n",
    "                          marker='o', label=label, linewidth=2, alpha=0.8)\n",
    "        \n",
    "        axes[0,0].set_title('Performance Trends Over Years\\\\n(Top Model-Strategy Combinations)', \n",
    "                           fontsize=14, fontweight='bold')\n",
    "        axes[0,0].set_xlabel('Year', fontweight='bold')\n",
    "        axes[0,0].set_ylabel('Top-1 Accuracy (%)', fontweight='bold')\n",
    "        axes[0,0].legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=8)\n",
    "        axes[0,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Year-over-year performance distribution\n",
    "    if 'year' in temporal_df.columns and len(temporal_df['year'].unique()) > 1:\n",
    "        sns.boxplot(data=temporal_df, x='year', y='Top1_Accuracy_Percent', ax=axes[0,1])\n",
    "        axes[0,1].set_title('Performance Distribution by Year', fontsize=14, fontweight='bold')\n",
    "        axes[0,1].set_xlabel('Year', fontweight='bold')\n",
    "        axes[0,1].set_ylabel('Top-1 Accuracy (%)', fontweight='bold')\n",
    "        axes[0,1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Plot 3: Dataset volume vs performance over years\n",
    "    if len(temporal_df) > 10:\n",
    "        scatter = axes[1,0].scatter(temporal_df['Predictions_Count'], \n",
    "                                  temporal_df['Top1_Accuracy_Percent'],\n",
    "                                  c=temporal_df['year'], cmap='viridis', \n",
    "                                  alpha=0.6, s=50)\n",
    "        axes[1,0].set_xlabel('Predictions Count (per year)', fontweight='bold')\n",
    "        axes[1,0].set_ylabel('Top-1 Accuracy (%)', fontweight='bold')\n",
    "        axes[1,0].set_title('Dataset Volume vs Performance\\\\n(Color = Year)', fontsize=14, fontweight='bold')\n",
    "        axes[1,0].set_xscale('log')\n",
    "        plt.colorbar(scatter, ax=axes[1,0], label='Year')\n",
    "    \n",
    "    # Plot 4: Performance consistency across years\n",
    "    if len(temporal_df) > 5:\n",
    "        # Calculate coefficient of variation for each model-strategy\n",
    "        consistency_data = []\n",
    "        for (model, strategy), group in temporal_df.groupby(['model', 'strategy']):\n",
    "            if len(group) > 2:  # Need at least 3 years for meaningful CV\n",
    "                cv = (group['Top1_Accuracy_Percent'].std() / group['Top1_Accuracy_Percent'].mean()) * 100\n",
    "                consistency_data.append({\n",
    "                    'Model_Strategy': f\"{model.replace('_', ' ')[:10]}\\\\n({strategy.replace('_', ' ')[:8]})\",\n",
    "                    'Consistency_Score': max(0, 100 - cv),  # Higher score = more consistent\n",
    "                    'Mean_Performance': group['Top1_Accuracy_Percent'].mean(),\n",
    "                    'Years_Count': len(group)\n",
    "                })\n",
    "        \n",
    "        if consistency_data:\n",
    "            consistency_df = pd.DataFrame(consistency_data)\n",
    "            consistency_df = consistency_df.sort_values('Mean_Performance', ascending=True).tail(8)  # Top 8\n",
    "            \n",
    "            bars = axes[1,1].barh(consistency_df['Model_Strategy'], consistency_df['Consistency_Score'])\n",
    "            axes[1,1].set_xlabel('Consistency Score (0-100)', fontweight='bold')\n",
    "            axes[1,1].set_title('Performance Consistency Across Years\\\\n(Higher = More Stable)', \n",
    "                               fontsize=14, fontweight='bold')\n",
    "            \n",
    "            # Color bars by mean performance\n",
    "            norm = plt.Normalize(consistency_df['Mean_Performance'].min(), consistency_df['Mean_Performance'].max())\n",
    "            for bar, perf in zip(bars, consistency_df['Mean_Performance']):\n",
    "                bar.set_color(plt.cm.RdYlGn(norm(perf)))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save plot\n",
    "    plot_path = plots_dir / 'temporal_performance_analysis.png'\n",
    "    plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"üìä Saved temporal analysis plot: {plot_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # Show summary statistics\n",
    "    print(\"\\\\nüìà TEMPORAL PERFORMANCE SUMMARY:\")\n",
    "    print(\"=\" * 35)\n",
    "    \n",
    "    if len(temporal_df['year'].unique()) > 1:\n",
    "        yearly_avg = temporal_df.groupby('year')['Top1_Accuracy_Percent'].mean().sort_index()\n",
    "        print(\"üìÖ Average performance by year:\")\n",
    "        for year, perf in yearly_avg.items():\n",
    "            print(f\"   {year}: {perf:.2f}%\")\n",
    "        \n",
    "        # Calculate trend\n",
    "        years = yearly_avg.index.values\n",
    "        perfs = yearly_avg.values\n",
    "        if len(years) > 1:\n",
    "            trend = np.polyfit(years, perfs, 1)[0]  # Linear trend coefficient\n",
    "            direction = \"üìà Improving\" if trend > 0 else \"üìâ Declining\" if trend < 0 else \"‚û°Ô∏è Stable\"\n",
    "            print(f\"\\\\nüéØ Overall trend: {direction} ({trend:+.3f}% per year)\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Could not create temporal analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADVANCED VISUALIZATION 1: Radar Chart for Model Comparison\n",
    "import math\n",
    "\n",
    "def create_radar_chart_data():\n",
    "    \"\"\"\n",
    "    Create radar chart data for multi-dimensional model comparison\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Calculate multiple performance dimensions for each model\n",
    "        radar_data = []\n",
    "        \n",
    "        for model in comparison_df['Model'].unique():\n",
    "            model_data = comparison_df[comparison_df['Model'] == model]\n",
    "            \n",
    "            # Calculate average performance across strategies\n",
    "            avg_top1 = model_data['Top1_Accuracy_Percent'].mean()\n",
    "            avg_top5 = model_data['Top5_Hit_Rate_Percent'].mean()\n",
    "            avg_mrr = model_data['MRR_Percent'].mean()\n",
    "            \n",
    "            # Calculate consistency (inverse of standard deviation)\n",
    "            consistency = 100 - (model_data['Top1_Accuracy_Percent'].std() * 10)  # Scale for radar\n",
    "            consistency = max(0, min(100, consistency))  # Clamp between 0-100\n",
    "            \n",
    "            # Calculate data coverage (based on total predictions)\n",
    "            max_predictions = comparison_df['Total_Predictions'].max()\n",
    "            coverage = (model_data['Total_Predictions'].sum() / max_predictions) * 100\n",
    "            coverage = min(100, coverage)  # Cap at 100%\n",
    "            \n",
    "            # Calculate strategy versatility (number of strategies available)\n",
    "            strategy_count = len(model_data)\n",
    "            versatility = (strategy_count / 3) * 100  # Assuming max 3 strategies\n",
    "            \n",
    "            radar_data.append({\n",
    "                'Model': model.replace('_', ' ').title(),\n",
    "                'Model_Code': model,\n",
    "                'Top1_Accuracy': avg_top1,\n",
    "                'Top5_Hit_Rate': avg_top5,\n",
    "                'MRR': avg_mrr,\n",
    "                'Consistency': consistency,\n",
    "                'Data_Coverage': coverage,\n",
    "                'Strategy_Versatility': versatility\n",
    "            })\n",
    "        \n",
    "        return pd.DataFrame(radar_data)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error creating radar chart data: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Create and export radar chart data\n",
    "radar_df = create_radar_chart_data()\n",
    "\n",
    "if not radar_df.empty:\n",
    "    # Export for Canva\n",
    "    output_file = output_dir / 'model_radar_comparison.csv'\n",
    "    radar_df.to_csv(output_file, index=False)\n",
    "    print(f\"‚úÖ Exported radar chart data: {output_file}\")\n",
    "    print(f\"   üìä {len(radar_df)} models with 6 performance dimensions\")\n",
    "    \n",
    "    # Create visualization\n",
    "    fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(projection='polar'))\n",
    "    \n",
    "    # Categories for radar chart\n",
    "    categories = ['Top1_Accuracy', 'Top5_Hit_Rate', 'MRR', 'Consistency', 'Data_Coverage', 'Strategy_Versatility']\n",
    "    category_labels = ['Top-1\\nAccuracy', 'Top-5\\nHit Rate', 'Mean\\nReciprocal\\nRank', 'Consistency', 'Data\\nCoverage', 'Strategy\\nVersatility']\n",
    "    \n",
    "    # Number of variables\n",
    "    N = len(categories)\n",
    "    \n",
    "    # Compute angles for each category\n",
    "    angles = [n / float(N) * 2 * math.pi for n in range(N)]\n",
    "    angles += angles[:1]  # Complete the circle\n",
    "    \n",
    "    # Plot data for each model\n",
    "    colors = plt.cm.Set3(np.linspace(0, 1, len(radar_df)))\n",
    "    \n",
    "    for idx, (_, row) in enumerate(radar_df.iterrows()):\n",
    "        values = [row[cat] for cat in categories]\n",
    "        values += values[:1]  # Complete the circle\n",
    "        \n",
    "        ax.plot(angles, values, 'o-', linewidth=2, label=row['Model'], color=colors[idx], alpha=0.8)\n",
    "        ax.fill(angles, values, alpha=0.1, color=colors[idx])\n",
    "    \n",
    "    # Customize the chart\n",
    "    ax.set_xticks(angles[:-1])\n",
    "    ax.set_xticklabels(category_labels, fontsize=10)\n",
    "    ax.set_ylim(0, 100)\n",
    "    ax.set_yticks([20, 40, 60, 80, 100])\n",
    "    ax.set_yticklabels(['20%', '40%', '60%', '80%', '100%'], fontsize=8)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add legend\n",
    "    ax.legend(loc='upper right', bbox_to_anchor=(1.2, 1.1), fontsize=9)\n",
    "    \n",
    "    plt.title('Multi-Dimensional Model Performance Comparison\\\\n(Radar Chart)', \n",
    "              fontsize=14, fontweight='bold', y=1.08)\n",
    "    \n",
    "    # Save plot\n",
    "    plot_path = plots_dir / 'model_radar_comparison.png'\n",
    "    plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"üìä Saved radar chart: {plot_path}\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Display the data table\n",
    "    print(\"\\\\nüìã RADAR CHART DATA:\")\n",
    "    display(radar_df.round(2))\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Could not create radar chart data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üî• Additional Advanced Visualizations & Analysis\n",
    "\n",
    "This section contains advanced visualizations and analytics specifically designed for professional presentations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
