{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3428bdf",
   "metadata": {},
   "source": [
    "# VeronaCard¬†Next‚ÄëPOI Prediction ‚Äì Metrics & Exploratory Analysis\n",
    "\n",
    "*Generated automatically on 2025-06-25 13:01 UTC*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b32bbc",
   "metadata": {},
   "source": [
    "# üéØ Obiettivi dell'Analisi\n",
    "\n",
    "Questo notebook fornisce una **valutazione completa** delle performance del modello di predizione next-POI implementato in `veronacard_mob.py`, attraverso un framework di analisi strutturato e riproducibile.\n",
    "\n",
    "## üìã Pipeline di Analisi\n",
    "\n",
    "### üîÑ **1. Data Loading & Preprocessing**\n",
    "- **Caricamento automatico** di tutti i file CSV di predizione (`*_pred_*.csv`)\n",
    "- **Parsing robusto** con gestione errori per file malformati\n",
    "- **Estrazione automatica** dell'anno dai nomi file (es. `dati_2016_pred_model.csv` ‚Üí 2016)\n",
    "- **Validazione** delle predizioni (filtro per liste con esattamente 5 elementi)\n",
    "\n",
    "### üìä **2. Evaluation Metrics**\n",
    "Calcolo di **quattro metriche standard** per sistemi di raccomandazione:\n",
    "- **üéØ Top-1 Accuracy**: Precisione della predizione principale\n",
    "- **üìà Top-5 Hit Rate**: Presenza del POI corretto nella top-5\n",
    "- **‚ö° Mean Reciprocal Rank (MRR)**: Qualit√† del ranking delle predizioni\n",
    "- **üóÇÔ∏è Catalogue Coverage**: Diversit√† delle raccomandazioni\n",
    "\n",
    "### üìà **3. Multi-level Visualization**\n",
    "- **üìä Global metrics**: Performance aggregate su tutto il dataset\n",
    "- **üìÖ Temporal analysis**: Trend evolutivo per anno (2016‚Üí2020)\n",
    "- **üé® Interactive charts**: Grafici comparativi e heat-maps\n",
    "\n",
    "### üîç **4. Advanced Error Analysis Framework**\n",
    "Sistema modulare ed estendibile per analisi approfondita:\n",
    "- **üö® Worst-case analysis**: Identificazione delle coppie POI pi√π problematiche\n",
    "- **üî• Confusion matrices**: Visualizzazione degli errori sui POI pi√π frequenti\n",
    "- **üß† Explainability**: Analisi dei pattern testuali che causano errori (LIME/SHAP-ready)\n",
    "- **üìä Temporal patterns**: Analisi stagionale e trend temporali degli errori\n",
    "\n",
    "## üèóÔ∏è Caratteristiche Architetturali\n",
    "\n",
    "### **‚úÖ Robustezza**\n",
    "- **Error handling** completo per file CSV corrotti\n",
    "- **Graceful degradation** per dipendenze mancanti (LIME, etc.)\n",
    "- **Validation** automatica dei dati di input\n",
    "\n",
    "### **üîß Flessibilit√†**\n",
    "- **Path-agnostic**: Funziona con CSV nella cartella corrente o tramite `DATA_DIR`\n",
    "- **Modular design**: Sezioni indipendenti eseguibili singolarmente\n",
    "- **Extensible framework**: Facile aggiungere nuove metriche o analisi\n",
    "\n",
    "### **üìä Riproducibilit√†**\n",
    "- **Timestamp automatico** nel titolo\n",
    "- **Seed fisso** per operazioni random\n",
    "- **Versioning implicito** tramite nomi file con data\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Quick Start\n",
    "\n",
    "```python\n",
    "# Esecuzione standard - CSV nella cartella 'results/'\n",
    "# Nessuna configurazione richiesta\n",
    "\n",
    "# Esecuzione personalizzata\n",
    "DATA_DIR = \"/path/to/your/csv/files\"  # opzionale\n",
    "```\n",
    "\n",
    "## üìÅ Struttura File Attesa\n",
    "\n",
    "```\n",
    "project_root/\n",
    "‚îú‚îÄ‚îÄ notebook.ipynb           # questo notebook\n",
    "‚îú‚îÄ‚îÄ results/                 # directory predefinita\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ dati_2016_pred_model1.csv\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ dati_2017_pred_model1.csv\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ ...\n",
    "‚îî‚îÄ‚îÄ veronacard_mob.py       # script di generazione predizioni\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "> **üí° Pro Tip**: Il notebook √® progettato per **fail-safe operation** ‚Äî anche se alcuni file sono corrotti o alcune librerie mancano, l'analisi procede con le parti disponibili e fornisce sempre risultati utili."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07dfa4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "import ast\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams['figure.dpi'] = 120\n",
    "plt.rcParams['axes.grid'] = True\n",
    "\n",
    "DATA_DIR = \"../results/qwen2.5_7b/base_version_second/\" \n",
    "#DATA_DIR = \"../results_parziali_sequenziali_ministral/\" \n",
    "#DATA_DIR = \"../result_run_completa_with_geom_srv_univr/\" #llama 3.1:8b\n",
    "#DATA_DIR = \"../results\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a744cbc",
   "metadata": {},
   "source": [
    "## 1. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac05b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import glob, ast, os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Percorso ai file CSV\n",
    "csv_files = [Path(p) for p in glob.glob(os.path.join(DATA_DIR, '*_pred_*.csv'))]\n",
    "csv_files = sorted(csv_files)\n",
    "assert csv_files, \"No CSV files matching *_pred_*.csv found!\"\n",
    "\n",
    "dfs = []\n",
    "for fp in csv_files:\n",
    "    try:\n",
    "        df = pd.read_csv(fp)\n",
    "    except pd.errors.ParserError:\n",
    "        print(f\"Warning: Parser error in {fp}, trying with error handling...\")\n",
    "        df = pd.read_csv(fp, on_bad_lines='skip', engine='python')\n",
    "\n",
    "    # Estrai l'anno dal nome file, es. dati_2016_pred_...\n",
    "    year_token = next((part for part in fp.stem.split('_')\n",
    "                       if part.isdigit() and len(part) == 4), None)\n",
    "    df['year'] = int(year_token) if year_token else np.nan\n",
    "\n",
    "    df['prediction_list'] = df['prediction'].apply(\n",
    "        lambda x: ast.literal_eval(x) if isinstance(x, str) else []\n",
    "    )\n",
    "    df = df[df['prediction_list'].apply(len) == 5]\n",
    "    dfs.append(df)\n",
    "\n",
    "df_all = pd.concat(dfs, ignore_index=True)\n",
    "print(f\"Loaded {len(df_all):,} rows from {len(csv_files)} files\")\n",
    "df_all.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8882065",
   "metadata": {},
   "source": [
    "# 2. Definizione delle Metriche di Valutazione\n",
    "\n",
    "Per valutare le performance del modello di predizione next-POI, utilizziamo quattro metriche standard nel campo dei sistemi di raccomandazione e della predizione sequenziale.\n",
    "\n",
    "## Notazione\n",
    "\n",
    "Sia $y_i$ il vero next-POI (ground truth) per la sequenza $i$-esima, e sia $\\hat{y}_i^{(1)}, \\hat{y}_i^{(2)}, \\ldots, \\hat{y}_i^{(k)}$ la lista **ordinata** delle $k$ raccomandazioni prodotte dal modello per quella sequenza, dove $\\hat{y}_i^{(1)}$ √® la predizione con confidence pi√π alta.\n",
    "\n",
    "---\n",
    "\n",
    "## üìä Top-1 Accuracy\n",
    "\n",
    "Misura la **precisione della predizione principale** del modello.\n",
    "\n",
    "$$\\text{Acc}_{@1} = \\frac{1}{N}\\sum_{i=1}^{N}\\mathbf{1}\\!\\left\\{y_i = \\hat{y}_i^{(1)}\\right\\}$$\n",
    "\n",
    "**Interpretazione**: Percentuale di casi in cui la prima predizione del modello coincide esattamente con il POI realmente visitato.\n",
    "\n",
    "**Range**: [0, 1], dove 1 = predizione perfetta\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Top-k Hit Rate\n",
    "\n",
    "Misura se il POI corretto √® presente **tra le prime $k$ predizioni**.\n",
    "\n",
    "$$\\text{HR}_{@k} = \\frac{1}{N}\\sum_{i=1}^{N}\\mathbf{1}\\!\\left\\{y_i \\in \\{\\hat{y}_i^{(1)}, \\hat{y}_i^{(2)}, \\ldots, \\hat{y}_i^{(k)}\\}\\right\\}$$\n",
    "\n",
    "**Interpretazione**: Percentuale di casi in cui il POI corretto √® presente nella lista delle prime $k$ raccomandazioni. Per $k=5$: \"Il modello include il POI corretto nella sua top-5?\"\n",
    "\n",
    "**Range**: [0, 1], sempre $\\text{HR}_{@k} \\geq \\text{Acc}_{@1}$\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ö° Mean Reciprocal Rank (MRR)\n",
    "\n",
    "Considera sia la **presenza** del POI corretto nella lista che la sua **posizione**.\n",
    "\n",
    "$$\\text{MRR} = \\frac{1}{N}\\sum_{i=1}^{N}\\frac{1}{\\text{rank}_i}$$\n",
    "\n",
    "dove $\\text{rank}_i = \\min\\{r \\mid y_i = \\hat{y}_i^{(r)}\\}$ √® la posizione del POI corretto nella lista ordinata.\n",
    "\n",
    "**Interpretazione**: \n",
    "- Se il POI corretto √® al 1¬∞ posto ‚Üí contributo = 1.0\n",
    "- Se il POI corretto √® al 2¬∞ posto ‚Üí contributo = 0.5  \n",
    "- Se il POI corretto √® al 5¬∞ posto ‚Üí contributo = 0.2\n",
    "- Se il POI corretto non √® nella top-k ‚Üí contributo = 0.0\n",
    "\n",
    "**Range**: [0, 1], dove valori pi√π alti indicano che i POI corretti appaiono nelle prime posizioni\n",
    "\n",
    "---\n",
    "\n",
    "## üóÇÔ∏è Catalogue Coverage  \n",
    "\n",
    "Misura la **diversit√†** delle raccomandazioni prodotte dal modello.\n",
    "\n",
    "$$\\text{Coverage} = \\frac{\\left|\\bigcup_{i=1}^{N}\\{\\hat{y}_i^{(1)}, \\hat{y}_i^{(2)}, \\ldots, \\hat{y}_i^{(k)}\\}\\right|}{|\\mathcal{P}|}$$\n",
    "\n",
    "dove $\\mathcal{P}$ √® l'insieme completo dei POI presenti nel dataset (ground truth).\n",
    "\n",
    "**Interpretazione**: Frazione dei POI disponibili che il modello √® in grado di raccomandare. \n",
    "- Coverage = 1.0: il modello raccomanda tutti i POI del catalogo\n",
    "- Coverage = 0.1: il modello raccomanda solo il 10% dei POI disponibili\n",
    "\n",
    "**Importanza**: Previene il bias verso POI molto popolari e garantisce diversit√† nelle raccomandazioni.\n",
    "\n",
    "---\n",
    "\n",
    "## üí° Note Metodologiche\n",
    "\n",
    "- **$N$**: Numero totale di predizioni nel dataset di test\n",
    "- **$k = 5$**: Utilizziamo consistently una finestra di 5 raccomandazioni\n",
    "- **Ordinamento**: Le predizioni sono ordinate per confidence/probabilit√† decrescente\n",
    "- **Handling missing**: Se $y_i \\notin \\{\\hat{y}_i^{(1)}, \\ldots, \\hat{y}_i^{(k)}\\}$, contributo = 0 per tutte le metriche\n",
    "\n",
    "Queste metriche forniscono una **valutazione completa** del modello: precision (Acc@1), recall (HR@k), ranking quality (MRR), e diversity (Coverage)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04234505",
   "metadata": {},
   "source": [
    "### 2.1 Helper functions & per‚Äërow computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc35347",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def poi_id(x):\n",
    "    \"\"\"\n",
    "    Converte 'x' in un identificatore hashable comparabile con ground-truth.\n",
    "    ‚Ä¢ Se x √® un dict          ‚Üí prova a usare 'poi', 'poi_id', 'name', 'id'\n",
    "    ‚Ä¢ Se x √® list/tuple       ‚Üí ritorna tupla ricorsiva\n",
    "    ‚Ä¢ Altrimenti (str/int)    ‚Üí cast a str\n",
    "    \"\"\"\n",
    "    if isinstance(x, dict):\n",
    "        for key in ('poi', 'poi_id', 'name', 'id'):\n",
    "            if key in x:\n",
    "                return str(x[key])\n",
    "        # fallback: serializza in JSON ordinato\n",
    "        return json.dumps(x, sort_keys=True)\n",
    "    elif isinstance(x, (list, tuple)):\n",
    "        return tuple(map(poi_id, x))\n",
    "    else:\n",
    "        return str(x)\n",
    "\n",
    "# Normalizza prediction_list e ground_truth\n",
    "df_all['prediction_norm']    = df_all['prediction_list'].apply(lambda lst: [poi_id(e) for e in lst])\n",
    "df_all['ground_truth_norm']  = df_all['ground_truth'].apply(poi_id)\n",
    "\n",
    "# -- metriche element-wise --------------------------------------------\n",
    "df_all['hit@1'] = df_all['prediction_norm'].str[0] == df_all['ground_truth_norm']\n",
    "\n",
    "def top_k_hit(row, k=5):\n",
    "    return row['ground_truth_norm'] in row['prediction_norm'][:k]\n",
    "\n",
    "def reciprocal_rank(row, k=5):\n",
    "    try:\n",
    "        rank = row['prediction_norm'][:k].index(row['ground_truth_norm']) + 1\n",
    "        return 1.0 / rank\n",
    "    except ValueError:\n",
    "        return 0.0\n",
    "\n",
    "df_all['hit@5'] = df_all.apply(top_k_hit, axis=1)\n",
    "df_all['rr']    = df_all.apply(reciprocal_rank, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4b31a1",
   "metadata": {},
   "source": [
    "### 2.2 Global metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87936f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_global = {\n",
    "    \"Top-1 Accuracy\": df_all[\"hit@1\"].mean(),\n",
    "    \"Top-5 Hit Rate\": df_all[\"hit@5\"].mean(),\n",
    "    \"MRR\": df_all[\"rr\"].mean(),\n",
    "}\n",
    "\n",
    "# Catalogue Coverage\n",
    "coverage_set = {poi for preds in df_all[\"prediction_norm\"] for poi in preds}\n",
    "metrics_global[\"Catalogue Coverage\"] = (\n",
    "    len(coverage_set) / df_all[\"ground_truth_norm\"].nunique()\n",
    ")\n",
    "\n",
    "# Visualizza in percentuale\n",
    "metrics_df = (\n",
    "    pd.DataFrame(metrics_global, index=[\"Value\"])\n",
    "      .T\n",
    "      .style.format(\"{:.2%}\")\n",
    ")\n",
    "display(metrics_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1f7e3c",
   "metadata": {},
   "source": [
    "### 2.3 Metrics by year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b12f0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "by_year = (\n",
    "    df_all\n",
    "    .groupby('year')\n",
    "    .agg(\n",
    "        top1=('hit@1', 'mean'),\n",
    "        hit5=('hit@5', 'mean'),\n",
    "        mrr=('rr', 'mean'),\n",
    "        n=('card_id', 'size')\n",
    "    )\n",
    "    .reset_index()\n",
    "    .sort_values('year')\n",
    ")\n",
    "display(by_year)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ac7adc",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3f4d2744",
   "metadata": {},
   "source": [
    "## 3. Enhanced Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3j8n1a8ol",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-metric comparison chart\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Top-1 Accuracy\n",
    "bars1 = ax1.bar(by_year['year'].astype(str), by_year['top1']*100, \n",
    "                color='steelblue', alpha=0.7, edgecolor='black')\n",
    "median1 = by_year['top1'].median() * 100\n",
    "ax1.axhline(y=median1, color='red', linestyle='--', alpha=0.8)\n",
    "for bar, value in zip(bars1, by_year['top1']*100):\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.5,\n",
    "             f'{value:.1f}%', ha='center', va='bottom', fontweight='bold')\n",
    "ax1.set_title('Top-1 Accuracy', fontweight='bold', fontsize=14)\n",
    "ax1.set_ylabel('Accuracy (%)', fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Top-5 Hit Rate  \n",
    "bars2 = ax2.bar(by_year['year'].astype(str), by_year['hit5']*100,\n",
    "                color='forestgreen', alpha=0.7, edgecolor='black')\n",
    "median2 = by_year['hit5'].median() * 100\n",
    "ax2.axhline(y=median2, color='red', linestyle='--', alpha=0.8)\n",
    "for bar, value in zip(bars2, by_year['hit5']*100):\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.5,\n",
    "             f'{value:.1f}%', ha='center', va='bottom', fontweight='bold')\n",
    "ax2.set_title('Top-5 Hit Rate', fontweight='bold', fontsize=14)\n",
    "ax2.set_ylabel('Hit Rate (%)', fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# MRR\n",
    "bars3 = ax3.bar(by_year['year'].astype(str), by_year['mrr']*100,\n",
    "                color='darkorange', alpha=0.7, edgecolor='black')\n",
    "median3 = by_year['mrr'].median() * 100\n",
    "ax3.axhline(y=median3, color='red', linestyle='--', alpha=0.8)\n",
    "for bar, value in zip(bars3, by_year['mrr']*100):\n",
    "    ax3.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.5,\n",
    "             f'{value:.1f}%', ha='center', va='bottom', fontweight='bold')\n",
    "ax3.set_title('Mean Reciprocal Rank', fontweight='bold', fontsize=14)\n",
    "ax3.set_ylabel('MRR (%)', fontweight='bold')\n",
    "ax3.set_xlabel('Year', fontweight='bold')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Sample sizes\n",
    "bars4 = ax4.bar(by_year['year'].astype(str), by_year['n'],\n",
    "                color='purple', alpha=0.7, edgecolor='black')\n",
    "median4 = by_year['n'].median()\n",
    "ax4.axhline(y=median4, color='red', linestyle='--', alpha=0.8)\n",
    "for bar, value in zip(bars4, by_year['n']):\n",
    "    ax4.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 500,\n",
    "             f'{value:,}', ha='center', va='bottom', fontweight='bold')\n",
    "ax4.set_title('Sample Size by Year', fontweight='bold', fontsize=14)\n",
    "ax4.set_ylabel('Number of Predictions', fontweight='bold')\n",
    "ax4.set_xlabel('Year', fontweight='bold')\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('LLM Model Performance Analysis - All Metrics', fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cqoxdfys26",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3.1 Performance Trends Analysis\n",
    "\n",
    "# Line plot showing performance trends over years\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "# Plot all metrics as lines\n",
    "x_years = by_year['year'].astype(int)\n",
    "ax.plot(x_years, by_year['top1']*100, marker='o', linewidth=3, markersize=8, \n",
    "        label='Top-1 Accuracy', color='steelblue')\n",
    "ax.plot(x_years, by_year['hit5']*100, marker='s', linewidth=3, markersize=8,\n",
    "        label='Top-5 Hit Rate', color='forestgreen') \n",
    "ax.plot(x_years, by_year['mrr']*100, marker='^', linewidth=3, markersize=8,\n",
    "        label='Mean Reciprocal Rank', color='darkorange')\n",
    "\n",
    "# Add value annotations\n",
    "for i, year in enumerate(x_years):\n",
    "    ax.annotate(f\"{by_year.iloc[i]['top1']*100:.1f}%\", \n",
    "                (year, by_year.iloc[i]['top1']*100 + 1), \n",
    "                ha='center', fontsize=10, fontweight='bold')\n",
    "    ax.annotate(f\"{by_year.iloc[i]['hit5']*100:.1f}%\", \n",
    "                (year, by_year.iloc[i]['hit5']*100 + 1), \n",
    "                ha='center', fontsize=10, fontweight='bold')\n",
    "    ax.annotate(f\"{by_year.iloc[i]['mrr']*100:.1f}%\", \n",
    "                (year, by_year.iloc[i]['mrr']*100 + 1), \n",
    "                ha='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "# Styling\n",
    "ax.set_xlabel('Year', fontsize=14, fontweight='bold')\n",
    "ax.set_ylabel('Performance (%)', fontsize=14, fontweight='bold')\n",
    "ax.set_title('Model Performance Trends Over Time', fontsize=16, fontweight='bold', pad=20)\n",
    "ax.legend(fontsize=12, loc='best')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_xticks(x_years)\n",
    "ax.set_ylim(0, max(by_year['hit5']*100) * 1.1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45jb7n9ou9o",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3.2 POI Popularity Distribution Analysis\n",
    "\n",
    "# Analyze POI visit frequency and prediction accuracy\n",
    "poi_stats = df_all.groupby('ground_truth_norm').agg({\n",
    "    'hit@1': ['count', 'mean'],\n",
    "    'hit@5': 'mean',\n",
    "    'rr': 'mean'\n",
    "}).round(4)\n",
    "\n",
    "# Flatten column names\n",
    "poi_stats.columns = ['visit_count', 'accuracy', 'hit5_rate', 'mrr']\n",
    "poi_stats = poi_stats.sort_values('visit_count', ascending=False).head(15)\n",
    "\n",
    "# Enhanced POI analysis visualization\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 8))\n",
    "\n",
    "# Left plot: Visit counts with accuracy color coding\n",
    "colors = plt.cm.RdYlGn(poi_stats['accuracy'])\n",
    "bars1 = ax1.bar(range(len(poi_stats)), poi_stats['visit_count'], \n",
    "                color=colors, edgecolor='black', linewidth=1)\n",
    "\n",
    "# Add value labels\n",
    "for i, (bar, count, acc) in enumerate(zip(bars1, poi_stats['visit_count'], poi_stats['accuracy'])):\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 500,\n",
    "             f'{count:,}', ha='center', va='bottom', fontweight='bold', fontsize=10)\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., bar.get_height()/2,\n",
    "             f'{acc:.1%}', ha='center', va='center', fontweight='bold', \n",
    "             fontsize=9, color='white' if acc < 0.5 else 'black')\n",
    "\n",
    "ax1.set_xticks(range(len(poi_stats)))\n",
    "ax1.set_xticklabels(poi_stats.index, rotation=45, ha='right')\n",
    "ax1.set_ylabel('Visit Count', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('POI Visit Frequency (Color = Accuracy)', fontsize=14, fontweight='bold')\n",
    "ax1.grid(True, axis='y', alpha=0.3)\n",
    "\n",
    "# Right plot: Accuracy vs Visit Count scatter\n",
    "scatter = ax2.scatter(poi_stats['visit_count'], poi_stats['accuracy']*100, \n",
    "                     s=200, c=poi_stats['hit5_rate']*100, cmap='viridis', \n",
    "                     alpha=0.7, edgecolor='black')\n",
    "\n",
    "# Add POI labels\n",
    "for poi, row in poi_stats.iterrows():\n",
    "    ax2.annotate(poi[:15] + ('...' if len(poi) > 15 else ''), \n",
    "                (row['visit_count'], row['accuracy']*100),\n",
    "                xytext=(5, 5), textcoords='offset points', fontsize=9)\n",
    "\n",
    "ax2.set_xlabel('Visit Count', fontsize=12, fontweight='bold')\n",
    "ax2.set_ylabel('Top-1 Accuracy (%)', fontsize=12, fontweight='bold')\n",
    "ax2.set_title('Accuracy vs Popularity (Color = Hit@5 Rate)', fontsize=14, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Add colorbar\n",
    "cbar = plt.colorbar(scatter, ax=ax2)\n",
    "cbar.set_label('Top-5 Hit Rate (%)', fontsize=11, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary statistics\n",
    "print(f\"\\nüìä POI Analysis Summary:\")\n",
    "print(f\"   ‚Ä¢ Most popular POI: {poi_stats.index[0]} ({poi_stats.iloc[0]['visit_count']:,} visits)\")\n",
    "print(f\"   ‚Ä¢ Highest accuracy POI: {poi_stats.loc[poi_stats['accuracy'].idxmax()].name} ({poi_stats['accuracy'].max():.1%})\")\n",
    "print(f\"   ‚Ä¢ Correlation visit_count vs accuracy: {poi_stats['visit_count'].corr(poi_stats['accuracy']):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1uv7qt4246i",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3.3 Model Performance Summary Dashboard\n",
    "\n",
    "# Create comprehensive performance summary\n",
    "fig = plt.figure(figsize=(20, 12))\n",
    "gs = fig.add_gridspec(3, 4, height_ratios=[1, 1, 1], width_ratios=[1, 1, 1, 1])\n",
    "\n",
    "# Overall metrics (top row)\n",
    "ax_overall = fig.add_subplot(gs[0, :2])\n",
    "metrics_values = [metrics_global[\"Top-1 Accuracy\"]*100, \n",
    "                 metrics_global[\"Top-5 Hit Rate\"]*100,\n",
    "                 metrics_global[\"MRR\"]*100,\n",
    "                 metrics_global[\"Catalogue Coverage\"]*100]\n",
    "metrics_names = [\"Top-1\\nAccuracy\", \"Top-5\\nHit Rate\", \"Mean Reciprocal\\nRank\", \"Catalogue\\nCoverage\"]\n",
    "colors = ['steelblue', 'forestgreen', 'darkorange', 'purple']\n",
    "\n",
    "bars = ax_overall.bar(metrics_names, metrics_values, color=colors, alpha=0.7, edgecolor='black')\n",
    "for bar, value in zip(bars, metrics_values):\n",
    "    ax_overall.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 1,\n",
    "                   f'{value:.1f}%', ha='center', va='bottom', fontweight='bold', fontsize=14)\n",
    "ax_overall.set_title('Overall Model Performance Metrics', fontsize=16, fontweight='bold')\n",
    "ax_overall.set_ylabel('Performance (%)', fontsize=12, fontweight='bold')\n",
    "ax_overall.grid(True, alpha=0.3)\n",
    "\n",
    "# Year trend (top right)\n",
    "ax_trend = fig.add_subplot(gs[0, 2:])\n",
    "x_pos = range(len(by_year))\n",
    "ax_trend.plot(x_pos, by_year['top1']*100, 'o-', linewidth=2, markersize=6, label='Accuracy')\n",
    "ax_trend.plot(x_pos, by_year['hit5']*100, 's-', linewidth=2, markersize=6, label='Hit@5')\n",
    "ax_trend.set_xticks(x_pos)\n",
    "ax_trend.set_xticklabels(by_year['year'].astype(int))\n",
    "ax_trend.set_title('Performance Trend by Year', fontsize=16, fontweight='bold')\n",
    "ax_trend.set_ylabel('Performance (%)', fontsize=12, fontweight='bold')\n",
    "ax_trend.legend()\n",
    "ax_trend.grid(True, alpha=0.3)\n",
    "\n",
    "# Sample distribution (middle left)\n",
    "ax_samples = fig.add_subplot(gs[1, :2])\n",
    "bars_samples = ax_samples.bar(by_year['year'].astype(str), by_year['n'], \n",
    "                             color='mediumseagreen', alpha=0.7, edgecolor='black')\n",
    "for bar, value in zip(bars_samples, by_year['n']):\n",
    "    ax_samples.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 200,\n",
    "                   f'{value:,}', ha='center', va='bottom', fontweight='bold', fontsize=10)\n",
    "ax_samples.set_title('Sample Distribution by Year', fontsize=16, fontweight='bold')\n",
    "ax_samples.set_ylabel('Number of Predictions', fontsize=12, fontweight='bold')\n",
    "ax_samples.grid(True, alpha=0.3)\n",
    "\n",
    "# Top errors (middle right)\n",
    "ax_errors = fig.add_subplot(gs[1, 2:])\n",
    "top_errors = pairs.head(8)\n",
    "bars_err = ax_errors.barh(range(len(top_errors)), top_errors['errors'],\n",
    "                         color='lightcoral', alpha=0.7, edgecolor='black')\n",
    "ax_errors.set_yticks(range(len(top_errors)))\n",
    "ax_errors.set_yticklabels([f\"{row['ground_truth_norm'][:8]}‚Üí{row['first_pred'][:8]}\" \n",
    "                          for _, row in top_errors.iterrows()], fontsize=9)\n",
    "ax_errors.set_title('Top Error Patterns', fontsize=16, fontweight='bold')\n",
    "ax_errors.set_xlabel('Error Count', fontsize=12, fontweight='bold')\n",
    "ax_errors.grid(True, alpha=0.3)\n",
    "\n",
    "# Performance by POI popularity (bottom)\n",
    "ax_poi_perf = fig.add_subplot(gs[2, :])\n",
    "top_poi_for_viz = poi_stats.head(10)\n",
    "x_poi = range(len(top_poi_for_viz))\n",
    "\n",
    "# Dual y-axis plot\n",
    "ax_poi_perf2 = ax_poi_perf.twinx()\n",
    "\n",
    "bars_visits = ax_poi_perf.bar([x - 0.2 for x in x_poi], top_poi_for_viz['visit_count'], \n",
    "                             width=0.4, color='lightblue', alpha=0.7, label='Visit Count')\n",
    "bars_acc = ax_poi_perf2.bar([x + 0.2 for x in x_poi], top_poi_for_viz['accuracy']*100,\n",
    "                           width=0.4, color='orange', alpha=0.7, label='Accuracy (%)')\n",
    "\n",
    "ax_poi_perf.set_xticks(x_poi)\n",
    "ax_poi_perf.set_xticklabels([poi[:10] + ('...' if len(poi) > 10 else '') \n",
    "                           for poi in top_poi_for_viz.index], rotation=45, ha='right')\n",
    "ax_poi_perf.set_title('Performance vs Popularity - Top 10 POI', fontsize=16, fontweight='bold')\n",
    "ax_poi_perf.set_ylabel('Visit Count', fontsize=12, fontweight='bold', color='blue')\n",
    "ax_poi_perf2.set_ylabel('Accuracy (%)', fontsize=12, fontweight='bold', color='orange')\n",
    "ax_poi_perf.grid(True, alpha=0.3)\n",
    "\n",
    "# Add legends\n",
    "ax_poi_perf.legend(loc='upper left')\n",
    "ax_poi_perf2.legend(loc='upper right')\n",
    "\n",
    "plt.suptitle('üéØ LLM-Mob Model Performance Dashboard', fontsize=20, fontweight='bold', y=0.98)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cbabe05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Enhanced bar chart for Top-1 Accuracy\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "# Create bars\n",
    "bars = ax.bar(by_year['year'].astype(str), by_year['top1']*100, \n",
    "              color='steelblue', alpha=0.7, edgecolor='black', linewidth=1.2)\n",
    "\n",
    "# Add median line\n",
    "median_value = by_year['top1'].median() * 100\n",
    "ax.axhline(y=median_value, color='red', linestyle='--', linewidth=2, \n",
    "           label=f'Median: {median_value:.1f}%', alpha=0.8)\n",
    "\n",
    "# Add value labels on each bar\n",
    "for bar, value in zip(bars, by_year['top1']*100):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height + 0.5,\n",
    "            f'{value:.1f}%', ha='center', va='bottom', fontweight='bold', fontsize=11)\n",
    "\n",
    "# Styling improvements\n",
    "ax.set_ylabel('Top-1 Accuracy (%)', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Year', fontsize=14, fontweight='bold')\n",
    "ax.set_title('Top-1 Accuracy (%) by Year', fontsize=16, fontweight='bold', pad=20)\n",
    "ax.grid(True, axis='y', alpha=0.3, linestyle='-')\n",
    "ax.set_ylim(0, max(by_year['top1']*100) * 1.15)\n",
    "ax.legend(fontsize=12)\n",
    "\n",
    "# Add sample size annotations\n",
    "for i, (bar, n) in enumerate(zip(bars, by_year['n'])):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., -max(by_year['top1']*100) * 0.05,\n",
    "            f'n={n:,}', ha='center', va='top', fontsize=10, alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58242dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Enhanced bar chart for Top-5 Hit Rate\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "# Create bars\n",
    "bars = ax.bar(by_year['year'].astype(str), by_year['hit5']*100, \n",
    "              color='forestgreen', alpha=0.7, edgecolor='black', linewidth=1.2)\n",
    "\n",
    "# Add median line\n",
    "median_value = by_year['hit5'].median() * 100\n",
    "ax.axhline(y=median_value, color='red', linestyle='--', linewidth=2, \n",
    "           label=f'Median: {median_value:.1f}%', alpha=0.8)\n",
    "\n",
    "# Add value labels on each bar\n",
    "for bar, value in zip(bars, by_year['hit5']*100):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height + 0.5,\n",
    "            f'{value:.1f}%', ha='center', va='bottom', fontweight='bold', fontsize=11)\n",
    "\n",
    "# Styling improvements\n",
    "ax.set_ylabel('Top-5 Hit Rate (%)', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Year', fontsize=14, fontweight='bold')\n",
    "ax.set_title('Top-5 Hit Rate (%) by Year', fontsize=16, fontweight='bold', pad=20)\n",
    "ax.grid(True, axis='y', alpha=0.3, linestyle='-')\n",
    "ax.set_ylim(0, max(by_year['hit5']*100) * 1.1)\n",
    "ax.legend(fontsize=12)\n",
    "\n",
    "# Add sample size annotations\n",
    "for i, (bar, n) in enumerate(zip(bars, by_year['n'])):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., -max(by_year['hit5']*100) * 0.03,\n",
    "            f'n={n:,}', ha='center', va='top', fontsize=10, alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12cf7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Enhanced bar chart for MRR\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "# Create bars\n",
    "bars = ax.bar(by_year['year'].astype(str), by_year['mrr']*100, \n",
    "              color='darkorange', alpha=0.7, edgecolor='black', linewidth=1.2)\n",
    "\n",
    "# Add median line\n",
    "median_value = by_year['mrr'].median() * 100\n",
    "ax.axhline(y=median_value, color='red', linestyle='--', linewidth=2, \n",
    "           label=f'Median: {median_value:.1f}%', alpha=0.8)\n",
    "\n",
    "# Add value labels on each bar\n",
    "for bar, value in zip(bars, by_year['mrr']*100):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height + 0.5,\n",
    "            f'{value:.1f}%', ha='center', va='bottom', fontweight='bold', fontsize=11)\n",
    "\n",
    "# Styling improvements\n",
    "ax.set_ylabel('Mean Reciprocal Rank (%)', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Year', fontsize=14, fontweight='bold')\n",
    "ax.set_title('Mean Reciprocal Rank (%) by Year', fontsize=16, fontweight='bold', pad=20)\n",
    "ax.grid(True, axis='y', alpha=0.3, linestyle='-')\n",
    "ax.set_ylim(0, max(by_year['mrr']*100) * 1.15)\n",
    "ax.legend(fontsize=12)\n",
    "\n",
    "# Add sample size annotations\n",
    "for i, (bar, n) in enumerate(zip(bars, by_year['n'])):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., -max(by_year['mrr']*100) * 0.05,\n",
    "            f'n={n:,}', ha='center', va='top', fontsize=10, alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38d992e",
   "metadata": {},
   "source": [
    "### 3.1 Coverage breakdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dab3c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_pred_poi = (\n",
    "    pd.Series([poi for preds in df_all[\"prediction_norm\"] for poi in preds])\n",
    "      .nunique()\n",
    ")\n",
    "unique_gt_poi = df_all[\"ground_truth_norm\"].nunique()\n",
    "\n",
    "print(f\"POI distinct in predictions: {unique_pred_poi}\")\n",
    "print(f\"POI distinct in ground-truth: {unique_gt_poi}\")\n",
    "print(f\"Catalogue Coverage: {unique_pred_poi/unique_gt_poi:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2aee9a",
   "metadata": {},
   "source": [
    "## 4. Error analysis ‚Äì overview\n",
    "\n",
    "In questa sezione valutiamo **dove** e **perch√©** il modello sbaglia, seguendo tre filoni:\n",
    "\n",
    "1. **Worst-performing POI pairs**  \n",
    "   Scopriamo quali coppie `ground-truth ‚Üí first_pred` generano il maggior numero di errori.\n",
    "\n",
    "2. **Confusion matrix per subset**  \n",
    "   Heat-map delle frequenze (o tasso d‚Äôerrore) su un sotto-insieme di POI di interesse\n",
    "   (ad es. i 20 pi√π visitati o solo l‚Äôanno 2020).\n",
    "\n",
    "3. **Explainability (SHAP / LIME)**  \n",
    "   Analisi dei fattori testuali nella colonna `history` che portano a una predizione\n",
    "   sbagliata. Qui mostriamo un prototipo basato su LIME; lo script √® modulare\n",
    "   e pu√≤ essere sostituito da SHAP se usi modelli compatibili.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f8e9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced error analysis with improved visualization\n",
    "ERR = df_all.assign(first_pred=df_all[\"prediction_norm\"].str[0])\n",
    "ERR = ERR[~ERR[\"hit@1\"]]           \n",
    "\n",
    "# Count POI pairs with errors\n",
    "pairs = (\n",
    "    ERR.groupby([\"ground_truth_norm\", \"first_pred\"])\n",
    "        .size()\n",
    "        .reset_index(name=\"errors\")\n",
    "        .sort_values(\"errors\", ascending=False)\n",
    "        .head(15)          \n",
    ")\n",
    "\n",
    "display(pairs)\n",
    "\n",
    "# Enhanced horizontal bar plot\n",
    "fig, ax = plt.subplots(figsize=(14, 10))\n",
    "\n",
    "# Create color gradient based on error count\n",
    "colors = plt.cm.Reds(np.linspace(0.3, 0.9, len(pairs)))\n",
    "\n",
    "bars = ax.barh(\n",
    "    pairs.apply(lambda r: f\"{r['ground_truth_norm']} ‚Üí {r['first_pred']}\", axis=1),\n",
    "    pairs[\"errors\"],\n",
    "    color=colors,\n",
    "    edgecolor='black',\n",
    "    linewidth=0.8\n",
    ")\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, value in zip(bars, pairs[\"errors\"]):\n",
    "    width = bar.get_width()\n",
    "    ax.text(width + width * 0.01, bar.get_y() + bar.get_height()/2,\n",
    "            f'{value:,}', ha='left', va='center', fontweight='bold', fontsize=11)\n",
    "\n",
    "# Styling improvements\n",
    "ax.set_xlabel(\"Number of Errors\", fontsize=14, fontweight='bold')\n",
    "ax.set_ylabel(\"POI Pair (True ‚Üí Predicted)\", fontsize=14, fontweight='bold')\n",
    "ax.set_title(\"Worst-performing POI Pairs ‚Äì Top 15\", fontsize=16, fontweight='bold', pad=20)\n",
    "ax.grid(True, axis='x', alpha=0.3, linestyle='-')\n",
    "ax.invert_yaxis()\n",
    "\n",
    "# Add percentage annotations\n",
    "total_errors = len(ERR)\n",
    "for bar, value in zip(bars, pairs[\"errors\"]):\n",
    "    percentage = value / total_errors * 100\n",
    "    ax.text(bar.get_width() * 0.5, bar.get_y() + bar.get_height()/2,\n",
    "            f'{percentage:.1f}%', ha='center', va='center', \n",
    "            fontweight='bold', fontsize=10, color='white')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a501fa2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced confusion matrix visualization\n",
    "TOP_K = 20          \n",
    "YEAR  = None        \n",
    "\n",
    "subset = df_all.copy()\n",
    "if YEAR is not None:\n",
    "    subset = subset.query(\"year == @YEAR\")\n",
    "\n",
    "# Filter most frequent POIs\n",
    "top_poi = (\n",
    "    subset[\"ground_truth_norm\"]\n",
    "           .value_counts()\n",
    "           .head(TOP_K)\n",
    "           .index\n",
    ")\n",
    "mask = subset[\"ground_truth_norm\"].isin(top_poi) & subset[\"prediction_norm\"].str[0].isin(top_poi)\n",
    "cm_df = pd.crosstab(\n",
    "    subset.loc[mask, \"ground_truth_norm\"],\n",
    "    subset.loc[mask, \"prediction_norm\"].str[0],\n",
    "    rownames=[\"True\"],\n",
    "    colnames=[\"Pred\"],\n",
    "    dropna=False\n",
    ")\n",
    "\n",
    "# Normalize by rows for error rates\n",
    "cm_norm = cm_df.div(cm_df.sum(axis=1), axis=0)\n",
    "\n",
    "# Enhanced heat-map\n",
    "fig, ax = plt.subplots(figsize=(14, 12))\n",
    "im = ax.imshow(cm_norm.values, aspect=\"auto\", cmap='RdYlBu_r', vmin=0, vmax=1)\n",
    "\n",
    "# Customize ticks and labels\n",
    "ax.set_xticks(range(len(cm_norm.columns)))\n",
    "ax.set_xticklabels(cm_norm.columns, rotation=45, ha='right', fontsize=11)\n",
    "ax.set_yticks(range(len(cm_norm.index)))\n",
    "ax.set_yticklabels(cm_norm.index, fontsize=11)\n",
    "\n",
    "# Add text annotations\n",
    "for i in range(len(cm_norm.index)):\n",
    "    for j in range(len(cm_norm.columns)):\n",
    "        value = cm_norm.values[i, j]\n",
    "        if value > 0.01:  # Only show values > 1%\n",
    "            color = 'white' if value > 0.5 else 'black'\n",
    "            ax.text(j, i, f'{value:.2f}', ha='center', va='center',\n",
    "                   fontweight='bold', fontsize=9, color=color)\n",
    "\n",
    "# Enhanced styling\n",
    "ax.set_xlabel(\"Predicted POI\", fontsize=14, fontweight='bold')\n",
    "ax.set_ylabel(\"True POI\", fontsize=14, fontweight='bold')\n",
    "ax.set_title(f\"Confusion Matrix ‚Äì Top {TOP_K} POI\" + \n",
    "            (f\", Year {YEAR}\" if YEAR else \"\"), \n",
    "            fontsize=16, fontweight='bold', pad=20)\n",
    "\n",
    "# Enhanced colorbar\n",
    "cbar = fig.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "cbar.set_label(\"Prediction Probability\", fontsize=12, fontweight='bold')\n",
    "cbar.ax.tick_params(labelsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Add summary statistics\n",
    "print(f\"\\nüìä Confusion Matrix Statistics:\")\n",
    "print(f\"   ‚Ä¢ Diagonal accuracy: {np.diag(cm_norm).mean():.2%}\")\n",
    "print(f\"   ‚Ä¢ Most confused pair: {cm_norm.stack().drop(cm_norm.index).idxmax()}\")\n",
    "print(f\"   ‚Ä¢ Average prediction entropy: {-np.sum(cm_norm * np.log(cm_norm + 1e-10), axis=1).mean():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0439af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------\n",
    "# 4.3 Explainability \n",
    "# ---------------------------------------------------------------------\n",
    "import warnings, json, numpy as np, pandas as pd\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ERROR ANALYSIS - Simplified Version (No LIME required)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Analisi dei pattern testuali negli errori\n",
    "# ---------------------------------------------------------------------\n",
    "err_rows = df_all[~df_all[\"hit@1\"]].copy()\n",
    "correct_rows = df_all[df_all[\"hit@1\"]].copy()\n",
    "\n",
    "if err_rows.empty:\n",
    "    print(\"üéâ Nessun errore Top-1 da analizzare!\")\n",
    "else:\n",
    "    print(f\"üìä Analyzing {len(err_rows):,} prediction errors vs {len(correct_rows):,} correct predictions\")\n",
    "    \n",
    "    # Aggiungi colonne per l'analisi\n",
    "    err_rows['first_pred'] = err_rows[\"prediction_norm\"].str[0]\n",
    "    correct_rows['first_pred'] = correct_rows[\"prediction_norm\"].str[0]\n",
    "    \n",
    "    # ---------------------------------------------------------------------\n",
    "    # 1. Analisi lunghezza history\n",
    "    # ---------------------------------------------------------------------\n",
    "    print(\"\\n1Ô∏è‚É£ HISTORY LENGTH ANALYSIS\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    err_rows['history_length'] = err_rows['history'].astype(str).str.len()\n",
    "    correct_rows['history_length'] = correct_rows['history'].astype(str).str.len()\n",
    "    \n",
    "    print(f\"Average history length in ERRORS: {err_rows['history_length'].mean():.0f} chars\")\n",
    "    print(f\"Average history length in CORRECT: {correct_rows['history_length'].mean():.0f} chars\")\n",
    "    \n",
    "    # ---------------------------------------------------------------------\n",
    "    # 2. Analisi parole chiave pi√π frequenti negli errori\n",
    "    # ---------------------------------------------------------------------\n",
    "    print(\"\\n2Ô∏è‚É£ KEYWORD ANALYSIS IN ERROR CASES\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    def extract_keywords(text, min_length=3):\n",
    "        \"\"\"Estrae parole significative dal testo\"\"\"\n",
    "        if pd.isna(text):\n",
    "            return []\n",
    "        # Rimuovi caratteri speciali e converti in lowercase\n",
    "        words = re.findall(r'\\b[a-zA-Z√Ä-√ø]+\\b', str(text).lower())\n",
    "        # Filtra parole troppo corte e comuni\n",
    "        stop_words = {'and', 'the', 'poi', 'via', 'del', 'dei', 'per', 'con', 'una', 'uno', 'che', 'nel', 'sul'}\n",
    "        return [w for w in words if len(w) >= min_length and w not in stop_words]\n",
    "    \n",
    "    # Estrai keywords dalle history degli errori\n",
    "    error_keywords = []\n",
    "    for history in err_rows['history'].head(1000):  # Limita per performance\n",
    "        error_keywords.extend(extract_keywords(history))\n",
    "    \n",
    "    # Estrai keywords dalle history corrette\n",
    "    correct_keywords = []\n",
    "    for history in correct_rows['history'].head(1000):  # Limita per performance\n",
    "        correct_keywords.extend(extract_keywords(history))\n",
    "    \n",
    "    error_word_freq = Counter(error_keywords).most_common(15)\n",
    "    correct_word_freq = Counter(correct_keywords).most_common(15)\n",
    "    \n",
    "    print(\"Most frequent words in ERROR cases:\")\n",
    "    for word, freq in error_word_freq:\n",
    "        print(f\"  {word:>15s}: {freq:>4d}\")\n",
    "    \n",
    "    print(\"\\nMost frequent words in CORRECT cases:\")\n",
    "    for word, freq in correct_word_freq:\n",
    "        print(f\"  {word:>15s}: {freq:>4d}\")\n",
    "    \n",
    "    # ---------------------------------------------------------------------\n",
    "    # 3. Analisi dei POI pi√π problematici\n",
    "    # ---------------------------------------------------------------------\n",
    "    print(\"\\n3Ô∏è‚É£ MOST PROBLEMATIC POI (Ground Truth)\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # POI con pi√π errori\n",
    "    error_by_true_poi = (\n",
    "        err_rows['ground_truth_norm']\n",
    "        .value_counts()\n",
    "        .head(10)\n",
    "        .reset_index()\n",
    "    )\n",
    "    error_by_true_poi.columns = ['POI', 'Error_Count']\n",
    "    \n",
    "    # Calcola anche il tasso di errore\n",
    "    total_by_poi = df_all['ground_truth_norm'].value_counts()\n",
    "    error_by_true_poi['Total_Count'] = error_by_true_poi['POI'].map(total_by_poi)\n",
    "    error_by_true_poi['Error_Rate'] = error_by_true_poi['Error_Count'] / error_by_true_poi['Total_Count']\n",
    "    \n",
    "    print(\"POI with highest error counts:\")\n",
    "    for _, row in error_by_true_poi.iterrows():\n",
    "        print(f\"  {row['POI']:>25s}: {row['Error_Count']:>3d} errors ({row['Error_Rate']:>5.1%} rate)\")\n",
    "    \n",
    "    # ---------------------------------------------------------------------\n",
    "    # 4. Analisi delle predizioni pi√π frequenti negli errori\n",
    "    # ---------------------------------------------------------------------\n",
    "    print(\"\\n4Ô∏è‚É£ MOST FREQUENT WRONG PREDICTIONS\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    wrong_pred_freq = err_rows['first_pred'].value_counts().head(10)\n",
    "    print(\"Most frequently predicted (wrong) POI:\")\n",
    "    for poi, freq in wrong_pred_freq.items():\n",
    "        print(f\"  {poi:>25s}: {freq:>3d} times\")\n",
    "    \n",
    "    # ---------------------------------------------------------------------\n",
    "    # 5. Esempio dettagliato di errore\n",
    "    # ---------------------------------------------------------------------\n",
    "    print(\"\\n5Ô∏è‚É£ DETAILED ERROR EXAMPLE\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Prendi il primo errore disponibile\n",
    "    sample_idx = err_rows.index[0]\n",
    "    sample = df_all.loc[sample_idx]\n",
    "    \n",
    "    print(f\"Card ID: {sample.get('card_id', 'N/A')}\")\n",
    "    print(f\"Year: {sample.get('year', 'N/A')}\")\n",
    "    print(f\"True POI: {sample['ground_truth_norm']}\")\n",
    "    print(f\"Predicted POI: {sample['prediction_norm'][0]}\")\n",
    "    print(f\"Top-5 Predictions: {sample['prediction_norm']}\")\n",
    "    print(f\"History (first 200 chars): {str(sample['history'])[:200]}...\")\n",
    "    \n",
    "    # ---------------------------------------------------------------------\n",
    "    # 6. Matrice di confusione semplificata (top POI)\n",
    "    # ---------------------------------------------------------------------\n",
    "    print(\"\\n6Ô∏è‚É£ CONFUSION MATRIX - TOP 10 POI\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Prendi i 10 POI pi√π frequenti\n",
    "    top_poi = df_all['ground_truth_norm'].value_counts().head(10).index.tolist()\n",
    "    \n",
    "    # Filtra solo le predizioni che coinvolgono questi POI\n",
    "    confusion_data = df_all[\n",
    "        (df_all['ground_truth_norm'].isin(top_poi)) & \n",
    "        (df_all['prediction_norm'].str[0].isin(top_poi))\n",
    "    ].copy()\n",
    "    \n",
    "    confusion_matrix = pd.crosstab(\n",
    "        confusion_data['ground_truth_norm'],\n",
    "        confusion_data['prediction_norm'].str[0],\n",
    "        normalize='index'  # Normalizza per righe (% di errore per POI)\n",
    "    )\n",
    "    \n",
    "    print(\"Confusion matrix (% of predictions for each true POI):\")\n",
    "    print(\"Rows = True POI, Columns = Predicted POI\")\n",
    "    print(confusion_matrix.round(3))\n",
    "    \n",
    "    # ---------------------------------------------------------------------\n",
    "    # 7. Summary statistiche\n",
    "    # ---------------------------------------------------------------------\n",
    "    print(f\"\\n7Ô∏è‚É£ SUMMARY STATISTICS\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"Total predictions analyzed: {len(df_all):,}\")\n",
    "    print(f\"Total errors (Top-1): {len(err_rows):,} ({len(err_rows)/len(df_all):.2%})\")\n",
    "    print(f\"Unique POI in dataset: {df_all['ground_truth_norm'].nunique():,}\")\n",
    "    print(f\"Unique POI in predictions: {len({poi for preds in df_all['prediction_norm'] for poi in preds}):,}\")\n",
    "    \n",
    "    if len(csv_files) > 1:\n",
    "        error_by_year = df_all.groupby('year')['hit@1'].agg(['count', 'mean']).reset_index()\n",
    "        error_by_year['error_rate'] = 1 - error_by_year['mean']\n",
    "        print(f\"\\nError rate by year:\")\n",
    "        for _, row in error_by_year.iterrows():\n",
    "            if not pd.isna(row['year']):\n",
    "                print(f\"  {int(row['year'])}: {row['error_rate']:.2%} ({int(row['count'])} predictions)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ANALYSIS COMPLETE\")\n",
    "print(\"This analysis doesn't require LIME but provides valuable insights into prediction errors.\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
